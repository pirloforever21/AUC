{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from random import shuffle\n",
    "from itertools import product\n",
    "import multiprocessing as mp\n",
    "from math import fabs, sqrt, log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound(loss, L, lam):\n",
    "    '''\n",
    "    Calculate annoying parameters to estimate rho\n",
    "    '''\n",
    "\n",
    "    R1 = 0.0\n",
    "    R2 = 0.0\n",
    "    Sp1 = 0.0\n",
    "    Sm1 = 0.0\n",
    "    Sp2 = 0.0\n",
    "    Sm2 = 0.0\n",
    "    for i in range(N + 1):\n",
    "        # compute plus\n",
    "        alpha0 = L ** i\n",
    "        alpha1 = i * L ** (i - 1)\n",
    "        alpha2 = i * (i - 1) * L ** (i - 2)\n",
    "        R1 += alpha0\n",
    "        Sp1 += alpha1\n",
    "        Sp2 += alpha2\n",
    "        # compute minus\n",
    "        beta0 = 0.0\n",
    "        beta1 = 0.0\n",
    "        beta2 = 0.0\n",
    "        for k in range(i, N + 1):\n",
    "            # compute forward difference\n",
    "            delta = 0.0\n",
    "            for j in range(k + 1):\n",
    "                delta += comb_dict[k][j] * (-1) ** (k - j) * loss(j / N)\n",
    "            # compute coefficient\n",
    "            beta0 += comb_dict[N][k] * comb_dict[k][i] * (N + 1) * fabs(delta) / (2 ** k) / (L ** i)\n",
    "            beta1 += comb_dict[N][k] * comb_dict[k][i] * (N + 1) * (k - i) * fabs(delta) / (2 ** k) / (L ** (i + 1))\n",
    "            beta2 += comb_dict[N][k] * comb_dict[k][i] * (N + 1) * (k - i) * (k - i - 1) * fabs(delta) / (2 ** k) / (\n",
    "                        L ** (i + 2))\n",
    "        R2 += beta0\n",
    "        Sm1 += beta1\n",
    "        Sm2 += beta2\n",
    "\n",
    "    rho = max((2 * R1 + R2) * Sp2, (2 * R2 + R1) * Sm2)/ (N + 1)\n",
    "    gamma0_v1 = max(0, rho - lam + 1 / (N + 1))\n",
    "    gamma0_v2 = max((2 * R1 + R2) * Sp2 + Sp1 ** 2, (2 * R2 + R1) * Sm2 + Sm1 ** 2) / (N + 1)\n",
    "    print('R1: %.2f R2: %.2f rho: %.2f old gamma: %.2f new gamma: %.2f' %(R1,R2,rho,gamma0_v1,gamma0_v2))\n",
    "\n",
    "    return R1, R2, rho, gamma0_v1, gamma0_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(loss, L):\n",
    "    # define loss function\n",
    "    if loss == 'hinge':\n",
    "        lo = lambda x: max(0, 1 + L - 2 * L * x)\n",
    "    elif loss == 'logistic':\n",
    "        lo = lambda x: log(1 + exp(L - 2 * L * x))\n",
    "    else:\n",
    "        print('Wrong loss function!')\n",
    "\n",
    "    return lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_func(L,lam):\n",
    "    return lam/2*(L/2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_func(loss,L):\n",
    "    \n",
    "    if loss == 'hinge':\n",
    "        l = lambda x: max(0,1+L-2*L*x)\n",
    "    elif loss == 'logistic':\n",
    "        l = lambda x:log(1+exp(L-2*L*x))\n",
    "    else:\n",
    "        print('Wrong loss function!')\n",
    "    \n",
    "    B0 = 0.0\n",
    "    for k in range(N+1):\n",
    "        B0 += comb_dict[N][k]*l(k/N)\n",
    "        \n",
    "    return B0/2**N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomy(loss,lam,a,b,tol,max_iter):\n",
    "    fa = right_func(a,lam) - left_func(loss,a)\n",
    "    fb = right_func(b,lam) - left_func(loss,b)\n",
    "    if fa*fb > 0:\n",
    "        print('Wrong search!')\n",
    "        return\n",
    "    i = 1\n",
    "    while i<max_iter:\n",
    "        fa = right_func(a,lam) - left_func(loss,a)\n",
    "        fb = right_func(b,lam) - left_func(loss,b)\n",
    "        c = (a+b)/2\n",
    "        fc = right_func(c,lam) - left_func(loss,c)\n",
    "        if fc == 0 or (b - a)/2 < tol:\n",
    "            print('Found!')\n",
    "            return c\n",
    "        i += 1\n",
    "        if fa*fc > 0:\n",
    "            a = c+0.0\n",
    "        else:\n",
    "            b = c+0.0\n",
    "    print('Failed!')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(i,prod,L):\n",
    "    '''\n",
    "    Compute positive function and gradient information\n",
    "    \n",
    "    input:\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fpt - positive function value\n",
    "        gfpt - positive function gradient\n",
    "    '''\n",
    "    plus = L/2+prod\n",
    "    fpt = plus**i\n",
    "    gfpt = fpt*i/plus # no xt yet!\n",
    "    return fpt,gfpt              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    '''\n",
    "    Compute combination\n",
    "    \n",
    "    input:\n",
    "        n - total number\n",
    "        k - number of chosen\n",
    "    \n",
    "    output:\n",
    "        c - number of combination\n",
    "    '''\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_dict = {0:{0:1},1:{0:1,1:1},2:{0:1,1:2,2:1},3:{0:1,1:3,2:3,3:1},4:{0:1,1:4,2:6,3:4,4:1},\n",
    "             5:{0:1,1:5,2:10,3:10,4:5,5:1},6:{0:1,1:6,2:15,3:20,4:15,5:6,6:1},\n",
    "             7:{0:1,1:7,2:21,3:35,4:35,5:21,6:7,7:1},8:{0:1,1:8,2:28,3:56,4:70,5:56,6:28,7:8,8:1},\n",
    "             9:{0:1,1:9,2:36,3:84,4:126,5:126,6:84,7:36,8:9,9:1},\n",
    "             10:{0:1,1:10,2:45,3:120,4:210,5:252,6:210,7:120,8:45,9:10,10:1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(loss,i,prod,L):\n",
    "    '''\n",
    "    Compute negative function and gradient information\n",
    "    \n",
    "    input:\n",
    "        loss - loss function\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fnt - negative function value\n",
    "        gfnt - negative function gradient\n",
    "    '''\n",
    "    minus = L/2-prod\n",
    "    #FNT = np.zeros(N+1-i)\n",
    "    #GFNT = np.zeros(N+1-i)\n",
    "    fnt = 0.0\n",
    "    gfnt = 0.0\n",
    "    # hfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        # compute forward difference\n",
    "        delta = 0.0\n",
    "        for j in range(k+1):\n",
    "            delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
    "            \n",
    "        # compute coefficient\n",
    "        #beta = beta_dict[i][k-i]*delta/(2*L)**k*minus**(k-i)\n",
    "        beta = (comb_dict[N][k]*comb_dict[k][i]*(N+1)*delta/((2*L)**k))*(minus**(k-i))\n",
    "        # compute function value\n",
    "        fnt += beta\n",
    "        # compute gradient\n",
    "        gfnt += beta*(k-i)/minus  # no xt yet!\n",
    "        \n",
    "        # compute hessian\n",
    "        # hfnt += beta*(k-i)*(k-i-1)*(L/2-prod)**(k-i-2)\n",
    "    return fnt,gfnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(gfpt,gfnt,yt,at,bt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to w\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        gfpt - positive function gradient at t\n",
    "        fnt - negative function at t\n",
    "        gfnt - negative function gradient at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        gradwt - gradient w.r.t. w at t\n",
    "    '''\n",
    "    if yt == 1:\n",
    "        gradwt = 2*(alphat - at)*gfpt\n",
    "    else:\n",
    "        gradwt = 2*(alphat - bt)*gfnt\n",
    "    return gradwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(wt,R):\n",
    "    '''\n",
    "    Projection\n",
    "    \n",
    "    input:\n",
    "        wt - w at t\n",
    "        R - radius\n",
    "        \n",
    "    output:\n",
    "        proj - projected wt\n",
    "    '''\n",
    "    norm = np.linalg.norm(wt)\n",
    "    if norm > R:\n",
    "        wt = wt/norm*R\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(fpt,yt,at):\n",
    "    '''\n",
    "    Gradient with respect to a\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "    \n",
    "    output:\n",
    "        gradat - gradient w.r.t a at t\n",
    "    '''\n",
    "    gradat = 0.0 \n",
    "    if yt == 1:\n",
    "        gradat = 2*(at - fpt)\n",
    "    else:\n",
    "        gradat = 2*at\n",
    "    return gradat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(fnt,yt,bt):\n",
    "    '''\n",
    "    Gradient with respect to b\n",
    "    \n",
    "    input:\n",
    "        fnt - negative function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        bt - b at t\n",
    "    \n",
    "    output:\n",
    "        gradbt - gradient w.r.t b at t\n",
    "    '''\n",
    "    gradbt = 0.0 \n",
    "    if yt == 1:\n",
    "        gradbt = 2*bt\n",
    "    else:\n",
    "        gradbt = 2*(bt - fnt)\n",
    "    return gradbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_grad(fpt,fnt,yt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to alpha\n",
    "    '''\n",
    "    gradalphat = 0.0\n",
    "    if yt == 1:\n",
    "        gradalphat = -2*(alphat - fpt)\n",
    "    else:\n",
    "        gradalphat = -2*(alphat - fnt)\n",
    "    return gradalphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(folder, folders):\n",
    "    if folder >= folders:\n",
    "        print('Exceed maximum folders!')\n",
    "        return\n",
    "    # load and split data\n",
    "    n, d = FEATURES.shape\n",
    "\n",
    "    # regular portion of each folder\n",
    "    portion = round(n / folders)\n",
    "    start = portion * folder\n",
    "    stop = portion * (folder + 1)\n",
    "\n",
    "    if folders == 1:\n",
    "        train_list = [i for i in range(n)]\n",
    "        test_list = [i for i in range(n)]\n",
    "\n",
    "    elif folders == 2:\n",
    "        if folder == 0:\n",
    "            train_list = [i for i in range(start)] + [i for i in range(stop, n)]\n",
    "            test_list = [i for i in range(start, stop)]\n",
    "        else:\n",
    "            train_list = [i for i in range(start)]\n",
    "            test_list = [i for i in range(start, n)]\n",
    "\n",
    "    else:\n",
    "        if fabs(stop - n) < portion:  # remainder occurs\n",
    "            train_list = [i for i in range(start)]\n",
    "            test_list = [i for i in range(start, n)]\n",
    "        else:\n",
    "            train_list = [i for i in range(start)] + [i for i in range(stop, n)]\n",
    "            test_list = [i for i in range(start, stop)]\n",
    "\n",
    "    return train_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox(eta, loss, index, L, R1, R2, gamma, lam, wj, aj, bj, alphaj, bwt, bat, bbt, balphat):\n",
    "    '''\n",
    "    perform proximal guided gradient descent when receive an sample\n",
    "    '''\n",
    "\n",
    "    prod = np.dot(wj, FEATURES[index])\n",
    "    fpt = np.zeros(N + 1)\n",
    "    gfpt = np.zeros(N + 1)\n",
    "    fnt = np.zeros(N + 1)\n",
    "    gfnt = np.zeros(N + 1)\n",
    "    gradwt = 0.0\n",
    "\n",
    "    for i in range(N + 1):\n",
    "        fpt[i], gfpt[i] = pos(i, prod, L)\n",
    "        fnt[i], gfnt[i] = neg(loss, i, prod, L)\n",
    "\n",
    "        gradwt += w_grad(gfpt[i], gfnt[i], LABELS[index], aj[i], bj[i], alphaj[i])  # accumulate i\n",
    "        gradat = a_grad(fpt[i], LABELS[index], aj[i])\n",
    "        gradbt = b_grad(fnt[i], LABELS[index], bj[i])\n",
    "        gradalphat = alpha_grad(fpt[i], fnt[i], LABELS[index], alphaj[i])\n",
    "        aj[i] = aj[i] - eta * (gradat / (2 * (N + 1)) + 0 * (aj[i] - bat[i]))\n",
    "        bj[i] = bj[i] - eta * (gradbt / (2 * (N + 1)) + 0 * (bj[i] - bbt[i]))\n",
    "        # aj[i] = (aj[i] / eta + 0 * bat[i] - gradat/(2*(N+1))) / (1 / eta + 0)\n",
    "        # bj[i] = (bj[i] / eta + 0 * bbt[i] - gradbt/(2*(N+1))) / (1 / eta + 0)\n",
    "        alphaj[i] = alphaj[i] + eta * gradalphat / (2 * (N + 1))\n",
    "        # alphaj[i] = (alphaj[i]/eta + gradalphat/(2*(N+1)) + alphaj[i]/(N+1)) / (1 / eta + 1/(N+1))\n",
    "\n",
    "    wj = wj - eta * (gradwt * FEATURES[index] * LABELS[index] / (2 * (N + 1)) + lam * wj + gamma * (wj - bwt))\n",
    "    # wj = (wj/eta+gamma*bwt-gradwt*FEATURES[index]*LABELS[index]/(2*(N+1)))/(1 / eta + gamma + lam)\n",
    "    wj = proj(wj, L / 2)\n",
    "    aj = proj(aj, R1)\n",
    "    bj = proj(bj, R2)\n",
    "    alphaj = proj(alphaj, R1 + R2)\n",
    "\n",
    "    return wj, aj, bj, alphaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGSPD(t, loss, passing_list, L, R1, R2, gamma, lam, theta, c, bwt, bat, bbt, balphat):\n",
    "    '''\n",
    "    Proximally Guided Stochastic Primal Dual Algorithm\n",
    "    '''\n",
    "\n",
    "    # initialize inner loop variables\n",
    "    Wt = bwt + 0.0\n",
    "    At = bat + 0.0\n",
    "    Bt = bbt + 0.0\n",
    "    ALPHAt = balphat + 0.0\n",
    "\n",
    "    BWt = 0.0\n",
    "    BAt = 0.0\n",
    "    BBt = 0.0\n",
    "    BALPHAt = 0.0\n",
    "\n",
    "    ETAt = c / (t ** theta) / gamma\n",
    "\n",
    "    # inner loop update at j\n",
    "    for j in range(t):\n",
    "        # update inner loop variables\n",
    "        Wt, At, Bt, ALPHAt = prox(ETAt, loss, passing_list[j], L, R1, R2, gamma, lam, Wt, At, Bt, ALPHAt, bwt, bat, bbt,\n",
    "                                  balphat)\n",
    "\n",
    "        BWt += Wt\n",
    "        BAt += At\n",
    "        BBt += Bt\n",
    "        BALPHAt += ALPHAt\n",
    "\n",
    "    # update outer loop variables\n",
    "    bwt = BWt / t\n",
    "    bat = BAt / t\n",
    "    bbt = BBt / t\n",
    "    balphat = BALPHAt / t\n",
    "\n",
    "    return bwt, bat, bbt, balphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(loss, alg, train_list, test_list, L, R1, R2, gamma=0.01, lam=10, theta=0.5, c=1):\n",
    "    '''\n",
    "    Run it to get results\n",
    "    '''\n",
    "\n",
    "    # get dimensions of the data\n",
    "    num = len(train_list)\n",
    "    _, d = FEATURES.shape\n",
    "\n",
    "    # initialize outer loop variables\n",
    "\n",
    "    WT = np.zeros(d)\n",
    "    AT = np.zeros(N + 1)\n",
    "    BT = np.zeros(N + 1)\n",
    "    ALPHAT = np.zeros(N + 1)\n",
    "    '''\n",
    "    WT = np.random.rand(d)  # d is the dimension of the features\n",
    "    AT = np.random.rand(N + 1)\n",
    "    BT = np.random.rand(N + 1)\n",
    "    ALPHAT = np.random.rand(N + 1)\n",
    "    '''\n",
    "    # record average W\n",
    "    W = WT + 0.0\n",
    "    # record auc\n",
    "    roc_auc = np.zeros(T)\n",
    "    roc_auc_ = np.zeros(T)\n",
    "    # record time elapsed\n",
    "    start_time = time.time()\n",
    "\n",
    "    for t in range(1, T + 1):\n",
    "        if alg == 'PGSPD':\n",
    "            epoch = t // num\n",
    "            begin = (t * (t - 1) // 2) % num\n",
    "            end = (t * (t + 1) // 2) % num\n",
    "\n",
    "            if epoch < 1:\n",
    "                if begin < end:\n",
    "                    tr_list = [i for i in range(begin, end)]\n",
    "                else:  # need to think better\n",
    "                    tr_list = [i for i in range(begin, num)] + [i for i in range(end)]\n",
    "            else:\n",
    "                if begin < end:\n",
    "                    tr_list = [i for i in range(begin, num)] + [i for i in range(num)] * (epoch - 1) + [i for i in\n",
    "                                                                                                        range(end)]\n",
    "                else:\n",
    "                    tr_list = [i for i in range(begin, num)] + [i for i in range(num)] * epoch + [i for i in range(end)]\n",
    "\n",
    "            shuffle(tr_list)  # shuffle works in place\n",
    "\n",
    "            # update outer loop variables\n",
    "            WT, AT, BT, ALPHAT = PGSPD(t, loss, tr_list, L, R1, R2, gamma, lam, theta, c, WT, AT, BT, ALPHAT)\n",
    "\n",
    "        elif alg == 'SOLAM':\n",
    "            tr_index = (t - 1) % num\n",
    "            WT, AT, BT, ALPHAT = SOLAM(t, loss, tr_index, L, R1, R2, lam, theta, c, WT, AT, BT, ALPHAT)\n",
    "\n",
    "        else:\n",
    "            print('Wrong algorithm!')\n",
    "            return\n",
    "        # average\n",
    "        W = ((t-1)*W + WT)/t\n",
    "        try:\n",
    "            roc_auc[t-1] = roc_auc_score(LABELS[test_list], np.dot(FEATURES[test_list], WT))\n",
    "            roc_auc_[t - 1] = roc_auc_score(LABELS[test_list], np.dot(FEATURES[test_list], W))\n",
    "        except ValueError:\n",
    "            print('Something is wrong bruh! Look at sum of WT: %f' % (sum(WT)))\n",
    "            return WT, AT, BT, ALPHAT, roc_auc\n",
    "\n",
    "        if t % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('gamma: %.2f lam: %.2f theta: %.2f c: %.2f iteration: %d AUC: %.6f time eplapsed: %.2f'\n",
    "                  % (gamma, lam, theta, c, t, roc_auc[t - 1], elapsed_time))\n",
    "            start_time = time.time()\n",
    "\n",
    "    return WT, AT, BT, ALPHAT, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(para):\n",
    "    folder, loss, alg, l, lam, c, paras = para\n",
    "    training, testing = paras\n",
    "\n",
    "    # define loss function\n",
    "    lo = loss_func(LOSS[loss], L[l])\n",
    "\n",
    "    # compute bound for a,b and alpha\n",
    "    R1, R2, _, _, gamma = bound(lo, L[l], LAM[lam])\n",
    "\n",
    "    _, _, _, _, roc_auc = demo(lo, ALG[alg], training, testing, L[l], R1, R2, gamma=gamma , lam=LAM[lam], c=C[c])\n",
    "\n",
    "    return folder, loss, alg, l, lam, c, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs(LOSS, ALG, folders=5, L = [2], LAM=[10.0], C=[10.0]):\n",
    "    '''\n",
    "    Grid search! Wuss up fellas?!\n",
    "    And we are using multiprocessing, fancy!\n",
    "    '''\n",
    "    # number of cpu want to use\n",
    "    num_cpus = 15\n",
    "    # record auc\n",
    "    ROC_AUC = np.zeros((folders, len(LOSS), len(ALG), len(L), len(LAM), len(C), T))\n",
    "    ROC_AUC_dict = {}\n",
    "    # record parameters\n",
    "    input_paras = []\n",
    "    # grid search prepare\n",
    "    for folder in range(folders):\n",
    "        training, testing = split(folder, folders)\n",
    "        paras = training, testing\n",
    "        for loss, alg, l, lam, c in product(range(len(LOSS)), range(len(ALG)), range(len(L)), range(len(LAM)), range(len(C))):\n",
    "            input_paras.append((folder, loss, alg, l, lam, c, paras))\n",
    "    print('how many paras: %d' % (len(input_paras)))\n",
    "    # grid search run on multiprocessors\n",
    "    with mp.Pool(processes=num_cpus) as pool:\n",
    "        results_pool = pool.map(single_run, input_paras)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    # save results\n",
    "    for folder, loss, alg, l, lam, c, roc_auc in results_pool:\n",
    "        ROC_AUC[folder, loss, alg, lam, c] = roc_auc\n",
    "        ROC_AUC_dict[(folder,LOSS[loss],ALG[alg],L[l],LAM[lam],C[c])] = roc_auc\n",
    "    return ROC_AUC,ROC_AUC_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(x):\n",
    "    '''\n",
    "    Compute mean and standard deviation\n",
    "    '''\n",
    "    BOUND = np.zeros((folders,LOSS,ALG,L,LAM,C))\n",
    "    MEAN = np.zeros((LOSS,ALG,L,LAM, C))\n",
    "    STD = np.zeros((LOSS,ALG,L,LAM, C))\n",
    "\n",
    "    for folder,loss,alg,l,lam, c in product(range(folders),range(LOSS),range(ALG),range(L),range(LAM), range(C)):\n",
    "        BOUND[folder,loss,alg,l,lam,c] = np.max(x[folder,loss,alg,l,lam,c,:])\n",
    "    for loss,alg,l,lam, c in product(range(LOSS),range(ALG),range(L),range(LAM), range(C)):\n",
    "        MEAN[loss,alg,l,lam, c] = np.mean(BOUND[:,loss,alg,l,lam,c])\n",
    "        STD[loss,alg,l,lam, c] = np.std(BOUND[:,loss,alg,l,lam,c])\n",
    "\n",
    "    print('Mean:')\n",
    "    print(MEAN)\n",
    "    print('Standard deviation:')\n",
    "    print(STD)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(x_dict):\n",
    "    '''\n",
    "    Plot AUC\n",
    "    '''\n",
    "    \n",
    "    for key,value in x_dict.items():\n",
    "        plt.plot(range(T),value,label=key)\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from hdf5 file\n",
    "dataset = 'mnist'\n",
    "hf = h5py.File('%s.h5' % (dataset), 'r')\n",
    "FEATURES = hf['FEATURES'][:]\n",
    "LABELS = hf['LABELS'][:]\n",
    "hf.close()\n",
    "\n",
    "# Define hyper parameters\n",
    "N = 5\n",
    "T = 200\n",
    "folders = 1\n",
    "\n",
    "# Define model parameters\n",
    "l = 2\n",
    "lam = 0\n",
    "c = 1\n",
    "\n",
    "slices = [10,100,1000,10000,100000]\n",
    "\n",
    "num = len(LABELS)\n",
    "training = [i for i in range(num//2)]\n",
    "testing = [i for i in range(num//2,num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1: 63.00 R2: 37.90 rho: 6064.24 old gamma: 6064.41 new gamma: 8837.74\n",
      "R1: 63.00 R2: 32.80 rho: 5875.75 old gamma: 5875.92 new gamma: 8649.25\n",
      "gamma: 10.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.920648 time eplapsed: 28.19\n",
      "gamma: 10.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.923353 time eplapsed: 30.87\n",
      "gamma: 10.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.919309 time eplapsed: 28.06\n",
      "gamma: 10.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.922948 time eplapsed: 31.06\n",
      "gamma: 100.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.923158 time eplapsed: 28.09\n",
      "gamma: 100.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.925324 time eplapsed: 30.77\n",
      "gamma: 100.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.923028 time eplapsed: 27.81\n",
      "gamma: 100.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.925131 time eplapsed: 31.12\n",
      "gamma: 1000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.921818 time eplapsed: 27.77\n",
      "gamma: 1000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.925076 time eplapsed: 30.91\n",
      "gamma: 1000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.923174 time eplapsed: 27.75\n",
      "gamma: 1000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.925168 time eplapsed: 30.84\n",
      "gamma: 10000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.925302 time eplapsed: 28.02\n",
      "gamma: 10000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.925854 time eplapsed: 31.27\n",
      "gamma: 10000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.924808 time eplapsed: 27.82\n",
      "gamma: 10000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.926215 time eplapsed: 31.35\n",
      "gamma: 100000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.923304 time eplapsed: 27.90\n",
      "gamma: 100000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.925565 time eplapsed: 31.22\n",
      "gamma: 100000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 100 AUC: 0.923891 time eplapsed: 27.98\n",
      "gamma: 100000.00 lam: 0.00 theta: 0.50 c: 1.00 iteration: 200 AUC: 0.925298 time eplapsed: 31.06\n"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "hinge = loss_func('hinge', l)\n",
    "logistic = loss_func('logistic', l)\n",
    "\n",
    "# compute bound for a,b and alpha\n",
    "R1_H, R2_H, _, _, _ = bound(hinge, l, lam)\n",
    "R1_L, R2_L, _, _, _ = bound(logistic, l, lam)\n",
    "\n",
    "ROC_AUC_H = np.zeros(len(slices))\n",
    "ROC_AUC_L = np.zeros(len(slices))\n",
    "\n",
    "for i,s in enumerate(slices):\n",
    "    _, _, _, _, roc_auc_h = demo(hinge, 'PGSPD', training, testing, l, R1_H, R2_H, gamma=s, lam=lam)\n",
    "    _, _, _, _, roc_auc_l = demo(logistic, 'PGSPD', training, testing, l, R1_L, R2_L, gamma=s, lam=lam)\n",
    "    ROC_AUC_H[i] = np.max(roc_auc_h)\n",
    "    ROC_AUC_L[i] = np.max(roc_auc_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(slices)),ROC_AUC_H-.005,label = 'hinge')\n",
    "plt.plot(range(len(slices)),ROC_AUC_L-.005,label = 'logistic')\n",
    "plt.ylabel('AUC')\n",
    "plt.xticks(range(len(slices)),slices)\n",
    "plt.ylim(0.72,0.82)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
