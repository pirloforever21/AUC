{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file and seperate x and y\n",
    "def loader(filename):\n",
    "    '''\n",
    "    Data file loader\n",
    "    \n",
    "    input:\n",
    "        filename - filename\n",
    "    \n",
    "    output:\n",
    "        x - sample features\n",
    "        y - sample labels\n",
    "    '''\n",
    "    raw_df = pd.read_csv(filename,header=None,sep = '\\s+|:',engine='python')\n",
    "    y = torch.tensor(raw_df[0].values)\n",
    "    x = torch.tensor(raw_df[raw_df.columns[2::2]].values)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = loader('diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1.0 # range\n",
    "N = 10 # degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(i,t,prod):\n",
    "    '''\n",
    "    Compute positive function and gradient information\n",
    "    \n",
    "    input:\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fpt - positive function value\n",
    "        gfpt - positive function gradient\n",
    "    '''\n",
    "    fpt = 0.0 \n",
    "    gfpt = 0.0 \n",
    "    fpt = (L/2+prod)**i \n",
    "    gfpt = i*(L/2+prod)**(i-1) # no xt yet!\n",
    "    return fpt,gfpt               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be rewrite as a lambda function\n",
    "def hinge(x):\n",
    "    '''\n",
    "    hinge loss function\n",
    "    \n",
    "    input:\n",
    "        x - x value\n",
    "    \n",
    "    output:\n",
    "        phi - hinge loss value\n",
    "    '''\n",
    "    phi = max(0,x)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    '''\n",
    "    Compute combination\n",
    "    \n",
    "    input:\n",
    "        n - total number\n",
    "        k - number of chosen\n",
    "    \n",
    "    output:\n",
    "        c - number of combination\n",
    "    '''\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(loss,i,t,prod):\n",
    "    '''\n",
    "    Compute negative function and gradient information\n",
    "    \n",
    "    input:\n",
    "        loss - loss function\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fnt - negative function value\n",
    "        gfnt - negative function gradient\n",
    "    '''\n",
    "    fnt = 0.0 # n stands for negative\n",
    "    gfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        # compute forward difference\n",
    "        delta = 0.0\n",
    "        for j in range(k+1):\n",
    "            delta += (-1)**(k-j)*comb(k,j)*loss(i/N)\n",
    "        # compute coefficient\n",
    "        beta = comb(N,k)*comb(k,i)*(N+1)*delta/(2*L)**k\n",
    "        # compute function value\n",
    "        fnt += beta*(L/2-prod)**(k-i)\n",
    "        # compute gradient\n",
    "        gfnt += beta*(k-i)*(L/2-prod)**(k-i-1)  # no xt yet!\n",
    "    return fnt,gfnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hat(t,yt,ptm1):\n",
    "    '''\n",
    "    Approximate probability\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        yt - label at t\n",
    "        ptm1 - p at t-1\n",
    "    \n",
    "    output:\n",
    "        pt - p at t\n",
    "    '''\n",
    "    pt = ((t-1)*ptm1 + (yt+1)/2)/t # m stands for minus\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_hat(t,fpt,yt,ptm1,atm1):\n",
    "    '''\n",
    "    Approximate primal a\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        atm1 - a at t-1\n",
    "    \n",
    "    output:\n",
    "        at - a at t\n",
    "    '''\n",
    "    at = (fpt*((yt+1)/2) + (t-1)*ptm1*atm1)/t # do not update pt yet!\n",
    "    return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_hat(t,fmt,yt,ptm1,btm1):\n",
    "    '''\n",
    "    Approximate primal b\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fmt - negative function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        btm1 - b at t-1\n",
    "    \n",
    "    output:\n",
    "        bt - b at t-1\n",
    "    '''\n",
    "    bt = (fmt*((-yt+1)/2) + (t-1)*(1-ptm1)*btm1)/t # indicator of y=-1!\n",
    "    return bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_step(t,at,bt):\n",
    "    '''\n",
    "    Compute dual alpha\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        \n",
    "    output:\n",
    "        alphat - alpha at t\n",
    "    '''\n",
    "    alphat = at + bt\n",
    "    return alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(fpt,gfpt,fnt,gfnt,yt,pt,at,bt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to w\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        gfpt - positive function gradient at t\n",
    "        fnt - negative function at t\n",
    "        gfnt - negative function gradient at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        gradwt - gradient w.r.t. w at t\n",
    "    '''\n",
    "    gradwt = 0.0\n",
    "    if yt == 1:\n",
    "        gradwt = 2*alphat*(1-pt)*gfpt + 2*(1-pt)*(fpt-at)*gfpt - 2*(1-pt)*fpt*gfpt\n",
    "    else:\n",
    "        gradwt = 2*alphat*pt*gfnt + 2*pt*(fnt-bt)*gfnt - 2*pt*fnt*gfnt\n",
    "    return gradwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(fpt,yt,pt,at):\n",
    "    '''\n",
    "    Gradient with respect to a\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "    \n",
    "    output:\n",
    "        gradat - gradient w.r.t a at t\n",
    "    '''\n",
    "    gradat = 0.0 \n",
    "    if yt == 1:\n",
    "        gradat = -2*(1-pt)*(fpt-at)\n",
    "    else:\n",
    "        pass\n",
    "    return gradat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(fnt,yt,pt,bt):\n",
    "    '''\n",
    "    Gradient with respect to b\n",
    "    \n",
    "    input:\n",
    "        fnt - negative function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        bt - b at t\n",
    "    \n",
    "    output:\n",
    "        gradbt - gradient w.r.t b at t\n",
    "    '''\n",
    "    gradbt = 0.0 \n",
    "    if yt == 1:\n",
    "        pass\n",
    "    else:\n",
    "        gradbt = -2*pt*(fnt-bt)\n",
    "    return gradbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOLAM(T,loss,pt = 0.0,wt = 0.0,at = 0.0,bt = 0.0,alphat = 0.0):\n",
    "    '''\n",
    "    Stochastic Online AUC Maximization\n",
    "    \n",
    "    input:\n",
    "        T - total number of iteration\n",
    "        loss - loss function\n",
    "        pt - p at t\n",
    "        wt - w at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        W - record of each wt\n",
    "        A - record of each at\n",
    "        B - record of each bt\n",
    "        ALPHA - record of each alphat\n",
    "    '''\n",
    "    # Initialize W\n",
    "    W = torch.zeros(T)\n",
    "    A = torch.zeros(T)\n",
    "    B = torch.zeros(T)\n",
    "    ALPHA = torch.zeros(T)\n",
    "    \n",
    "    # Loop\n",
    "    for t in range(1,T+1):\n",
    "        \n",
    "        # Update pt\n",
    "        pt = p_hat(t,y[t],pt)\n",
    "        \n",
    "        # Update wt,at,bt\n",
    "        prod = wt*x[t]\n",
    "        fpt = torch.zeros(N+1)\n",
    "        gfpt = torch.zeros(N+1)\n",
    "        fnt = torch.zeros(N+1)\n",
    "        gfnt = torch.zeros(N+1)\n",
    "        gradwt = 0.0\n",
    "        gradat = 0.0\n",
    "        gradbt = 0.0\n",
    "        \n",
    "        for i in range(N+1): # add up info of each i\n",
    "            fpt[i],gfpt[i] = pos(i,t,prod) # partial info\n",
    "            gfpt[i] *= x[t] # get xt now!\n",
    "            fnt[i],gfnt[i] = neg(loss,i,t,prod)\n",
    "            gfnt[i] *= x[t] \n",
    "            gradwt += w_grad(fpt[i],gfpt[i],fnt[i],gfnt[i],yt,pt,at,bt,alphat) \n",
    "            gradat += a_grad(fpt[i],yt,pt,at)\n",
    "            gradbt += b_grad(fnt[i],yt,pt,bt)\n",
    "            \n",
    "        wt -= 1/t*gradwt/(N+1) # step size as 1/t gradient descent\n",
    "        at -= 1/t*gradat/(N+1)\n",
    "        bt -= 1/t*gradbt/(N+1)\n",
    "        alphat = at+bt\n",
    "\n",
    "        W[t-1] = wt\n",
    "        A[t-1] = at\n",
    "        B[t-1] = bt\n",
    "        ALPHA[t-1] = alphat\n",
    "    return W,A,B,ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
