{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file and seperate x and y\n",
    "def loader(filename):\n",
    "    raw_df = pd.read_csv(filename,header=None,sep = '\\s+|:',engine='python')\n",
    "    y = torch.tensor(raw_df[0].values)\n",
    "    x = torch.tensor(raw_df[raw_df.columns[2::2]].values)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = loader('diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1.0 # range\n",
    "N = 10 # degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_pos(i,t,prod):\n",
    "    fpt = 0.0 # p stands for positive\n",
    "    gfpt = 0.0 # g stands for gradient\n",
    "    fpt = (L/2+prod)**i \n",
    "    gfpt = i*(L/2+prod)**(i-1) # no xt yet!\n",
    "    return fpt,gfpt               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be rewrite as a lambda function\n",
    "def hinge(x):\n",
    "    phi = max(0,x)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef(loss,i,k):\n",
    "    # compute forward difference\n",
    "    delta = 0.0\n",
    "    for j in range(k+1):\n",
    "        delta += (-1)**(k-j)*comb(k,j)*loss(i/N)\n",
    "    # compute coefficient\n",
    "    beta = comb(N,k)*comb(k,i)*(N+1)*delta/(2*L)**k\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_neg(loss,i,t,prod):\n",
    "    fnt = 0.0 # n stands for negative\n",
    "    gfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        beta = coef(loss,i,k)\n",
    "        fnt += beta*(L/2-prod)**(k-i)\n",
    "        gfnt += beta*(k-i)*(L/2-prod)**(k-i-1)  # no xt yet!\n",
    "    return fnt,gfnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hat(t,yt,ptm1):\n",
    "    # Approximate probability\n",
    "    pt = ((t-1)*ptm1 + (yt+1)/2)/t # m stands for minus\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_hat(t,fpt,yt,ptm1,atm1):\n",
    "    # Approximate a\n",
    "    at = (fpt*((yt+1)/2) + (t-1)*ptm1*atm1)/t # do not update pt yet!\n",
    "    return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_hat(t,fmt,yt,ptm1,btm1):\n",
    "    # Approximate b\n",
    "    bt = (fmt*((-yt+1)/2) + (t-1)*(1-ptm1)*btm1)/t # indicator of y=-1!\n",
    "    return bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_step(t,at,bt):\n",
    "    alphat = at + bt\n",
    "    return alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(i,k,t,wt,fpt,fnt,yt,pt,at,bt,alphat):\n",
    "    gfpt = i*(L/2+wt*xt)**(i-1)*xt # g stands for gradient\n",
    "    gfnt = coef(loss,i,k)\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(t,fpt,fmt,yt,pt,at,bt,alphat):\n",
    "    # Define the gradient at current t\n",
    "    at = 0.0 # to be determined\n",
    "    return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(t,fpt,fmt,yt,pt,at,bt,alphat):\n",
    "    bt = 0.0\n",
    "    return bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPAM(T,wt=0.0,pt=0.0,at=0.0,bt=0.0):\n",
    "    '''\n",
    "    This is it!\n",
    "    '''\n",
    "    # Initialize W\n",
    "    W = torch.empty(T)\n",
    "    \n",
    "    # Loop\n",
    "    for t in range(1,T+1):\n",
    "        # Compute scoring function\n",
    "        fpt = obj_pos(t,wt,x[t])\n",
    "        fmt = obj_neg(t,wt,x[t])\n",
    "        # Update at,bt without update pt yet\n",
    "        at = a_hat(t,fpt,y[t],pt,at)\n",
    "        bt = b_hat(t,fmt,y[t],pt,bt)\n",
    "        # Update pt\n",
    "        pt = p_hat(t,y[t],pt)\n",
    "        # Compute real at,bt,alphat\n",
    "        at *= 1/pt\n",
    "        bt *= 1/(1-pt)\n",
    "        alphat = at + bt\n",
    "        # Compute gradient\n",
    "        gt = F_grad(t,fpt,fmt,y[t],pt,at,bt,alphat)\n",
    "        # Gradient descent\n",
    "        wt += -1/t * gt\n",
    "        \n",
    "        # Record wt\n",
    "        W[t-1] = wt\n",
    "    return W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
