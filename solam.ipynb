{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "#import torch\n",
    "from math import factorial\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(i,prod):\n",
    "    '''\n",
    "    Compute positive function and gradient information\n",
    "    \n",
    "    input:\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fpt - positive function value\n",
    "        gfpt - positive function gradient\n",
    "    '''\n",
    "    fpt = 0.0 \n",
    "    gfpt = 0.0 \n",
    "    fpt = (L/2+prod)**i \n",
    "    gfpt = i*(L/2+prod)**(i-1) # no xt yet!\n",
    "    return fpt,gfpt               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    '''\n",
    "    Compute combination\n",
    "    \n",
    "    input:\n",
    "        n - total number\n",
    "        k - number of chosen\n",
    "    \n",
    "    output:\n",
    "        c - number of combination\n",
    "    '''\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(loss,i,prod):\n",
    "    '''\n",
    "    Compute negative function and gradient information\n",
    "    \n",
    "    input:\n",
    "        loss - loss function\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fnt - negative function value\n",
    "        gfnt - negative function gradient\n",
    "    '''\n",
    "    fnt = 0.0 # n stands for negative\n",
    "    gfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        # compute forward difference\n",
    "        delta = 0.0\n",
    "        for j in range(k+1):\n",
    "            delta += (-1)**(k-j)*comb(k,j)*loss(j/N)\n",
    "        # compute coefficient\n",
    "        beta = comb(N,k)*comb(k,i)*(N+1)*delta/(2*L)**k\n",
    "        # compute function value\n",
    "        fnt += beta*(L/2-prod)**(k-i)\n",
    "        # compute gradient\n",
    "        gfnt += beta*(k-i)*(L/2-prod)**(k-i-1)  # no xt yet!\n",
    "    return fnt,gfnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hat(t,yt,ptm1):\n",
    "    '''\n",
    "    Approximate probability\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        yt - label at t\n",
    "        ptm1 - p at t-1\n",
    "    \n",
    "    output:\n",
    "        pt - p at t\n",
    "    '''\n",
    "    pt = (t*ptm1 + (yt+1)/2)/(t+1) # m stands for minus\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_hat(t,fpt,yt,ptm1,atm1):\n",
    "    '''\n",
    "    Approximate primal a\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        atm1 - a at t-1\n",
    "    \n",
    "    output:\n",
    "        at - a at t\n",
    "    '''\n",
    "    at = (fpt*((yt+1)/2) + t*ptm1*atm1)/(t+1) # do not update pt yet!\n",
    "    return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_hat(t,fmt,yt,ptm1,btm1):\n",
    "    '''\n",
    "    Approximate primal b\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fmt - negative function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        btm1 - b at t-1\n",
    "    \n",
    "    output:\n",
    "        bt - b at t-1\n",
    "    '''\n",
    "    bt = (fmt*((-yt+1)/2) + t*(1-ptm1)*btm1)/(t+1) # indicator of y=-1!\n",
    "    return bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_step(t,at,bt):\n",
    "    '''\n",
    "    Compute dual alpha\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        \n",
    "    output:\n",
    "        alphat - alpha at t\n",
    "    '''\n",
    "    alphat = at + bt\n",
    "    return alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(gfpt,gfnt,yt,at,bt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to w\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        gfpt - positive function gradient at t\n",
    "        fnt - negative function at t\n",
    "        gfnt - negative function gradient at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        gradwt - gradient w.r.t. w at t\n",
    "    '''\n",
    "    gradwt = 0.0\n",
    "    if yt == 1:\n",
    "        gradwt = 2*(alphat - at)*gfpt\n",
    "    else:\n",
    "        gradwt = 2*(alphat - bt)*gfnt\n",
    "    return gradwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(wt,R):\n",
    "    '''\n",
    "    Projection\n",
    "    \n",
    "    input:\n",
    "        wt - w at t\n",
    "        R - radius\n",
    "        \n",
    "    output:\n",
    "        proj - projected wt\n",
    "    '''\n",
    "    norm = np.linalg.norm(wt)\n",
    "    if norm > R:\n",
    "        wt = wt/norm*R\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(fpt,yt,at):\n",
    "    '''\n",
    "    Gradient with respect to a\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "    \n",
    "    output:\n",
    "        gradat - gradient w.r.t a at t\n",
    "    '''\n",
    "    gradat = 0.0 \n",
    "    if yt == 1:\n",
    "        gradat = 2*(at - fpt)\n",
    "    else:\n",
    "        gradat = 2*at\n",
    "    return gradat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(fnt,yt,bt):\n",
    "    '''\n",
    "    Gradient with respect to b\n",
    "    \n",
    "    input:\n",
    "        fnt - negative function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        bt - b at t\n",
    "    \n",
    "    output:\n",
    "        gradbt - gradient w.r.t b at t\n",
    "    '''\n",
    "    gradbt = 0.0 \n",
    "    if yt == 1:\n",
    "        gradbt = 2*bt\n",
    "    else:\n",
    "        gradbt = 2*(bt - fnt)\n",
    "    return gradbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_grad(fpt,fnt,yt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to alpha\n",
    "    '''\n",
    "    gradalphat = 0.0\n",
    "    if yt == 1:\n",
    "        gradalphat = -2*(alphat - fpt)\n",
    "    else:\n",
    "        gradalphat = -2*(alphat - fnt)\n",
    "    return gradalphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(pt,fpt,fnt,at,bt,alphat,yt):\n",
    "    '''\n",
    "    Compute objective function value\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        pt - \n",
    "        wt - \n",
    "    \n",
    "    output:\n",
    "        F - objective funciton value\n",
    "    '''\n",
    "    F = 0.0\n",
    "    if yt == 1:\n",
    "        F = (1-pt)*(-pt*alphat**2 + 2*alphat*fpt+(fpt-at)**2 - fpt**2)\n",
    "    else:\n",
    "        F = pt*(-(1-pt)*alphat**2 + 2*alphat*fnt+(fnt-bt)**2 - fnt**2)\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(filename):\n",
    "    '''\n",
    "    Data file loader\n",
    "    \n",
    "    input:\n",
    "        filename - filename\n",
    "    \n",
    "    output:\n",
    "        x - sample features\n",
    "        y - sample labels\n",
    "    '''\n",
    "    # raw data\n",
    "    L = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in csv.reader(file, delimiter = ' '):\n",
    "            line[0] = '0:'+line[0]\n",
    "            line.remove('')\n",
    "            L.append(dict(i.split(':') for i in line))\n",
    "    df = pd.DataFrame(L,dtype=float).fillna(0)\n",
    "    X = df.iloc[:,1:].values\n",
    "    Y = df.iloc[:,0].values\n",
    "    # normalize\n",
    "    norm = np.linalg.norm(X,axis=1)\n",
    "    X = X/norm[:,None]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox(eta,loss,wj,aj,bj,alphaj,bwt,bat,bbt,balphat,x,y):\n",
    "    '''\n",
    "    perform proximal guided gradient descent when receive an sample\n",
    "    '''\n",
    "    prod = np.inner(wj,x)\n",
    "    fpt = np.zeros(N+1)\n",
    "    gfpt = np.zeros(N+1)\n",
    "    fnt = np.zeros(N+1)\n",
    "    gfnt = np.zeros(N+1)\n",
    "    aJ = np.zeros(N+1)\n",
    "    bJ = np.zeros(N+1)\n",
    "    alphaJ = np.zeros(N+1)\n",
    "    gradwt = 0.0\n",
    "    gradat = 0.0\n",
    "    gradbt = 0.0\n",
    "    for i in range(N+1):\n",
    "        fpt[i],gfpt[i] = pos(i,prod)\n",
    "        fnt[i],gfnt[i] = neg(loss,i,prod)\n",
    "        gradwt += w_grad(gfpt[i],gfnt[i],y,aj[i],bj[i],alphaj[i]) # accumulate i\n",
    "        gradat = a_grad(fpt[i],y,aj[i])\n",
    "        gradbt = b_grad(fnt[i],y,bj[i])\n",
    "        gradalphat = alpha_grad(fpt[i],fnt[i],y,alphaj[i])\n",
    "        aJ[i] = aj[i] - eta*(gradat/(N+1)+gamma*(aj[i]-bat[i]))\n",
    "        bJ[i] = bj[i] - eta*(gradbt/(N+1)+gamma*(bj[i]-bbt[i]))\n",
    "        alphaJ[i] = alphaj[i] + eta*gradalphat/(N+1)\n",
    "    wJ = wj - eta*(gradwt*x/(N+1)+ gamma*(wj - bwt))\n",
    "    wJ = proj(wJ,1)\n",
    "    aJ = proj(aJ,1)\n",
    "    bJ = proj(bJ,1)\n",
    "    alphaJ = proj(alphaJ,1)\n",
    "    \n",
    "    return wJ,aJ,bJ,alphaJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGSPD(t,loss,X,Y,bwt,bat,bbt,balphat):\n",
    "    '''\n",
    "    Proximally Guided Stochastic Primal Dual Algorithm\n",
    "    '''\n",
    "    \n",
    "    # initialize inner loop variables\n",
    "    Wt = bwt+0.0\n",
    "    At = bat+0.0\n",
    "    Bt = bbt+0.0\n",
    "    ALPHAt = balphat+0.0\n",
    "    \n",
    "    BWt = np.zeros(d)\n",
    "    BAt = np.zeros(N+1)\n",
    "    BBt = np.zeros(N+1)\n",
    "    BALPHAt = np.zeros(N+1)\n",
    "    \n",
    "    ETAt = np.zeros(t).reshape((t,1))\n",
    "    # inner loop update at j\n",
    "    for j in range(1,t+1): \n",
    "        # step size\n",
    "        ETAt[j-1] = 1/np.sqrt(T)/M # M is the bound for gradient\n",
    "        BWt += ETAt[j-1]*Wt\n",
    "        BAt += ETAt[j-1]*At\n",
    "        BBt += ETAt[j-1]*Bt\n",
    "        BALPHAt += ETAt[j-1]*ALPHAt\n",
    "        # update inner loop variables\n",
    "        Wt,At,Bt,ALPHAt = prox(ETAt[j-1],loss,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat,X[j-1,:],Y[j-1])\n",
    "        \n",
    "    # update outer loop variables\n",
    "    bwt = BWt/sum(ETAt)\n",
    "    bat = BAT/sum(ETAt)\n",
    "    bbt = BBt/sum(ETAt)\n",
    "    balphat = BALPHAt/sum(ETAt)\n",
    "    \n",
    "    return bwt,bat,bbt,balphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 AUC: 0.500000\n",
      "iteration: 2 AUC: 0.500000\n",
      "iteration: 3 AUC: 0.605224\n",
      "iteration: 4 AUC: 0.252806\n",
      "iteration: 5 AUC: 0.241910\n",
      "iteration: 6 AUC: 0.217291\n",
      "iteration: 7 AUC: 0.248604\n",
      "iteration: 8 AUC: 0.254276\n",
      "iteration: 9 AUC: 0.228254\n",
      "iteration: 10 AUC: 0.218209\n",
      "iteration: 11 AUC: 0.230284\n",
      "iteration: 12 AUC: 0.211164\n",
      "iteration: 13 AUC: 0.196537\n",
      "iteration: 14 AUC: 0.201224\n",
      "iteration: 15 AUC: 0.204194\n",
      "iteration: 16 AUC: 0.208634\n",
      "iteration: 17 AUC: 0.206448\n",
      "iteration: 18 AUC: 0.202007\n",
      "iteration: 19 AUC: 0.201164\n",
      "iteration: 20 AUC: 0.206179\n",
      "iteration: 21 AUC: 0.202515\n",
      "iteration: 22 AUC: 0.195634\n",
      "iteration: 23 AUC: 0.199104\n",
      "iteration: 24 AUC: 0.199164\n",
      "iteration: 25 AUC: 0.201269\n",
      "iteration: 26 AUC: 0.205530\n",
      "iteration: 27 AUC: 0.201216\n",
      "iteration: 28 AUC: 0.201552\n",
      "iteration: 29 AUC: 0.209291\n",
      "iteration: 30 AUC: 0.214821\n",
      "iteration: 31 AUC: 0.224276\n",
      "iteration: 32 AUC: 0.233358\n",
      "iteration: 33 AUC: 0.223619\n",
      "iteration: 34 AUC: 0.216649\n",
      "iteration: 35 AUC: 0.209761\n",
      "iteration: 36 AUC: 0.207403\n",
      "iteration: 37 AUC: 0.216127\n",
      "iteration: 38 AUC: 0.219343\n",
      "iteration: 39 AUC: 0.216179\n",
      "iteration: 40 AUC: 0.211604\n",
      "iteration: 41 AUC: 0.224761\n",
      "iteration: 42 AUC: 0.241381\n",
      "iteration: 43 AUC: 0.235955\n",
      "iteration: 44 AUC: 0.241784\n",
      "iteration: 45 AUC: 0.242888\n",
      "iteration: 46 AUC: 0.229515\n",
      "iteration: 47 AUC: 0.224000\n",
      "iteration: 48 AUC: 0.225269\n",
      "iteration: 49 AUC: 0.222470\n",
      "iteration: 50 AUC: 0.223164\n",
      "iteration: 51 AUC: 0.224970\n",
      "iteration: 52 AUC: 0.225799\n",
      "iteration: 53 AUC: 0.224485\n",
      "iteration: 54 AUC: 0.222731\n",
      "iteration: 55 AUC: 0.224463\n",
      "iteration: 56 AUC: 0.221149\n",
      "iteration: 57 AUC: 0.222254\n",
      "iteration: 58 AUC: 0.224619\n",
      "iteration: 59 AUC: 0.228903\n",
      "iteration: 60 AUC: 0.224463\n",
      "iteration: 61 AUC: 0.230672\n",
      "iteration: 62 AUC: 0.234507\n",
      "iteration: 63 AUC: 0.238858\n",
      "iteration: 64 AUC: 0.243843\n",
      "iteration: 65 AUC: 0.245358\n",
      "iteration: 66 AUC: 0.243127\n",
      "iteration: 67 AUC: 0.245299\n",
      "iteration: 68 AUC: 0.247478\n",
      "iteration: 69 AUC: 0.250590\n",
      "iteration: 70 AUC: 0.247440\n",
      "iteration: 71 AUC: 0.247836\n",
      "iteration: 72 AUC: 0.252612\n",
      "iteration: 73 AUC: 0.247851\n",
      "iteration: 74 AUC: 0.246433\n",
      "iteration: 75 AUC: 0.248082\n",
      "iteration: 76 AUC: 0.247045\n",
      "iteration: 77 AUC: 0.245799\n",
      "iteration: 78 AUC: 0.246306\n",
      "iteration: 79 AUC: 0.244418\n",
      "iteration: 80 AUC: 0.244030\n",
      "iteration: 81 AUC: 0.240776\n",
      "iteration: 82 AUC: 0.240075\n",
      "iteration: 83 AUC: 0.239739\n",
      "iteration: 84 AUC: 0.236821\n",
      "iteration: 85 AUC: 0.235993\n",
      "iteration: 86 AUC: 0.235410\n",
      "iteration: 87 AUC: 0.234507\n",
      "iteration: 88 AUC: 0.237604\n",
      "iteration: 89 AUC: 0.236545\n",
      "iteration: 90 AUC: 0.233978\n",
      "iteration: 91 AUC: 0.233284\n",
      "iteration: 92 AUC: 0.236082\n",
      "iteration: 93 AUC: 0.233522\n",
      "iteration: 94 AUC: 0.235209\n",
      "iteration: 95 AUC: 0.233701\n",
      "iteration: 96 AUC: 0.233687\n",
      "iteration: 97 AUC: 0.232396\n",
      "iteration: 98 AUC: 0.228709\n",
      "iteration: 99 AUC: 0.230037\n",
      "iteration: 100 AUC: 0.229060\n",
      "iteration: 101 AUC: 0.228381\n",
      "iteration: 102 AUC: 0.228731\n",
      "iteration: 103 AUC: 0.228978\n",
      "iteration: 104 AUC: 0.228373\n",
      "iteration: 105 AUC: 0.228597\n",
      "iteration: 106 AUC: 0.228500\n",
      "iteration: 107 AUC: 0.228246\n",
      "iteration: 108 AUC: 0.228672\n",
      "iteration: 109 AUC: 0.227985\n",
      "iteration: 110 AUC: 0.227007\n",
      "iteration: 111 AUC: 0.226664\n",
      "iteration: 112 AUC: 0.228104\n",
      "iteration: 113 AUC: 0.228485\n",
      "iteration: 114 AUC: 0.228485\n",
      "iteration: 115 AUC: 0.229597\n",
      "iteration: 116 AUC: 0.228358\n",
      "iteration: 117 AUC: 0.227112\n",
      "iteration: 118 AUC: 0.227194\n",
      "iteration: 119 AUC: 0.226873\n",
      "iteration: 120 AUC: 0.226000\n",
      "iteration: 121 AUC: 0.225500\n",
      "iteration: 122 AUC: 0.223896\n",
      "iteration: 123 AUC: 0.222978\n",
      "iteration: 124 AUC: 0.222627\n",
      "iteration: 125 AUC: 0.222888\n",
      "iteration: 126 AUC: 0.223590\n",
      "iteration: 127 AUC: 0.224343\n",
      "iteration: 128 AUC: 0.223530\n",
      "iteration: 129 AUC: 0.223299\n",
      "iteration: 130 AUC: 0.223530\n",
      "iteration: 131 AUC: 0.223440\n",
      "iteration: 132 AUC: 0.224082\n",
      "iteration: 133 AUC: 0.223664\n",
      "iteration: 134 AUC: 0.223410\n",
      "iteration: 135 AUC: 0.222425\n",
      "iteration: 136 AUC: 0.223090\n",
      "iteration: 137 AUC: 0.223284\n",
      "iteration: 138 AUC: 0.223993\n",
      "iteration: 139 AUC: 0.224142\n",
      "iteration: 140 AUC: 0.224500\n",
      "iteration: 141 AUC: 0.225209\n",
      "iteration: 142 AUC: 0.225142\n",
      "iteration: 143 AUC: 0.223619\n",
      "iteration: 144 AUC: 0.223746\n",
      "iteration: 145 AUC: 0.223015\n",
      "iteration: 146 AUC: 0.223284\n",
      "iteration: 147 AUC: 0.222306\n",
      "iteration: 148 AUC: 0.222328\n",
      "iteration: 149 AUC: 0.222164\n",
      "iteration: 150 AUC: 0.221507\n",
      "iteration: 151 AUC: 0.221739\n",
      "iteration: 152 AUC: 0.220769\n",
      "iteration: 153 AUC: 0.220724\n",
      "iteration: 154 AUC: 0.219590\n",
      "iteration: 155 AUC: 0.219590\n",
      "iteration: 156 AUC: 0.220164\n",
      "iteration: 157 AUC: 0.219440\n",
      "iteration: 158 AUC: 0.220396\n",
      "iteration: 159 AUC: 0.220507\n",
      "iteration: 160 AUC: 0.220142\n",
      "iteration: 161 AUC: 0.220030\n",
      "iteration: 162 AUC: 0.219806\n",
      "iteration: 163 AUC: 0.219590\n",
      "iteration: 164 AUC: 0.219597\n",
      "iteration: 165 AUC: 0.220246\n",
      "iteration: 166 AUC: 0.219948\n",
      "iteration: 167 AUC: 0.219851\n",
      "iteration: 168 AUC: 0.219672\n",
      "iteration: 169 AUC: 0.219933\n",
      "iteration: 170 AUC: 0.219910\n",
      "iteration: 171 AUC: 0.220134\n",
      "iteration: 172 AUC: 0.219358\n",
      "iteration: 173 AUC: 0.219090\n",
      "iteration: 174 AUC: 0.218918\n",
      "iteration: 175 AUC: 0.218672\n",
      "iteration: 176 AUC: 0.219119\n",
      "iteration: 177 AUC: 0.218149\n",
      "iteration: 178 AUC: 0.218007\n",
      "iteration: 179 AUC: 0.218172\n",
      "iteration: 180 AUC: 0.217978\n",
      "iteration: 181 AUC: 0.217276\n",
      "iteration: 182 AUC: 0.216530\n",
      "iteration: 183 AUC: 0.216336\n",
      "iteration: 184 AUC: 0.216358\n",
      "iteration: 185 AUC: 0.215813\n",
      "iteration: 186 AUC: 0.215963\n",
      "iteration: 187 AUC: 0.216328\n",
      "iteration: 188 AUC: 0.216299\n",
      "iteration: 189 AUC: 0.216187\n",
      "iteration: 190 AUC: 0.215791\n",
      "iteration: 191 AUC: 0.215657\n",
      "iteration: 192 AUC: 0.216545\n",
      "iteration: 193 AUC: 0.216828\n",
      "iteration: 194 AUC: 0.216955\n",
      "iteration: 195 AUC: 0.216746\n",
      "iteration: 196 AUC: 0.216881\n",
      "iteration: 197 AUC: 0.216590\n",
      "iteration: 198 AUC: 0.216567\n",
      "iteration: 199 AUC: 0.216679\n",
      "iteration: 200 AUC: 0.216649\n"
     ]
    }
   ],
   "source": [
    "hinge = lambda x:max(0,1+L-2*L*x)\n",
    "logistics = lambda x:np.log(1+np.exp(L-2*L*x))\n",
    "\n",
    "L = 2\n",
    "N = 10\n",
    "M,_ = neg(hinge,0,0) # weak convexity parameter\n",
    "rho = M*N*N\n",
    "gamma = rho+1\n",
    "M = rho/N\n",
    "T = 200 # DO NOT make it longer than n!\n",
    "\n",
    "FEATURES,LABELS = loader('diabetes')\n",
    "\n",
    "# get dimensions of the data\n",
    "n,d = FEATURES.shape\n",
    "    \n",
    "# initialize outer loop variables\n",
    "BWT = np.zeros(d) # d is the dimension of the features\n",
    "BAT = np.zeros(N+1)\n",
    "BBT = np.zeros(N+1)\n",
    "BALPHAT = np.zeros(N+1)\n",
    "\n",
    "roc_auc = np.zeros(T)\n",
    "for t in range(1,T+1):\n",
    "    # sample a point\n",
    "    index = np.random.randint(n, size=t)\n",
    "    #start = (t*(t-1)//2)%n\n",
    "    #end = (t*(t+1)//2)%n\n",
    "    features = FEATURES[index,:]\n",
    "    labels = LABELS[index]\n",
    "    # update outer loop variables\n",
    "    BWT,BAT,BBT,BALPHAT = PGSPD(t,hinge,features,labels,BWT,BAT,BBT,BALPHAT)\n",
    "    fpr, tpr, _ = roc_curve(LABELS, np.dot(FEATURES,BWT))\n",
    "    roc_auc[t-1] = auc(fpr, tpr)\n",
    "    print('iteration: %d AUC: %f' %(t,roc_auc[t-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOLAM(t,batch,loss,pt,wt,at,bt,alphat):\n",
    "    '''\n",
    "    Stochastic Online AUC Maximization step\n",
    "    \n",
    "    input:\n",
    "        T - total number of iteration\n",
    "        F - objective function value\n",
    "        loss - loss function\n",
    "        pt - p at t\n",
    "        wt - w at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        W - record of each wt\n",
    "        A - record of each at\n",
    "        B - record of each bt\n",
    "        ALPHA - record of each alphat\n",
    "    '''\n",
    "    # Loop in the batch\n",
    "    eta = 1/np.sqrt(t+1)/2\n",
    "    for k in range(batch):\n",
    "        \n",
    "        # Update pt\n",
    "        pt = p_hat(t*batch+k,y[(t*batch+k)%M],pt)\n",
    "        # Update wt,at,bt\n",
    "        prod = np.inner(wt,x[(t*batch+k)%M])\n",
    "        fpt = np.zeros(N+1)\n",
    "        gfpt = np.zeros(N+1)\n",
    "        fnt = np.zeros(N+1)\n",
    "        gfnt = np.zeros(N+1)\n",
    "        gradwt = 0.0\n",
    "        gradat = 0.0\n",
    "        gradbt = 0.0\n",
    "        \n",
    "        \n",
    "        for i in range(N+1): # add up info of each i\n",
    "            fpt[i],gfpt[i] = pos(i,t,prod) # partial info\n",
    "            fnt[i],gfnt[i] = neg(loss,i,t,prod)\n",
    "            gradwt += w_grad(fpt[i],gfpt[i],fnt[i],gfnt[i],y[(t*batch+k)%M],pt,at[i],bt[i],alphat[i])\n",
    "            gradat = a_grad(fpt[i],y[(t*batch+k)%M],pt,at[i])\n",
    "            gradbt = b_grad(fnt[i],y[(t*batch+k)%M],pt,bt[i])\n",
    "            at[i] -= eta*gradat/(N+1)/batch\n",
    "            bt[i] -= eta*gradbt/(N+1)/batch\n",
    "            alphat[i] = at[i]+bt[i]\n",
    "            #F += obj(pt,fpt[i],fnt[i],at,bt,alphat,y[(t*batch+k)%T])\n",
    "        \n",
    "        wt -= eta*gradwt*y[(t*batch+k)%M]*x[(t*batch+k)%M]/(N+1)/batch # step size as 1/t gradient descent\n",
    "        \n",
    "    wt = proj(wt,1)    \n",
    "        \n",
    "    return pt,wt,at,bt,alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L = 2 # range\n",
    "N = 10 # degree\n",
    "T = 100 # iteration\n",
    "\n",
    "hinge = lambda x:max(0,1+L-2*L*x)\n",
    "logistics = lambda x:np.log(1+np.exp(L-2*L*x))\n",
    "\n",
    "pt = 0.0\n",
    "wt = np.zeros(d)\n",
    "at = np.zeros(N+1)\n",
    "bt = np.zeros(N+1)\n",
    "alphat = np.zeros(N+1)\n",
    "\n",
    "roc_auc = np.zeros(T)\n",
    "batch = 1\n",
    "for t in range(T):\n",
    "    pt,wt,at,bt,alphat = SOLAM(t,batch,hinge,pt,wt,at,bt,alphat)\n",
    "    fpr, tpr, _ = roc_curve(y, np.dot(x,wt))\n",
    "    roc_auc[t] = auc(fpr, tpr)\n",
    "    print('iteration: %d AUC: %f' %(t+1,roc_auc[t]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
