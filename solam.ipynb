{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from math import factorial\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(i,prod,L):\n",
    "    '''\n",
    "    Compute positive function and gradient information\n",
    "    \n",
    "    input:\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fpt - positive function value\n",
    "        gfpt - positive function gradient\n",
    "    '''\n",
    "    fpt = 0.0 \n",
    "    gfpt = 0.0 \n",
    "    fpt = (L/2+prod)**i \n",
    "    gfpt = i*(L/2+prod)**(i-1) # no xt yet!\n",
    "    hfpt = i*(i-1)*(L/2+prod)**(i-2)\n",
    "    return fpt,gfpt,hfpt               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    '''\n",
    "    Compute combination\n",
    "    \n",
    "    input:\n",
    "        n - total number\n",
    "        k - number of chosen\n",
    "    \n",
    "    output:\n",
    "        c - number of combination\n",
    "    '''\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(loss,i,prod,L):\n",
    "    '''\n",
    "    Compute negative function and gradient information\n",
    "    \n",
    "    input:\n",
    "        loss - loss function\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fnt - negative function value\n",
    "        gfnt - negative function gradient\n",
    "    '''\n",
    "    fnt = 0.0 # n stands for negative\n",
    "    gfnt = 0.0\n",
    "    hfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        # compute forward difference\n",
    "        delta = 0.0\n",
    "        for j in range(k+1):\n",
    "            delta += (-1)**(k-j)*comb(k,j)*loss(j/N)\n",
    "        # compute coefficient\n",
    "        beta = comb(N,k)*comb(k,i)*(N+1)*delta/(2*L)**k\n",
    "        # compute function value\n",
    "        fnt += beta*(L/2-prod)**(k-i)\n",
    "        # compute gradient\n",
    "        gfnt += beta*(k-i)*(L/2-prod)**(k-i-1)  # no xt yet!\n",
    "        # compute hessian\n",
    "        hfnt += beta*(k-i)*(k-i-1)*(L/2-prod)**(k-i-2)\n",
    "    return fnt,gfnt,hfnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(gfpt,gfnt,yt,at,bt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to w\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        gfpt - positive function gradient at t\n",
    "        fnt - negative function at t\n",
    "        gfnt - negative function gradient at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        gradwt - gradient w.r.t. w at t\n",
    "    '''\n",
    "    gradwt = 0.0\n",
    "    if yt == 1:\n",
    "        gradwt = 2*(alphat - at)*gfpt\n",
    "    else:\n",
    "        gradwt = 2*(alphat - bt)*gfnt\n",
    "    return gradwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_hess(hfpt,hfnt,yt,at,bt,alphat):\n",
    "    hesswt = 0.0\n",
    "    if yt == 1:\n",
    "        hesswt = 2*(alphat - at)*hfpt\n",
    "    else:\n",
    "        hesswt = 2*(alphat - bt)*hfnt\n",
    "    return hesswt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(wt,R):\n",
    "    '''\n",
    "    Projection\n",
    "    \n",
    "    input:\n",
    "        wt - w at t\n",
    "        R - radius\n",
    "        \n",
    "    output:\n",
    "        proj - projected wt\n",
    "    '''\n",
    "    norm = np.linalg.norm(wt)\n",
    "    if norm > R:\n",
    "        wt = wt/norm*R\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(fpt,yt,at):\n",
    "    '''\n",
    "    Gradient with respect to a\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "    \n",
    "    output:\n",
    "        gradat - gradient w.r.t a at t\n",
    "    '''\n",
    "    gradat = 0.0 \n",
    "    if yt == 1:\n",
    "        gradat = 2*(at - fpt)\n",
    "    else:\n",
    "        gradat = 2*at\n",
    "    return gradat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(fnt,yt,bt):\n",
    "    '''\n",
    "    Gradient with respect to b\n",
    "    \n",
    "    input:\n",
    "        fnt - negative function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        bt - b at t\n",
    "    \n",
    "    output:\n",
    "        gradbt - gradient w.r.t b at t\n",
    "    '''\n",
    "    gradbt = 0.0 \n",
    "    if yt == 1:\n",
    "        gradbt = 2*bt\n",
    "    else:\n",
    "        gradbt = 2*(bt - fnt)\n",
    "    return gradbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_grad(fpt,fnt,yt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to alpha\n",
    "    '''\n",
    "    gradalphat = 0.0\n",
    "    if yt == 1:\n",
    "        gradalphat = -2*(alphat - fpt)\n",
    "    else:\n",
    "        gradalphat = -2*(alphat - fnt)\n",
    "    return gradalphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(filename):\n",
    "    '''\n",
    "    Data file loader\n",
    "    \n",
    "    input:\n",
    "        filename - filename\n",
    "    \n",
    "    output:\n",
    "        x - sample features\n",
    "        y - sample labels\n",
    "    '''\n",
    "    # raw data\n",
    "    L = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in csv.reader(file, delimiter = ' '):\n",
    "            line[0] = '0:'+line[0]\n",
    "            line.remove('')\n",
    "            L.append(dict(i.split(':') for i in line))\n",
    "    df = pd.DataFrame(L,dtype=float).fillna(0)\n",
    "    X = df.iloc[:,1:].values\n",
    "    Y = df.iloc[:,0].values\n",
    "    # centralize\n",
    "    mean = np.mean(X,axis=1)\n",
    "    #X = (X.transpose() - mean).transpose()\n",
    "    # normalize\n",
    "    norm = np.linalg.norm(X,axis=1)\n",
    "    X = X/norm[:,None]\n",
    "    # convert to binary class\n",
    "    r = np.ptp(Y)\n",
    "    index = np.argwhere(Y<r//2)\n",
    "    INDEX = np.argwhere(Y>=r//2)\n",
    "    Y[index] = -1\n",
    "    Y[INDEX] = 1\n",
    "    Y = Y.astype(int)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOLAM(t,loss,batch,X,Y,L,lam,M,wt,at,bt,alphat):\n",
    "    '''\n",
    "    Stochastic Online AUC Maximization step\n",
    "    \n",
    "    input:\n",
    "        T - total number of iteration\n",
    "        F - objective function value\n",
    "        loss - loss function\n",
    "        pt - p at t\n",
    "        wt - w at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        W - record of each wt\n",
    "        A - record of each at\n",
    "        B - record of each bt\n",
    "        ALPHA - record of each alphat\n",
    "    '''\n",
    "    # Loop in the batch\n",
    "    peta = 1/t/M\n",
    "    deta = np.sqrt(np.log(T)/T)\n",
    "    for k in range(batch):\n",
    "        \n",
    "        # Update wt,at,bt\n",
    "        prod = np.dot(wt,X[k])\n",
    "        fpt = np.zeros(N+1)\n",
    "        gfpt = np.zeros(N+1)\n",
    "        fnt = np.zeros(N+1)\n",
    "        gfnt = np.zeros(N+1)\n",
    "        gradwt = 0.0\n",
    "        gradat = 0.0\n",
    "        gradbt = 0.0\n",
    "        gradalphat = 0.0\n",
    "        \n",
    "        for i in range(N+1): # add up info of each i\n",
    "            fpt[i],gfpt[i] = pos(i,prod,L) # partial info\n",
    "            fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
    "            gradwt += w_grad(gfpt[i],gfnt[i],Y[k],at[i],bt[i],alphat[i])\n",
    "            gradat = a_grad(fpt[i],Y[k],at[i])\n",
    "            gradbt = b_grad(fnt[i],Y[k],bt[i])\n",
    "            gradalphat = alpha_grad(fpt[i],fnt[i],Y[k],alphat[i])\n",
    "            at[i] -= deta*gradat/(N+1)/batch\n",
    "            bt[i] -= deta*gradbt/(N+1)/batch\n",
    "            alphat[i] += deta*gradalphat/(N+1)/batch\n",
    "        \n",
    "        wt = wt - peta*(gradwt*Y[k]*X[k]/(N+1)/batch + lam*wt) # step size as 1/t gradient descent\n",
    "        \n",
    "    wt = proj(wt,L/2)    \n",
    "        \n",
    "    return wt,at,bt,alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox(eta,loss,x,y,L,gamma,lam,wj,aj,bj,alphaj,bwt,bat,bbt,balphat):\n",
    "    '''\n",
    "    perform proximal guided gradient descent when receive an sample\n",
    "    '''\n",
    "    prod = np.inner(wj,x)\n",
    "    fpt = np.zeros(N+1)\n",
    "    gfpt = np.zeros(N+1)\n",
    "    hfpt = np.zeros(N+1)\n",
    "    fnt = np.zeros(N+1)\n",
    "    gfnt = np.zeros(N+1)\n",
    "    hfnt = np.zeros(N+1)\n",
    "    gradwt = 0.0\n",
    "    gradat = 0.0\n",
    "    gradbt = 0.0\n",
    "    gradalphat = 0.0\n",
    "    hesswt = 0.0\n",
    "    for i in range(N+1):\n",
    "        fpt[i],gfpt[i],hfpt[i] = pos(i,prod,L)\n",
    "        fnt[i],gfnt[i],hfnt[i] = neg(loss,i,prod,L)\n",
    "        gradwt += w_grad(gfpt[i],gfnt[i],y,aj[i],bj[i],alphaj[i])# accumulate i\n",
    "        hesswt += w_hess(hfpt[i],hfnt[i],y,aj[i],bj[i],alphaj[i])\n",
    "        gradat = a_grad(fpt[i],y,aj[i])\n",
    "        gradbt = b_grad(fnt[i],y,bj[i])\n",
    "        gradalphat = alpha_grad(fpt[i],fnt[i],y,alphaj[i])\n",
    "        aj[i] = aj[i] - eta*(gradat/(N+1)+gamma*(aj[i]-bat[i]))\n",
    "        bj[i] = bj[i] - eta*(gradbt/(N+1)+gamma*(bj[i]-bbt[i]))\n",
    "        alphaj[i] = alphaj[i] + eta*gradalphat/(N+1)\n",
    "    hessian = hesswt*np.outer(x,x)\n",
    "    # eigen,_ = np.linalg.eig(hessian)\n",
    "    \n",
    "    # print('minimum eigenvalue: %f' %(np.min(eigen)))\n",
    "    wj = wj - eta*(gradwt*x*y/(N+1) + lam*wj + gamma*(wj - bwt))\n",
    "    wj = proj(wj,L/2)\n",
    "    #aJ = proj(aJ,1)\n",
    "    #bJ = proj(bJ,1)\n",
    "    #alphaJ = proj(alphaJ,1)\n",
    "    \n",
    "    return wj,aj,bj,alphaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGSPD(t,loss,X,Y,L,gamma,lam,M,bwt,bat,bbt,balphat):\n",
    "    '''\n",
    "    Proximally Guided Stochastic Primal Dual Algorithm\n",
    "    '''\n",
    "    \n",
    "    # initialize inner loop variables\n",
    "    Wt = bwt+0.0\n",
    "    At = bat+0.0\n",
    "    Bt = bbt+0.0\n",
    "    ALPHAt = balphat+0.0\n",
    "    \n",
    "    BWt = Wt+0.0\n",
    "    BAt = At+0.0\n",
    "    BBt = Bt+0.0\n",
    "    BALPHAt = ALPHAt+0.0\n",
    "    \n",
    "    ETAt = 1/t/M # M is the bound for gradient\n",
    "    # inner loop update at j\n",
    "    for j in range(t): \n",
    "        # update inner loop variables\n",
    "        Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
    "        BWt += Wt\n",
    "        BAt += At\n",
    "        BBt += Bt\n",
    "        BALPHAt += ALPHAt\n",
    "        \n",
    "    # update outer loop variables\n",
    "    bwt = BWt/t\n",
    "    bat = BAt/t\n",
    "    bbt = BBt/t\n",
    "    balphat = BALPHAt/t\n",
    "    \n",
    "    return bwt,bat,bbt,balphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(FEATURES,LABELS,folder,folders):\n",
    "    \n",
    "    if folder > folders:\n",
    "        print('Exceed maximum folders!')\n",
    "        return\n",
    "    # load and split data\n",
    "    #FEATURES,LABELS = loader(dataset)\n",
    "    n,d = FEATURES.shape\n",
    "    # regular portion of each folder\n",
    "    portion = round(n/folders)\n",
    "    start = portion*folder\n",
    "    stop = portion*(folder+1)\n",
    "    if np.abs(stop - n) < portion: # remainder occurs\n",
    "        X_train = FEATURES[:start,:]\n",
    "        Y_train = LABELS[:start]\n",
    "        X_test = FEATURES[start:,:]\n",
    "        Y_test = LABELS[start:]\n",
    "    else:\n",
    "        mask = np.ones(n, bool)\n",
    "        mask[start:stop] = False\n",
    "        X_train = FEATURES[mask,:]\n",
    "        Y_train = LABELS[mask]\n",
    "        X_test = FEATURES[start:stop]\n",
    "        Y_test = LABELS[start:stop]\n",
    "    # get dimensions of the data\n",
    "    n,_ = X_train.shape\n",
    "    # number of epoch\n",
    "    epoch = T//n+1\n",
    "    # augment by epoch\n",
    "    #X_train_augmented = np.tile(X_train,(epoch,1)) # might have memory burden\n",
    "    #Y_train_augmented = np.tile(Y_train,epoch)\n",
    "    \n",
    "    #return X_train_augmented,X_test,Y_train_augmented,Y_test\n",
    "    return X_train,X_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(X_train_augmented,X_test,Y_train_augmented,Y_test,loss,alg,L = 2.0,gamma=0.0,lam=0.0,M=1.0,WT=0,AT=0,BT=0,ALPHAT=0):\n",
    "    '''\n",
    "    Run it to get results\n",
    "    '''\n",
    "    # define loss function\n",
    "    if loss == 'hinge':\n",
    "        loss = lambda x:max(0,1+L-2*L*x)\n",
    "    elif loss == 'logistic':\n",
    "        loss = lambda x:np.log(1+np.exp(L-2*L*x))\n",
    "    else:\n",
    "        print('Wrong loss function!')\n",
    "        return\n",
    "    \n",
    "    # get dimensions of the data\n",
    "    num,d = X_train_augmented.shape\n",
    "    \n",
    "    # initialize outer loop variables\n",
    "    if type(WT) == int:\n",
    "        WT = np.random.rand(d) # d is the dimension of the features\n",
    "        AT = np.random.rand(N+1)\n",
    "        BT = np.random.rand(N+1)\n",
    "        ALPHAT = np.random.rand(N+1)\n",
    "\n",
    "    # record auc\n",
    "    roc_auc = np.zeros(T)\n",
    "    # record time elapsed\n",
    "    start_time = time.time()\n",
    "    for t in range(1,T+1):\n",
    "        \n",
    "        if alg == 'PGSPD':\n",
    "            if t<num:\n",
    "                begin = (t*(t-1)//2)%num\n",
    "                end = (t*(t+1)//2)%num\n",
    "                if begin < end:\n",
    "                    x_train = X_train_augmented[begin:end]\n",
    "                    y_train = Y_train_augmented[begin:end]\n",
    "                else: # need to think better\n",
    "                    #s2 = time.time()\n",
    "                    x_train = np.append(X_train_augmented[begin:],X_train_augmented[:end],axis=0)\n",
    "                    y_train = np.append(Y_train_augmented[begin:],Y_train_augmented[:end],axis=0)\n",
    "                    #e2 = time.time() - s2\n",
    "                    #print('append time : %f' %(e1))\n",
    "                x_train, y_train = shuffle(x_train,y_train)\n",
    "                # update outer loop variables\n",
    "                WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,M,WT,AT,BT,ALPHAT)\n",
    "            else:\n",
    "                x_train, y_train = shuffle(X_train_augmented,Y_train_augmented)\n",
    "                WT,AT,BT,ALPHAT = PGSPD(num,loss,x_train,y_train,L,gamma,lam,M,WT,AT,BT,ALPHAT)\n",
    "            '''\n",
    "            # sample a point\n",
    "            begin = ((t-1)*num//batch)%num\n",
    "            end = (t*num//batch)%num\n",
    "            if begin < end:\n",
    "                x_train = X_train_augmented[begin:end]\n",
    "                y_train = Y_train_augmented[begin:end]\n",
    "            else: # need to think better\n",
    "                x_train = np.append(X_train_augmented[begin:],X_train_augmented[:end],axis=0)\n",
    "                y_train = np.append(Y_train_augmented[begin:],Y_train_augmented[:end],axis=0)\n",
    "            # update outer loop variables\n",
    "            WT,AT,BT,ALPHAT = PGSPD(num//batch,loss,x_train,y_train,L,gamma,lam,M,WT,AT,BT,ALPHAT)\n",
    "            '''\n",
    "        elif alg == 'SOLAM':\n",
    "            # sample a point\n",
    "            begin = (t-1)*batch%num\n",
    "            end = t*batch%num\n",
    "            if begin < end:\n",
    "                x_train = X_train_augmented[begin:end]\n",
    "                y_train = Y_train_augmented[begin:end]\n",
    "            else: # need to think better\n",
    "                x_train = np.append(X_train_augmented[begin:],X_train_augmented[:end],axis=0)\n",
    "                y_train = np.append(Y_train_augmented[begin:],Y_train_augmented[:end],axis=0)\n",
    "            WT,AT,BT,ALPHAT = SOLAM(t,loss,batch,x_train,y_train,L,lam,M,WT,AT,BT,ALPHAT)\n",
    "            \n",
    "        fpr, tpr, _ = roc_curve(Y_test, np.dot(X_test,WT))\n",
    "        roc_auc[t-1] = auc(fpr, tpr)\n",
    "        if t%100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('iteration: %d AUC: %f time eplapsed: %f' %(t,roc_auc[t-1],elapsed_time))\n",
    "            start_time = time.time()\n",
    "    \n",
    "    return WT,AT,BT,ALPHAT,roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(dataset,loss,alg,folders,L = 2.0,gamma=0.0,lam=0.0,M=1.0,WT=0,AT=0,BT=0,ALPHAT=0):\n",
    "    '''\n",
    "    Cross validation\n",
    "    '''\n",
    "    # Load data set\n",
    "    FEATURES,LABELS = loader(dataset) \n",
    "    \n",
    "    # record auc\n",
    "    AUC_ROC = np.zeros(folders)\n",
    "    \n",
    "    # cross validation\n",
    "    for folder in range(folders):\n",
    "        print('folder = %d' %(folder))\n",
    "        X_train_augmented,X_test,Y_train_augmented,Y_test = split(FEATURES,LABELS,folder,folders)\n",
    "        \n",
    "        _,_,_,_,roc_auc = demo(X_train_augmented,X_test,Y_train_augmented,Y_test,loss,alg,L=L,gamma=gamma,lam=lam,M=M,WT=WT,AT=AT,BT=BT,ALPHAT=ALPHAT)\n",
    "        AUC_ROC[folder] = np.max(roc_auc)\n",
    "    print('auc score: %f +/- %f' %(np.mean(AUC_ROC),np.std(AUC_ROC)))\n",
    "    return AUC_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(para):\n",
    "    folder,l,gamma,lam,m,paras = para\n",
    "    X_train_augmented,X_test,Y_train_augmented, Y_test,loss,alg = paras\n",
    "    _,_,_,_,roc_auc = demo(X_train_augmented,\n",
    "                           X_test,\n",
    "                           Y_train_augmented,\n",
    "                           Y_test,\n",
    "                           loss,\n",
    "                           alg,\n",
    "                           L=L[l],gamma=GAMMA[gamma],lam=LAM[lam],M=M[m])\n",
    "    return folder,l,gamma,lam,m, np.max(roc_auc)\n",
    "\n",
    "def removed():\n",
    "    \n",
    "    # cross validation\n",
    "    for folder in range(folders):\n",
    "        print('folder = %d' %(folder))\n",
    "        X_train_augmented,X_test,Y_train_augmented,Y_test = split(FEATURES,LABELS,folder,folders)\n",
    "        \n",
    "        # bloody grid search!\n",
    "        for l in range(len(L)):\n",
    "            for gamma in range(len(GAMMA)):\n",
    "                for lam in range(len(LAM)):\n",
    "                    for m in range(len(M)):\n",
    "                        print('current parameters: L = %.2f, GAMMA = %.2f, LAM = %.2f, M = %.2f' %(L[l],GAMMA[gamma],LAM[lam],M[m]))\n",
    "    \n",
    "def gs(dataset,loss,alg,folders,L=[2.0],GAMMA=[0.0],LAM=[0.0],M=[1.0],WT=0,AT=0,BT=0,ALPHAT=0):\n",
    "    '''\n",
    "    Grid search! Wuss up fellas?!\n",
    "    '''\n",
    "    # Load data set\n",
    "    from itertools import product\n",
    "    import multiprocessing\n",
    "    FEATURES,LABELS = loader(dataset) \n",
    "    # record auc\n",
    "    num_cpus = 2\n",
    "    AUC_ROC = np.zeros((folders,len(L),len(GAMMA),len(LAM),len(M)))\n",
    "    input_paras = []\n",
    "    for folder in range(folders):\n",
    "        X_train_augmented,X_test,Y_train_augmented,Y_test = split(FEATURES,LABELS,folder,folders)\n",
    "        paras = X_train_augmented,X_test,Y_train_augmented, Y_test,loss,alg\n",
    "        for l,gamma,lam,m in product(range(len(L)),range(len(GAMMA)),range(len(LAM)),range(len(M))):\n",
    "            input_paras.append((folder,l,gamma,lam,m,paras))\n",
    "    print('how many paras: %d' % len(input_paras))\n",
    "    pool = multiprocessing.Pool(processes=num_cpus)\n",
    "    results_pool = pool.map(single_run,input_paras)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for folder,l,gamma,lam,m, auc in results_pool:\n",
    "        AUC_ROC[folder][l][gamma][lam][m] = auc\n",
    "                        \n",
    "    return AUC_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10\n",
    "T=200\n",
    "batch=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[2.0,20.0]\n",
    "GAMMA = [0.1,1.0]\n",
    "LAM = [0.1,1.0]\n",
    "M = [0.1,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many paras: 80\n",
      "iteration: 100 AUC: 0.752037 time eplapsed: 14.868645\n",
      "iteration: 100 AUC: 0.771296 time eplapsed: 14.957772\n",
      "iteration: 100 AUC: 0.746852 time eplapsed: 15.330565\n",
      "iteration: 100 AUC: 0.749074 time eplapsed: 15.406876\n",
      "iteration: 100 AUC: 0.751111 time eplapsed: 15.682979\n",
      "iteration: 100 AUC: 0.770926 time eplapsed: 15.830920\n",
      "iteration: 100 AUC: 0.655185 time eplapsed: 14.858499\n",
      "iteration: 100 AUC: 0.781852 time eplapsed: 14.759761\n",
      "iteration: 100 AUC: 0.740556 time eplapsed: 15.968044\n",
      "iteration: 100 AUC: 0.741111 time eplapsed: 16.001140\n",
      "iteration: 100 AUC: 0.746111 time eplapsed: 15.494668\n",
      "iteration: 100 AUC: 0.719815 time eplapsed: 15.516332\n",
      "iteration: 100 AUC: 0.681771 time eplapsed: 16.279061\n",
      "iteration: 100 AUC: 0.751852 time eplapsed: 16.297339\n",
      "iteration: 100 AUC: 0.748958 time eplapsed: 15.454711\n",
      "iteration: 100 AUC: 0.713519 time eplapsed: 15.437275\n",
      "iteration: 100 AUC: 0.715104 time eplapsed: 15.585638\n",
      "iteration: 100 AUC: 0.760185 time eplapsed: 15.561741\n",
      "iteration: 100 AUC: 0.731424 time eplapsed: 14.954904\n",
      "iteration: 100 AUC: 0.747963 time eplapsed: 15.046680\n",
      "iteration: 100 AUC: 0.735069 time eplapsed: 14.965200\n",
      "iteration: 100 AUC: 0.736979 time eplapsed: 15.009211\n",
      "iteration: 100 AUC: 0.712153 time eplapsed: 16.386763\n",
      "iteration: 100 AUC: 0.287847 time eplapsed: 16.528410\n",
      "iteration: 100 AUC: 0.734201 time eplapsed: 15.499437\n",
      "iteration: 100 AUC: 0.794900 time eplapsed: 15.395813\n",
      "iteration: 100 AUC: 0.692014 time eplapsed: 14.245910\n",
      "iteration: 100 AUC: 0.832519 time eplapsed: 14.241383\n",
      "iteration: 100 AUC: 0.654514 time eplapsed: 14.171236\n",
      "iteration: 100 AUC: 0.725267 time eplapsed: 14.144041\n",
      "iteration: 100 AUC: 0.392187 time eplapsed: 15.157400\n",
      "iteration: 100 AUC: 0.810092 time eplapsed: 14.992934\n",
      "iteration: 100 AUC: 0.709170 time eplapsed: 13.832993\n",
      "iteration: 100 AUC: 0.374132 time eplapsed: 14.140076\n",
      "iteration: 100 AUC: 0.719117 time eplapsed: 14.495694\n",
      "iteration: 100 AUC: 0.729861 time eplapsed: 14.443220\n",
      "iteration: 100 AUC: 0.714053 time eplapsed: 20.497670\n",
      "iteration: 100 AUC: 0.724826 time eplapsed: 20.751266\n",
      "iteration: 100 AUC: 0.372581 time eplapsed: 18.724219\n",
      "iteration: 100 AUC: 0.662153 time eplapsed: 19.140484\n",
      "iteration: 100 AUC: 0.797613 time eplapsed: 14.554662\n",
      "iteration: 100 AUC: 0.798930 time eplapsed: 14.440013\n",
      "iteration: 100 AUC: 0.681317 time eplapsed: 16.389443\n",
      "iteration: 100 AUC: 0.813519 time eplapsed: 16.315794\n",
      "iteration: 100 AUC: 0.651474 time eplapsed: 20.216025\n",
      "iteration: 100 AUC: 0.736887 time eplapsed: 19.900062\n",
      "iteration: 100 AUC: 0.731778 time eplapsed: 14.084973\n",
      "iteration: 100 AUC: 0.746529 time eplapsed: 14.060732\n",
      "iteration: 100 AUC: 0.756474 time eplapsed: 17.425384\n",
      "iteration: 100 AUC: 0.532827 time eplapsed: 17.714406\n",
      "iteration: 100 AUC: 0.765814 time eplapsed: 14.643370\n",
      "iteration: 100 AUC: 0.651112 time eplapsed: 14.754198\n",
      "iteration: 100 AUC: 0.763794 time eplapsed: 15.402095\n",
      "iteration: 100 AUC: 0.676433 time eplapsed: 15.543675\n",
      "iteration: 100 AUC: 0.636175 time eplapsed: 14.493081\n",
      "iteration: 100 AUC: 0.349430 time eplapsed: 14.579973\n",
      "iteration: 100 AUC: 0.762684 time eplapsed: 14.065163\n",
      "iteration: 100 AUC: 0.785653 time eplapsed: 13.924819\n",
      "iteration: 100 AUC: 0.743702 time eplapsed: 15.105353\n",
      "iteration: 100 AUC: 0.845424 time eplapsed: 15.063007\n",
      "iteration: 100 AUC: 0.762179 time eplapsed: 14.595182\n",
      "iteration: 100 AUC: 0.761716 time eplapsed: 14.747238\n",
      "iteration: 100 AUC: 0.769297 time eplapsed: 1221.564139\n",
      "iteration: 100 AUC: 0.736961 time eplapsed: 1221.507557\n",
      "iteration: 100 AUC: 0.764753 time eplapsed: 22.458669\n",
      "iteration: 100 AUC: 0.762661 time eplapsed: 22.474693\n",
      "iteration: 100 AUC: 0.766419 time eplapsed: 22.385513\n",
      "iteration: 100 AUC: 0.755669 time eplapsed: 22.933424\n",
      "iteration: 100 AUC: 0.524565 time eplapsed: 24.667628\n",
      "iteration: 100 AUC: 0.758314 time eplapsed: 23.953471\n",
      "iteration: 100 AUC: 0.766818 time eplapsed: 15.873201\n",
      "iteration: 100 AUC: 0.755480 time eplapsed: 16.004357\n",
      "iteration: 100 AUC: 0.785147 time eplapsed: 16.080244\n",
      "iteration: 100 AUC: 0.752268 time eplapsed: 16.555788\n",
      "iteration: 100 AUC: 0.823129 time eplapsed: 18.261202\n",
      "iteration: 100 AUC: 0.566515 time eplapsed: 18.080917\n",
      "iteration: 100 AUC: 0.688587 time eplapsed: 14.729628\n",
      "iteration: 100 AUC: 0.564437 time eplapsed: 14.765493\n",
      "iteration: 100 AUC: 0.616969 time eplapsed: 14.231794\n",
      "iteration: 100 AUC: 0.383598 time eplapsed: 14.088967\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "diabetes_hinge = gs('diabetes','hinge','PGSPD',5,L,GAMMA,LAM,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[0.81      , 0.78074074],\n",
       "          [0.80759259, 0.81148148]],\n",
       "\n",
       "         [[0.79592593, 0.785     ],\n",
       "          [0.78944444, 0.77148148]]],\n",
       "\n",
       "\n",
       "        [[[0.78944444, 0.7787037 ],\n",
       "          [0.79555556, 0.79925926]],\n",
       "\n",
       "         [[0.76722222, 0.78666667],\n",
       "          [0.78962963, 0.78388889]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0.78003472, 0.78402778],\n",
       "          [0.75902778, 0.74878472]],\n",
       "\n",
       "         [[0.74826389, 0.77048611],\n",
       "          [0.76510417, 0.73680556]]],\n",
       "\n",
       "\n",
       "        [[[0.75225694, 0.74305556],\n",
       "          [0.76440972, 0.75190972]],\n",
       "\n",
       "         [[0.73350694, 0.75034722],\n",
       "          [0.74809028, 0.73402778]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0.82962561, 0.85530837],\n",
       "          [0.82691264, 0.84861639]],\n",
       "\n",
       "         [[0.79291011, 0.74064026],\n",
       "          [0.77192982, 0.68204015]]],\n",
       "\n",
       "\n",
       "        [[[0.79761259, 0.77482366],\n",
       "          [0.82166757, 0.83034907]],\n",
       "\n",
       "         [[0.76867426, 0.76505697],\n",
       "          [0.71260626, 0.75438596]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0.82871422, 0.85294563],\n",
       "          [0.82886567, 0.832147  ]],\n",
       "\n",
       "         [[0.77222475, 0.7701045 ],\n",
       "          [0.79140795, 0.79524459]]],\n",
       "\n",
       "\n",
       "        [[[0.79786966, 0.78605684],\n",
       "          [0.80968247, 0.80882427]],\n",
       "\n",
       "         [[0.77757585, 0.80549245],\n",
       "          [0.76944823, 0.8070574 ]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[0.8388133 , 0.8021542 ],\n",
       "          [0.81405896, 0.83900227]],\n",
       "\n",
       "         [[0.8021542 , 0.85941043],\n",
       "          [0.78968254, 0.77362056]]],\n",
       "\n",
       "\n",
       "        [[[0.787226  , 0.79176115],\n",
       "          [0.79006047, 0.78155707]],\n",
       "\n",
       "         [[0.78439153, 0.77437642],\n",
       "          [0.78533636, 0.7904384 ]]]]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
