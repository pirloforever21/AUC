{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import torch\n",
    "from math import factorial\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(i,t,prod):\n",
    "    '''\n",
    "    Compute positive function and gradient information\n",
    "    \n",
    "    input:\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fpt - positive function value\n",
    "        gfpt - positive function gradient\n",
    "    '''\n",
    "    fpt = 0.0 \n",
    "    gfpt = 0.0 \n",
    "    fpt = (L/2+prod)**i # no xt yet!\n",
    "    gfpt = i*(L/2+prod)**(i-1) \n",
    "    return fpt,gfpt               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    '''\n",
    "    Compute combination\n",
    "    \n",
    "    input:\n",
    "        n - total number\n",
    "        k - number of chosen\n",
    "    \n",
    "    output:\n",
    "        c - number of combination\n",
    "    '''\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(loss,i,t,prod):\n",
    "    '''\n",
    "    Compute negative function and gradient information\n",
    "    \n",
    "    input:\n",
    "        loss - loss function\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fnt - negative function value\n",
    "        gfnt - negative function gradient\n",
    "    '''\n",
    "    fnt = 0.0 # n stands for negative\n",
    "    gfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        # compute forward difference\n",
    "        delta = 0.0\n",
    "        for j in range(k+1):\n",
    "            delta += (-1)**(k-j)*comb(k,j)*loss(j/N)\n",
    "        # compute coefficient\n",
    "        beta = comb(N,k)*comb(k,i)*(N+1)*delta/(2*L)**k\n",
    "        # compute function value\n",
    "        fnt += beta*(L/2-prod)**(k-i)\n",
    "        # compute gradient\n",
    "        gfnt += beta*(k-i)*(L/2-prod)**(k-i-1)  # no xt yet!\n",
    "    return fnt,gfnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hat(t,yt,ptm1):\n",
    "    '''\n",
    "    Approximate probability\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        yt - label at t\n",
    "        ptm1 - p at t-1\n",
    "    \n",
    "    output:\n",
    "        pt - p at t\n",
    "    '''\n",
    "    pt = (t*ptm1 + (yt+1)/2)/(t+1) # m stands for minus\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_hat(t,fpt,yt,ptm1,atm1):\n",
    "    '''\n",
    "    Approximate primal a\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        atm1 - a at t-1\n",
    "    \n",
    "    output:\n",
    "        at - a at t\n",
    "    '''\n",
    "    at = (fpt*((yt+1)/2) + t*ptm1*atm1)/(t+1) # do not update pt yet!\n",
    "    return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_hat(t,fmt,yt,ptm1,btm1):\n",
    "    '''\n",
    "    Approximate primal b\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fmt - negative function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        btm1 - b at t-1\n",
    "    \n",
    "    output:\n",
    "        bt - b at t-1\n",
    "    '''\n",
    "    bt = (fmt*((-yt+1)/2) + t*(1-ptm1)*btm1)/(t+1) # indicator of y=-1!\n",
    "    return bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_step(t,at,bt):\n",
    "    '''\n",
    "    Compute dual alpha\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        \n",
    "    output:\n",
    "        alphat - alpha at t\n",
    "    '''\n",
    "    alphat = at + bt\n",
    "    return alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(fpt,gfpt,fnt,gfnt,yt,pt,at,bt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to w\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        gfpt - positive function gradient at t\n",
    "        fnt - negative function at t\n",
    "        gfnt - negative function gradient at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        gradwt - gradient w.r.t. w at t\n",
    "    '''\n",
    "    gradwt = 0.0\n",
    "    if yt == 1:\n",
    "        gradwt = (2*alphat*(1-pt) + 2*(1-pt)*(fpt-at) - 2*(1-pt)*fpt)*gfpt\n",
    "    else:\n",
    "        gradwt = (2*alphat*pt + 2*pt*(fnt-bt) - 2*pt*fnt)*gfnt\n",
    "    return gradwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(wt,R):\n",
    "    '''\n",
    "    Projection\n",
    "    \n",
    "    input:\n",
    "        wt - w at t\n",
    "        R - radius\n",
    "        \n",
    "    output:\n",
    "        proj - projected wt\n",
    "    '''\n",
    "    norm = np.linalg.norm(wt)\n",
    "    if norm > R:\n",
    "        wt = wt/norm*R\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(fpt,yt,pt,at):\n",
    "    '''\n",
    "    Gradient with respect to a\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "    \n",
    "    output:\n",
    "        gradat - gradient w.r.t a at t\n",
    "    '''\n",
    "    gradat = 0.0 \n",
    "    if yt == 1:\n",
    "        gradat = -2*(1-pt)*(fpt-at)\n",
    "    else:\n",
    "        pass\n",
    "    return gradat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(fnt,yt,pt,bt):\n",
    "    '''\n",
    "    Gradient with respect to b\n",
    "    \n",
    "    input:\n",
    "        fnt - negative function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        bt - b at t\n",
    "    \n",
    "    output:\n",
    "        gradbt - gradient w.r.t b at t\n",
    "    '''\n",
    "    gradbt = 0.0 \n",
    "    if yt == 1:\n",
    "        pass\n",
    "    else:\n",
    "        gradbt = -2*pt*(fnt-bt)\n",
    "    return gradbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(pt,fpt,fnt,at,bt,alphat,yt):\n",
    "    '''\n",
    "    Compute objective function value\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        pt - \n",
    "        wt - \n",
    "    \n",
    "    output:\n",
    "        F - objective funciton value\n",
    "    '''\n",
    "    F = 0.0\n",
    "    if yt == 1:\n",
    "        F = -pt*(1-pt)*alphat**2 + 2*alphat*(1-pt)*fpt+(1-pt)*(fpt-at)**2 - (1-pt)*fpt**2\n",
    "    else:\n",
    "        F = -pt*(1-pt)*alphat**2 + 2*alphat*pt*fnt+pt*(fnt-bt)**2 - pt*fnt**2\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOLAM(t,batch,loss,pt,wt,at,bt,alphat):\n",
    "    '''\n",
    "    Stochastic Online AUC Maximization step\n",
    "    \n",
    "    input:\n",
    "        T - total number of iteration\n",
    "        F - objective function value\n",
    "        loss - loss function\n",
    "        pt - p at t\n",
    "        wt - w at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        W - record of each wt\n",
    "        A - record of each at\n",
    "        B - record of each bt\n",
    "        ALPHA - record of each alphat\n",
    "    '''\n",
    "    # Loop in the batch\n",
    "    eta = 1/np.sqrt(t+1)/2\n",
    "    for k in range(batch):\n",
    "        \n",
    "        # Update pt\n",
    "        pt = p_hat(t*batch+k,y[(t*batch+k)%M],pt)\n",
    "        # Update wt,at,bt\n",
    "        prod = np.inner(wt,x[(t*batch+k)%M])\n",
    "        fpt = np.zeros(N+1)\n",
    "        gfpt = np.zeros(N+1)\n",
    "        fnt = np.zeros(N+1)\n",
    "        gfnt = np.zeros(N+1)\n",
    "        gradwt = 0.0\n",
    "        gradat = 0.0\n",
    "        gradbt = 0.0\n",
    "        \n",
    "        \n",
    "        for i in range(N+1): # add up info of each i\n",
    "            fpt[i],gfpt[i] = pos(i,t,prod) # partial info\n",
    "            fnt[i],gfnt[i] = neg(loss,i,t,prod)\n",
    "            gradwt += w_grad(fpt[i],gfpt[i],fnt[i],gfnt[i],y[(t*batch+k)%M],pt,at[i],bt[i],alphat[i])\n",
    "            gradat = a_grad(fpt[i],y[(t*batch+k)%M],pt,at[i])\n",
    "            gradbt = b_grad(fnt[i],y[(t*batch+k)%M],pt,bt[i])\n",
    "            at[i] -= eta*gradat/(N+1)/batch\n",
    "            bt[i] -= eta*gradbt/(N+1)/batch\n",
    "            alphat[i] = at[i]+bt[i]\n",
    "            #F += obj(pt,fpt[i],fnt[i],at,bt,alphat,y[(t*batch+k)%T])\n",
    "        \n",
    "        wt -= eta*gradwt*y[(t*batch+k)%M]*x[(t*batch+k)%M]/(N+1)/batch # step size as 1/t gradient descent\n",
    "        \n",
    "    wt = proj(wt,1)    \n",
    "        \n",
    "    return pt,wt,at,bt,alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(filename):\n",
    "    '''\n",
    "    Data file loader\n",
    "    \n",
    "    input:\n",
    "        filename - filename\n",
    "    \n",
    "    output:\n",
    "        x - sample features\n",
    "        y - sample labels\n",
    "    '''\n",
    "    # raw data\n",
    "    L = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in csv.reader(file, delimiter = ' '):\n",
    "            line[0] = '0:'+line[0]\n",
    "            line.remove('')\n",
    "            L.append(dict(i.split(':') for i in line))\n",
    "    df = pd.DataFrame(L,dtype=float).fillna(0)\n",
    "    x = df.iloc[:,1:].values\n",
    "    y = df.iloc[:,0].values\n",
    "    # normalize\n",
    "    norm = np.linalg.norm(x,axis=1)\n",
    "    x = x/norm[:,None]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = loader('a9a')\n",
    "M,d = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 AUC: 0.500000\n",
      "iteration: 2 AUC: 0.500000\n",
      "iteration: 3 AUC: 0.500000\n",
      "iteration: 4 AUC: 0.500000\n",
      "iteration: 5 AUC: 0.500000\n",
      "iteration: 6 AUC: 0.500000\n",
      "iteration: 7 AUC: 0.500000\n",
      "iteration: 8 AUC: 0.500000\n",
      "iteration: 9 AUC: 0.500000\n",
      "iteration: 10 AUC: 0.500000\n",
      "iteration: 11 AUC: 0.500000\n",
      "iteration: 12 AUC: 0.500000\n",
      "iteration: 13 AUC: 0.653242\n",
      "iteration: 14 AUC: 0.674866\n",
      "iteration: 15 AUC: 0.694015\n",
      "iteration: 16 AUC: 0.606959\n",
      "iteration: 17 AUC: 0.661822\n",
      "iteration: 18 AUC: 0.682075\n",
      "iteration: 19 AUC: 0.612790\n",
      "iteration: 20 AUC: 0.617014\n",
      "iteration: 21 AUC: 0.689053\n",
      "iteration: 22 AUC: 0.728127\n",
      "iteration: 23 AUC: 0.681495\n",
      "iteration: 24 AUC: 0.621926\n",
      "iteration: 25 AUC: 0.641689\n",
      "iteration: 26 AUC: 0.704250\n",
      "iteration: 27 AUC: 0.706153\n",
      "iteration: 28 AUC: 0.737107\n",
      "iteration: 29 AUC: 0.719874\n",
      "iteration: 30 AUC: 0.680657\n",
      "iteration: 31 AUC: 0.684608\n",
      "iteration: 32 AUC: 0.696012\n",
      "iteration: 33 AUC: 0.680417\n",
      "iteration: 34 AUC: 0.674329\n",
      "iteration: 35 AUC: 0.666340\n",
      "iteration: 36 AUC: 0.668608\n",
      "iteration: 37 AUC: 0.673673\n",
      "iteration: 38 AUC: 0.681919\n",
      "iteration: 39 AUC: 0.709968\n",
      "iteration: 40 AUC: 0.689896\n",
      "iteration: 41 AUC: 0.674495\n",
      "iteration: 42 AUC: 0.651580\n",
      "iteration: 43 AUC: 0.630102\n",
      "iteration: 44 AUC: 0.635937\n",
      "iteration: 45 AUC: 0.641657\n",
      "iteration: 46 AUC: 0.685192\n",
      "iteration: 47 AUC: 0.671085\n",
      "iteration: 48 AUC: 0.669271\n",
      "iteration: 49 AUC: 0.655235\n",
      "iteration: 50 AUC: 0.655818\n",
      "iteration: 51 AUC: 0.656042\n",
      "iteration: 52 AUC: 0.667032\n",
      "iteration: 53 AUC: 0.688605\n",
      "iteration: 54 AUC: 0.702824\n",
      "iteration: 55 AUC: 0.698198\n",
      "iteration: 56 AUC: 0.726057\n",
      "iteration: 57 AUC: 0.715226\n",
      "iteration: 58 AUC: 0.703911\n",
      "iteration: 59 AUC: 0.689554\n",
      "iteration: 60 AUC: 0.676842\n",
      "iteration: 61 AUC: 0.662201\n",
      "iteration: 62 AUC: 0.663708\n",
      "iteration: 63 AUC: 0.652618\n",
      "iteration: 64 AUC: 0.690785\n",
      "iteration: 65 AUC: 0.690863\n",
      "iteration: 66 AUC: 0.679645\n",
      "iteration: 67 AUC: 0.684921\n",
      "iteration: 68 AUC: 0.692701\n",
      "iteration: 69 AUC: 0.723500\n",
      "iteration: 70 AUC: 0.726485\n",
      "iteration: 71 AUC: 0.729238\n",
      "iteration: 72 AUC: 0.730880\n",
      "iteration: 73 AUC: 0.757340\n",
      "iteration: 74 AUC: 0.758102\n",
      "iteration: 75 AUC: 0.753672\n",
      "iteration: 76 AUC: 0.752933\n",
      "iteration: 77 AUC: 0.744142\n",
      "iteration: 78 AUC: 0.737995\n",
      "iteration: 79 AUC: 0.743803\n",
      "iteration: 80 AUC: 0.737561\n",
      "iteration: 81 AUC: 0.738103\n",
      "iteration: 82 AUC: 0.729825\n",
      "iteration: 83 AUC: 0.727312\n",
      "iteration: 84 AUC: 0.719024\n",
      "iteration: 85 AUC: 0.717959\n",
      "iteration: 86 AUC: 0.719864\n",
      "iteration: 87 AUC: 0.744930\n",
      "iteration: 88 AUC: 0.734127\n",
      "iteration: 89 AUC: 0.734214\n",
      "iteration: 90 AUC: 0.729636\n",
      "iteration: 91 AUC: 0.720349\n",
      "iteration: 92 AUC: 0.722455\n",
      "iteration: 93 AUC: 0.727563\n",
      "iteration: 94 AUC: 0.726518\n",
      "iteration: 95 AUC: 0.751304\n",
      "iteration: 96 AUC: 0.751750\n",
      "iteration: 97 AUC: 0.773419\n",
      "iteration: 98 AUC: 0.792412\n",
      "iteration: 99 AUC: 0.792730\n",
      "iteration: 100 AUC: 0.792503\n"
     ]
    }
   ],
   "source": [
    "L = 2 # range\n",
    "N = 10 # degree\n",
    "T = 100 # iteration\n",
    "\n",
    "hinge = lambda x:max(0,1+L-2*L*x)\n",
    "logistics = lambda x:np.log(1+np.exp(L-2*L*x))\n",
    "\n",
    "pt = 0.0\n",
    "wt = np.zeros(d)\n",
    "at = np.zeros(N+1)\n",
    "bt = np.zeros(N+1)\n",
    "alphat = np.zeros(N+1)\n",
    "\n",
    "roc_auc = np.zeros(T)\n",
    "batch = 1\n",
    "for t in range(T):\n",
    "    pt,wt,at,bt,alphat = SOLAM(t,batch,hinge,pt,wt,at,bt,alphat)\n",
    "    fpr, tpr, _ = roc_curve(y, np.dot(x,wt))\n",
    "    roc_auc[t] = auc(fpr, tpr)\n",
    "    print('iteration: %d AUC: %f' %(t+1,roc_auc[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03103655,  0.00589547,  0.        ,  0.        ,  0.        ,\n",
       "       -0.00595456,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.00338473,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.01564074,\n",
       "       -0.01768592,  0.00167035, -0.00553201, -0.0110721 ,  0.00611444,\n",
       "       -0.02193379, -0.00473598, -0.00766909, -0.02937308,  0.00406984,\n",
       "       -0.00962938, -0.00538307, -0.00563907, -0.00582816,  0.        ,\n",
       "       -0.00128933, -0.00633509,  0.        , -0.00164763,  0.01445163,\n",
       "       -0.00170153,  0.        , -0.02248548, -0.02937308, -0.00473598,\n",
       "       -0.01501246,  0.02334658,  0.01119172,  0.00374704, -0.00873479,\n",
       "       -0.03415343, -0.00577534,  0.        , -0.00168889, -0.00165501,\n",
       "        0.00093232, -0.00401954, -0.01505361, -0.00014671, -0.00484544,\n",
       "       -0.00094273,  0.01124598, -0.0029773 , -0.01544951, -0.00828261,\n",
       "       -0.00638059, -0.00644782,  0.        ,  0.00652167, -0.0611348 ,\n",
       "        0.        ,  0.00232434, -0.02785073,  0.00334934, -0.00968576,\n",
       "       -0.00161952, -0.0147781 , -0.03361409,  0.0018024 , -0.00267509,\n",
       "        0.00430595, -0.00141844, -0.0123552 , -0.00972561, -0.03853482,\n",
       "       -0.04959535,  0.00133493, -0.05077328,  0.00251285, -0.01956802,\n",
       "       -0.0048087 ,  0.00248736, -0.03526536, -0.00208668,  0.01346833,\n",
       "       -0.04221394,  0.        , -0.00161799, -0.00345488,  0.        ,\n",
       "        0.        ,  0.        ,  0.00613158,  0.        ,  0.        ,\n",
       "        0.        ,  0.00255724,  0.        , -0.00141292,  0.        ,\n",
       "        0.00406984,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
