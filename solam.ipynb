{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "#import torch\n",
    "from math import factorial\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(i,prod):\n",
    "    '''\n",
    "    Compute positive function and gradient information\n",
    "    \n",
    "    input:\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fpt - positive function value\n",
    "        gfpt - positive function gradient\n",
    "    '''\n",
    "    fpt = 0.0 \n",
    "    gfpt = 0.0 \n",
    "    fpt = (L/2+prod)**i \n",
    "    gfpt = i*(L/2+prod)**(i-1) # no xt yet!\n",
    "    return fpt,gfpt               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    '''\n",
    "    Compute combination\n",
    "    \n",
    "    input:\n",
    "        n - total number\n",
    "        k - number of chosen\n",
    "    \n",
    "    output:\n",
    "        c - number of combination\n",
    "    '''\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(loss,i,prod):\n",
    "    '''\n",
    "    Compute negative function and gradient information\n",
    "    \n",
    "    input:\n",
    "        loss - loss function\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fnt - negative function value\n",
    "        gfnt - negative function gradient\n",
    "    '''\n",
    "    fnt = 0.0 # n stands for negative\n",
    "    gfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        # compute forward difference\n",
    "        delta = 0.0\n",
    "        for j in range(k+1):\n",
    "            delta += (-1)**(k-j)*comb(k,j)*loss(j/N)\n",
    "        # compute coefficient\n",
    "        beta = comb(N,k)*comb(k,i)*(N+1)*delta/(2*L)**k\n",
    "        # compute function value\n",
    "        fnt += beta*(L/2-prod)**(k-i)\n",
    "        # compute gradient\n",
    "        gfnt += beta*(k-i)*(L/2-prod)**(k-i-1)  # no xt yet!\n",
    "    return fnt,gfnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hat(t,yt,ptm1):\n",
    "    '''\n",
    "    Approximate probability\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        yt - label at t\n",
    "        ptm1 - p at t-1\n",
    "    \n",
    "    output:\n",
    "        pt - p at t\n",
    "    '''\n",
    "    pt = (t*ptm1 + (yt+1)/2)/(t+1) # m stands for minus\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_hat(t,fpt,yt,ptm1,atm1):\n",
    "    '''\n",
    "    Approximate primal a\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        atm1 - a at t-1\n",
    "    \n",
    "    output:\n",
    "        at - a at t\n",
    "    '''\n",
    "    at = (fpt*((yt+1)/2) + t*ptm1*atm1)/(t+1) # do not update pt yet!\n",
    "    return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_hat(t,fmt,yt,ptm1,btm1):\n",
    "    '''\n",
    "    Approximate primal b\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fmt - negative function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        btm1 - b at t-1\n",
    "    \n",
    "    output:\n",
    "        bt - b at t-1\n",
    "    '''\n",
    "    bt = (fmt*((-yt+1)/2) + t*(1-ptm1)*btm1)/(t+1) # indicator of y=-1!\n",
    "    return bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_step(t,at,bt):\n",
    "    '''\n",
    "    Compute dual alpha\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        \n",
    "    output:\n",
    "        alphat - alpha at t\n",
    "    '''\n",
    "    alphat = at + bt\n",
    "    return alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(gfpt,gfnt,yt,at,bt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to w\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        gfpt - positive function gradient at t\n",
    "        fnt - negative function at t\n",
    "        gfnt - negative function gradient at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        gradwt - gradient w.r.t. w at t\n",
    "    '''\n",
    "    gradwt = 0.0\n",
    "    if yt == 1:\n",
    "        gradwt = 2*(alphat - at)*gfpt\n",
    "    else:\n",
    "        gradwt = 2*(alphat - bt)*gfnt\n",
    "    return gradwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(wt,R):\n",
    "    '''\n",
    "    Projection\n",
    "    \n",
    "    input:\n",
    "        wt - w at t\n",
    "        R - radius\n",
    "        \n",
    "    output:\n",
    "        proj - projected wt\n",
    "    '''\n",
    "    norm = np.linalg.norm(wt)\n",
    "    if norm > R:\n",
    "        wt = wt/norm*R\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(fpt,yt,at):\n",
    "    '''\n",
    "    Gradient with respect to a\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "    \n",
    "    output:\n",
    "        gradat - gradient w.r.t a at t\n",
    "    '''\n",
    "    gradat = 0.0 \n",
    "    if yt == 1:\n",
    "        gradat = 2*(at - fpt)\n",
    "    else:\n",
    "        gradat = 2*at\n",
    "    return gradat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(fnt,yt,bt):\n",
    "    '''\n",
    "    Gradient with respect to b\n",
    "    \n",
    "    input:\n",
    "        fnt - negative function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        bt - b at t\n",
    "    \n",
    "    output:\n",
    "        gradbt - gradient w.r.t b at t\n",
    "    '''\n",
    "    gradbt = 0.0 \n",
    "    if yt == 1:\n",
    "        gradbt = 2*bt\n",
    "    else:\n",
    "        gradbt = 2*(bt - fnt)\n",
    "    return gradbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_grad(fpt,fnt,yt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to alpha\n",
    "    '''\n",
    "    gradalphat = 0.0\n",
    "    if yt == 1:\n",
    "        gradalphat = -2*(alphat - fpt)\n",
    "    else:\n",
    "        gradalphat = -2*(alphat - fnt)\n",
    "    return gradalphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(fpt,fnt,at,bt,alphat,yt):\n",
    "    '''\n",
    "    Compute objective function value\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        pt - \n",
    "        wt - \n",
    "    \n",
    "    output:\n",
    "        F - objective funciton value\n",
    "    '''\n",
    "    F = 0.0\n",
    "    if yt == 1:\n",
    "        F = (-pt*alphat**2 + 2*alphat*fpt+(fpt-at)**2 - fpt**2)\n",
    "    else:\n",
    "        F = pt*(-(1-pt)*alphat**2 + 2*alphat*fnt+(fnt-bt)**2 - fnt**2)\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(filename):\n",
    "    '''\n",
    "    Data file loader\n",
    "    \n",
    "    input:\n",
    "        filename - filename\n",
    "    \n",
    "    output:\n",
    "        x - sample features\n",
    "        y - sample labels\n",
    "    '''\n",
    "    # raw data\n",
    "    L = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in csv.reader(file, delimiter = ' '):\n",
    "            line[0] = '0:'+line[0]\n",
    "            line.remove('')\n",
    "            L.append(dict(i.split(':') for i in line))\n",
    "    df = pd.DataFrame(L,dtype=float).fillna(0)\n",
    "    X = df.iloc[:,1:].values\n",
    "    Y = df.iloc[:,0].values\n",
    "    # normalize\n",
    "    norm = np.linalg.norm(X,axis=1)\n",
    "    X = X/norm[:,None]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox(eta,loss,wj,aj,bj,alphaj,bwt,bat,bbt,balphat,x,y):\n",
    "    '''\n",
    "    perform proximal guided gradient descent when receive an sample\n",
    "    '''\n",
    "    prod = np.inner(wj,x)\n",
    "    fpt = np.zeros(N+1)\n",
    "    gfpt = np.zeros(N+1)\n",
    "    fnt = np.zeros(N+1)\n",
    "    gfnt = np.zeros(N+1)\n",
    "    aJ = np.zeros(N+1)\n",
    "    bJ = np.zeros(N+1)\n",
    "    alphaJ = np.zeros(N+1)\n",
    "    gradwt = 0.0\n",
    "    gradat = 0.0\n",
    "    gradbt = 0.0\n",
    "    for i in range(N+1):\n",
    "        fpt[i],gfpt[i] = pos(i,prod)\n",
    "        fnt[i],gfnt[i] = neg(loss,i,prod)\n",
    "        gradwt += w_grad(gfpt[i],gfnt[i],y,aj[i],bj[i],alphaj[i]) # accumulate i\n",
    "        gradat = a_grad(fpt[i],y,aj[i])\n",
    "        gradbt = b_grad(fnt[i],y,bj[i])\n",
    "        gradalphat = alpha_grad(fpt[i],fnt[i],y,alphaj[i])\n",
    "        aJ[i] = aj[i] - eta*(gradat/(N+1)+gamma*(aj[i]-bat[i]))\n",
    "        bJ[i] = bj[i] - eta*(gradbt/(N+1)+gamma*(bj[i]-bbt[i]))\n",
    "        #alphaJ[i] = aJ[i] + bJ[i]\n",
    "        alphaJ[i] = alphaj[i] + eta*gradalphat/(N+1)\n",
    "    wJ = wj - eta*(gradwt*x/(N+1) + wj + gamma*(wj - bwt))\n",
    "    wJ = proj(wJ,1)\n",
    "    #aJ = proj(aJ,1)\n",
    "    #bJ = proj(bJ,1)\n",
    "    #alphaJ = proj(alphaJ,1)\n",
    "    \n",
    "    return wJ,aJ,bJ,alphaJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGSPD(t,loss,X,Y,bwt,bat,bbt,balphat):\n",
    "    '''\n",
    "    Proximally Guided Stochastic Primal Dual Algorithm\n",
    "    '''\n",
    "    \n",
    "    # initialize inner loop variables\n",
    "    Wt = bwt+0.0\n",
    "    At = bat+0.0\n",
    "    Bt = bbt+0.0\n",
    "    ALPHAt = balphat+0.0\n",
    "    \n",
    "    BWt = np.zeros(d)\n",
    "    BAt = np.zeros(N+1)\n",
    "    BBt = np.zeros(N+1)\n",
    "    BALPHAt = np.zeros(N+1)\n",
    "    \n",
    "    ETAt = np.zeros(t).reshape((t,1))\n",
    "    # inner loop update at j\n",
    "    for j in range(1,t+1): \n",
    "        # step size\n",
    "        ETAt[j-1] = 1/np.sqrt(t) # M is the bound for gradient\n",
    "        BWt += ETAt[j-1]*Wt\n",
    "        BAt += ETAt[j-1]*At\n",
    "        BBt += ETAt[j-1]*Bt\n",
    "        BALPHAt += ETAt[j-1]*ALPHAt\n",
    "        # update inner loop variables\n",
    "        Wt,At,Bt,ALPHAt = prox(ETAt[j-1],loss,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat,X[j-1,:],Y[j-1])\n",
    "        \n",
    "    # update outer loop variables\n",
    "    bwt = Wt\n",
    "    bat = At\n",
    "    bbt = Bt\n",
    "    balphat = ALPHAt\n",
    "    #bwt = BWt/sum(ETAt)\n",
    "    #bat = BAT/sum(ETAt)\n",
    "    #bbt = BBt/sum(ETAt)\n",
    "    #balphat = BALPHAt/sum(ETAt)\n",
    "    \n",
    "    return bwt,bat,bbt,balphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinge = lambda x:max(0,1+L-2*L*x)\n",
    "logistics = lambda x:np.log(1+np.exp(L-2*L*x))\n",
    "\n",
    "L = 2\n",
    "N = 100\n",
    "M,_ = neg(hinge,0,0) # weak convexity parameter\n",
    "rho = M*N*N*N\n",
    "gamma = 0.0\n",
    "M = rho/N\n",
    "T = 100 # DO NOT make it longer than n!\n",
    "\n",
    "FEATURES,LABELS = loader('diabetes')\n",
    "\n",
    "# get dimensions of the data\n",
    "n,d = FEATURES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 AUC: 0.500000\n",
      "iteration: 2 AUC: 0.500000\n",
      "iteration: 3 AUC: 0.689858\n",
      "iteration: 4 AUC: 0.679224\n",
      "iteration: 5 AUC: 0.431493\n",
      "iteration: 6 AUC: 0.618590\n",
      "iteration: 7 AUC: 0.751806\n",
      "iteration: 8 AUC: 0.332216\n",
      "iteration: 9 AUC: 0.750858\n",
      "iteration: 10 AUC: 0.311746\n",
      "iteration: 11 AUC: 0.504970\n",
      "iteration: 12 AUC: 0.675187\n",
      "iteration: 13 AUC: 0.227455\n",
      "iteration: 14 AUC: 0.273515\n",
      "iteration: 15 AUC: 0.323687\n",
      "iteration: 16 AUC: 0.745836\n",
      "iteration: 17 AUC: 0.399127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-347dc7534259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# update outer loop variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mBWT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBAT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBBT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBALPHAT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPGSPD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhinge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBWT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBAT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBBT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBALPHAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBWT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-1e5d84842398>\u001b[0m in \u001b[0;36mPGSPD\u001b[0;34m(t, loss, X, Y, bwt, bat, bbt, balphat)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mBALPHAt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mETAt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mALPHAt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# update inner loop variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mWt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mALPHAt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mETAt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mALPHAt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbwt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbbt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbalphat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# update outer loop variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-561174c0e49d>\u001b[0m in \u001b[0;36mprox\u001b[0;34m(eta, loss, wj, aj, bj, alphaj, bwt, bat, bbt, balphat, x, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgfpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mfnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgfnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mgradwt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgfnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malphaj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# accumulate i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mgradat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3ab4eae3a1aa>\u001b[0m in \u001b[0;36mneg\u001b[0;34m(loss, i, prod)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mdelta\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# compute coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-112b5ce5af1a>\u001b[0m in \u001b[0;36mcomb\u001b[0;34m(n, k)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcombination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     '''\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize outer loop variables\n",
    "BWT = np.zeros(d) # d is the dimension of the features\n",
    "BAT = np.zeros(N+1)\n",
    "BBT = np.zeros(N+1)\n",
    "BALPHAT = np.zeros(N+1)\n",
    "\n",
    "roc_auc = np.zeros(T)\n",
    "for t in range(1,T+1):\n",
    "    # sample a point\n",
    "    index = np.random.randint(n, size=t)\n",
    "    #start = (t*(t-1)//2)%n\n",
    "    #end = (t*(t+1)//2)%n\n",
    "    features = FEATURES[index,:]\n",
    "    labels = LABELS[index]\n",
    "    # update outer loop variables\n",
    "    BWT,BAT,BBT,BALPHAT = PGSPD(t,hinge,features,labels,BWT,BAT,BBT,BALPHAT)\n",
    "    fpr, tpr, _ = roc_curve(LABELS, np.dot(FEATURES,BWT))\n",
    "    roc_auc[t-1] = auc(fpr, tpr)\n",
    "    print('iteration: %d AUC: %f' %(t,roc_auc[t-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOLAM(t,batch,loss,wt,at,bt,alphat):\n",
    "    '''\n",
    "    Stochastic Online AUC Maximization step\n",
    "    \n",
    "    input:\n",
    "        T - total number of iteration\n",
    "        F - objective function value\n",
    "        loss - loss function\n",
    "        pt - p at t\n",
    "        wt - w at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        W - record of each wt\n",
    "        A - record of each at\n",
    "        B - record of each bt\n",
    "        ALPHA - record of each alphat\n",
    "    '''\n",
    "    # Loop in the batch\n",
    "    eta = 1/np.sqrt(t+1)/2\n",
    "    for k in range(batch):\n",
    "        \n",
    "        # Update pt\n",
    "        #pt = p_hat(t*batch+k,y[(t*batch+k)%M],pt)\n",
    "        # Update wt,at,bt\n",
    "        prod = np.inner(wt,FEATURES[(t*batch+k)%n])\n",
    "        fpt = np.zeros(N+1)\n",
    "        gfpt = np.zeros(N+1)\n",
    "        fnt = np.zeros(N+1)\n",
    "        gfnt = np.zeros(N+1)\n",
    "        gradwt = 0.0\n",
    "        gradat = 0.0\n",
    "        gradbt = 0.0\n",
    "        \n",
    "        \n",
    "        for i in range(N+1): # add up info of each i\n",
    "            fpt[i],gfpt[i] = pos(i,prod) # partial info\n",
    "            fnt[i],gfnt[i] = neg(loss,i,prod)\n",
    "            gradwt += w_grad(gfpt[i],gfnt[i],LABELS[(t*batch+k)%n],at[i],bt[i],alphat[i])\n",
    "            gradat = a_grad(fpt[i],LABELS[(t*batch+k)%n],at[i])\n",
    "            gradbt = b_grad(fnt[i],LABELS[(t*batch+k)%n],bt[i])\n",
    "            at[i] -= eta*gradat/(N+1)/batch\n",
    "            bt[i] -= eta*gradbt/(N+1)/batch\n",
    "            alphat[i] = at[i]+bt[i]\n",
    "            #F += obj(pt,fpt[i],fnt[i],at,bt,alphat,y[(t*batch+k)%T])\n",
    "        \n",
    "        wt -= eta*gradwt*FEATURES[(t*batch+k)%n]/(N+1)/batch # step size as 1/t gradient descent\n",
    "        \n",
    "    wt = proj(wt,1)    \n",
    "        \n",
    "    return wt,at,bt,alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 AUC: 0.500000\n",
      "iteration: 2 AUC: 0.768634\n",
      "iteration: 3 AUC: 0.713828\n",
      "iteration: 4 AUC: 0.749388\n",
      "iteration: 5 AUC: 0.714515\n",
      "iteration: 6 AUC: 0.721552\n",
      "iteration: 7 AUC: 0.738448\n",
      "iteration: 8 AUC: 0.730821\n",
      "iteration: 9 AUC: 0.708731\n",
      "iteration: 10 AUC: 0.704306\n",
      "iteration: 11 AUC: 0.701522\n",
      "iteration: 12 AUC: 0.675299\n",
      "iteration: 13 AUC: 0.637269\n",
      "iteration: 14 AUC: 0.622440\n",
      "iteration: 15 AUC: 0.603261\n",
      "iteration: 16 AUC: 0.612127\n",
      "iteration: 17 AUC: 0.620940\n",
      "iteration: 18 AUC: 0.624963\n",
      "iteration: 19 AUC: 0.639896\n",
      "iteration: 20 AUC: 0.646306\n",
      "iteration: 21 AUC: 0.651321\n",
      "iteration: 22 AUC: 0.645843\n",
      "iteration: 23 AUC: 0.630746\n",
      "iteration: 24 AUC: 0.632179\n",
      "iteration: 25 AUC: 0.619515\n",
      "iteration: 26 AUC: 0.615978\n",
      "iteration: 27 AUC: 0.609433\n",
      "iteration: 28 AUC: 0.624045\n",
      "iteration: 29 AUC: 0.606254\n",
      "iteration: 30 AUC: 0.605545\n",
      "iteration: 31 AUC: 0.599657\n",
      "iteration: 32 AUC: 0.601209\n",
      "iteration: 33 AUC: 0.614537\n",
      "iteration: 34 AUC: 0.620545\n",
      "iteration: 35 AUC: 0.613269\n",
      "iteration: 36 AUC: 0.626664\n",
      "iteration: 37 AUC: 0.618687\n",
      "iteration: 38 AUC: 0.615396\n",
      "iteration: 39 AUC: 0.627172\n",
      "iteration: 40 AUC: 0.623575\n",
      "iteration: 41 AUC: 0.620978\n",
      "iteration: 42 AUC: 0.614634\n",
      "iteration: 43 AUC: 0.613172\n",
      "iteration: 44 AUC: 0.590149\n",
      "iteration: 45 AUC: 0.584716\n",
      "iteration: 46 AUC: 0.583157\n",
      "iteration: 47 AUC: 0.586881\n",
      "iteration: 48 AUC: 0.602201\n",
      "iteration: 49 AUC: 0.606970\n",
      "iteration: 50 AUC: 0.617328\n",
      "iteration: 51 AUC: 0.626791\n",
      "iteration: 52 AUC: 0.636060\n",
      "iteration: 53 AUC: 0.643015\n",
      "iteration: 54 AUC: 0.621985\n",
      "iteration: 55 AUC: 0.613216\n",
      "iteration: 56 AUC: 0.626097\n",
      "iteration: 57 AUC: 0.611672\n",
      "iteration: 58 AUC: 0.619343\n",
      "iteration: 59 AUC: 0.614701\n",
      "iteration: 60 AUC: 0.623433\n",
      "iteration: 61 AUC: 0.635276\n",
      "iteration: 62 AUC: 0.630075\n",
      "iteration: 63 AUC: 0.639530\n",
      "iteration: 64 AUC: 0.643582\n",
      "iteration: 65 AUC: 0.640791\n",
      "iteration: 66 AUC: 0.644925\n",
      "iteration: 67 AUC: 0.648955\n",
      "iteration: 68 AUC: 0.645015\n",
      "iteration: 69 AUC: 0.651985\n",
      "iteration: 70 AUC: 0.652045\n",
      "iteration: 71 AUC: 0.659440\n",
      "iteration: 72 AUC: 0.661067\n",
      "iteration: 73 AUC: 0.648881\n",
      "iteration: 74 AUC: 0.650619\n",
      "iteration: 75 AUC: 0.658134\n",
      "iteration: 76 AUC: 0.671851\n",
      "iteration: 77 AUC: 0.674030\n",
      "iteration: 78 AUC: 0.677052\n",
      "iteration: 79 AUC: 0.679448\n",
      "iteration: 80 AUC: 0.682791\n",
      "iteration: 81 AUC: 0.685739\n",
      "iteration: 82 AUC: 0.692164\n",
      "iteration: 83 AUC: 0.694463\n",
      "iteration: 84 AUC: 0.698560\n",
      "iteration: 85 AUC: 0.690440\n",
      "iteration: 86 AUC: 0.693306\n",
      "iteration: 87 AUC: 0.690522\n",
      "iteration: 88 AUC: 0.693104\n",
      "iteration: 89 AUC: 0.682179\n",
      "iteration: 90 AUC: 0.685396\n",
      "iteration: 91 AUC: 0.689716\n",
      "iteration: 92 AUC: 0.689112\n",
      "iteration: 93 AUC: 0.689560\n",
      "iteration: 94 AUC: 0.682493\n",
      "iteration: 95 AUC: 0.683500\n",
      "iteration: 96 AUC: 0.679828\n",
      "iteration: 97 AUC: 0.683985\n",
      "iteration: 98 AUC: 0.689933\n",
      "iteration: 99 AUC: 0.693560\n",
      "iteration: 100 AUC: 0.695179\n",
      "iteration: 101 AUC: 0.688709\n",
      "iteration: 102 AUC: 0.688552\n",
      "iteration: 103 AUC: 0.689567\n",
      "iteration: 104 AUC: 0.693851\n",
      "iteration: 105 AUC: 0.695388\n",
      "iteration: 106 AUC: 0.697933\n",
      "iteration: 107 AUC: 0.698701\n",
      "iteration: 108 AUC: 0.697604\n",
      "iteration: 109 AUC: 0.701022\n",
      "iteration: 110 AUC: 0.704813\n",
      "iteration: 111 AUC: 0.700888\n",
      "iteration: 112 AUC: 0.690030\n",
      "iteration: 113 AUC: 0.693866\n",
      "iteration: 114 AUC: 0.696269\n",
      "iteration: 115 AUC: 0.687313\n",
      "iteration: 116 AUC: 0.677687\n",
      "iteration: 117 AUC: 0.674000\n",
      "iteration: 118 AUC: 0.677082\n",
      "iteration: 119 AUC: 0.680604\n",
      "iteration: 120 AUC: 0.683634\n",
      "iteration: 121 AUC: 0.681679\n",
      "iteration: 122 AUC: 0.683328\n",
      "iteration: 123 AUC: 0.685843\n",
      "iteration: 124 AUC: 0.681522\n",
      "iteration: 125 AUC: 0.684030\n",
      "iteration: 126 AUC: 0.689731\n",
      "iteration: 127 AUC: 0.689769\n",
      "iteration: 128 AUC: 0.691716\n",
      "iteration: 129 AUC: 0.691425\n",
      "iteration: 130 AUC: 0.688851\n",
      "iteration: 131 AUC: 0.682261\n",
      "iteration: 132 AUC: 0.678619\n",
      "iteration: 133 AUC: 0.674313\n",
      "iteration: 134 AUC: 0.675940\n",
      "iteration: 135 AUC: 0.679761\n",
      "iteration: 136 AUC: 0.680664\n",
      "iteration: 137 AUC: 0.684373\n",
      "iteration: 138 AUC: 0.688604\n",
      "iteration: 139 AUC: 0.688045\n",
      "iteration: 140 AUC: 0.690284\n",
      "iteration: 141 AUC: 0.688134\n",
      "iteration: 142 AUC: 0.687813\n",
      "iteration: 143 AUC: 0.690269\n",
      "iteration: 144 AUC: 0.687291\n",
      "iteration: 145 AUC: 0.686925\n",
      "iteration: 146 AUC: 0.691030\n",
      "iteration: 147 AUC: 0.694090\n",
      "iteration: 148 AUC: 0.695933\n",
      "iteration: 149 AUC: 0.690963\n",
      "iteration: 150 AUC: 0.693530\n",
      "iteration: 151 AUC: 0.694709\n",
      "iteration: 152 AUC: 0.694545\n",
      "iteration: 153 AUC: 0.681164\n",
      "iteration: 154 AUC: 0.680993\n",
      "iteration: 155 AUC: 0.667276\n",
      "iteration: 156 AUC: 0.659164\n",
      "iteration: 157 AUC: 0.663724\n",
      "iteration: 158 AUC: 0.667552\n",
      "iteration: 159 AUC: 0.671701\n",
      "iteration: 160 AUC: 0.656687\n",
      "iteration: 161 AUC: 0.655224\n",
      "iteration: 162 AUC: 0.655284\n",
      "iteration: 163 AUC: 0.657955\n",
      "iteration: 164 AUC: 0.661530\n",
      "iteration: 165 AUC: 0.661478\n",
      "iteration: 166 AUC: 0.661873\n",
      "iteration: 167 AUC: 0.662216\n",
      "iteration: 168 AUC: 0.662075\n",
      "iteration: 169 AUC: 0.663007\n",
      "iteration: 170 AUC: 0.664276\n",
      "iteration: 171 AUC: 0.664216\n",
      "iteration: 172 AUC: 0.662440\n",
      "iteration: 173 AUC: 0.667746\n",
      "iteration: 174 AUC: 0.672201\n",
      "iteration: 175 AUC: 0.676418\n",
      "iteration: 176 AUC: 0.665694\n",
      "iteration: 177 AUC: 0.665993\n",
      "iteration: 178 AUC: 0.664493\n",
      "iteration: 179 AUC: 0.660000\n",
      "iteration: 180 AUC: 0.655127\n",
      "iteration: 181 AUC: 0.657097\n",
      "iteration: 182 AUC: 0.659440\n",
      "iteration: 183 AUC: 0.667672\n",
      "iteration: 184 AUC: 0.670761\n",
      "iteration: 185 AUC: 0.669209\n",
      "iteration: 186 AUC: 0.656127\n",
      "iteration: 187 AUC: 0.640649\n",
      "iteration: 188 AUC: 0.642470\n",
      "iteration: 189 AUC: 0.645866\n",
      "iteration: 190 AUC: 0.646299\n",
      "iteration: 191 AUC: 0.649157\n",
      "iteration: 192 AUC: 0.647537\n",
      "iteration: 193 AUC: 0.641403\n",
      "iteration: 194 AUC: 0.636097\n",
      "iteration: 195 AUC: 0.638470\n",
      "iteration: 196 AUC: 0.634851\n",
      "iteration: 197 AUC: 0.638940\n",
      "iteration: 198 AUC: 0.645075\n",
      "iteration: 199 AUC: 0.651724\n",
      "iteration: 200 AUC: 0.651769\n"
     ]
    }
   ],
   "source": [
    "#pt = 0.0\n",
    "wt = np.zeros(d)\n",
    "at = np.zeros(N+1)\n",
    "bt = np.zeros(N+1)\n",
    "alphat = np.zeros(N+1)\n",
    "\n",
    "roc_auc = np.zeros(T)\n",
    "batch = 1\n",
    "for t in range(T):\n",
    "    wt,at,bt,alphat = SOLAM(t,batch,hinge,wt,at,bt,alphat)\n",
    "    fpr, tpr, _ = roc_curve(LABELS, np.dot(FEATURES,wt))\n",
    "    roc_auc[t] = auc(fpr, tpr)\n",
    "    print('iteration: %d AUC: %f' %(t+1,roc_auc[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
