{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import torch\n",
    "from math import factorial\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(i,t,prod):\n",
    "    '''\n",
    "    Compute positive function and gradient information\n",
    "    \n",
    "    input:\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fpt - positive function value\n",
    "        gfpt - positive function gradient\n",
    "    '''\n",
    "    fpt = 0.0 \n",
    "    gfpt = 0.0 \n",
    "    fpt = (L/2+prod)**i # no xt yet!\n",
    "    gfpt = i*(L/2+prod)**(i-1) \n",
    "    return fpt,gfpt               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    '''\n",
    "    Compute combination\n",
    "    \n",
    "    input:\n",
    "        n - total number\n",
    "        k - number of chosen\n",
    "    \n",
    "    output:\n",
    "        c - number of combination\n",
    "    '''\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(loss,i,t,prod):\n",
    "    '''\n",
    "    Compute negative function and gradient information\n",
    "    \n",
    "    input:\n",
    "        loss - loss function\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fnt - negative function value\n",
    "        gfnt - negative function gradient\n",
    "    '''\n",
    "    fnt = 0.0 # n stands for negative\n",
    "    gfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        # compute forward difference\n",
    "        delta = 0.0\n",
    "        for j in range(k+1):\n",
    "            delta += (-1)**(k-j)*comb(k,j)*loss(j/N)\n",
    "        # compute coefficient\n",
    "        beta = comb(N,k)*comb(k,i)*(N+1)*delta/(2*L)**k\n",
    "        # compute function value\n",
    "        fnt += beta*(L/2-prod)**(k-i)\n",
    "        # compute gradient\n",
    "        gfnt += beta*(k-i)*(L/2-prod)**(k-i-1)  # no xt yet!\n",
    "    return fnt,gfnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hat(t,yt,ptm1):\n",
    "    '''\n",
    "    Approximate probability\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        yt - label at t\n",
    "        ptm1 - p at t-1\n",
    "    \n",
    "    output:\n",
    "        pt - p at t\n",
    "    '''\n",
    "    pt = (t*ptm1 + (yt+1)/2)/(t+1) # m stands for minus\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_hat(t,fpt,yt,ptm1,atm1):\n",
    "    '''\n",
    "    Approximate primal a\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        atm1 - a at t-1\n",
    "    \n",
    "    output:\n",
    "        at - a at t\n",
    "    '''\n",
    "    at = (fpt*((yt+1)/2) + t*ptm1*atm1)/(t+1) # do not update pt yet!\n",
    "    return at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_hat(t,fmt,yt,ptm1,btm1):\n",
    "    '''\n",
    "    Approximate primal b\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        fmt - negative function at t\n",
    "        yt - sample label at t\n",
    "        ptm1 - p at t-1\n",
    "        btm1 - b at t-1\n",
    "    \n",
    "    output:\n",
    "        bt - b at t-1\n",
    "    '''\n",
    "    bt = (fmt*((-yt+1)/2) + t*(1-ptm1)*btm1)/(t+1) # indicator of y=-1!\n",
    "    return bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_step(t,at,bt):\n",
    "    '''\n",
    "    Compute dual alpha\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        \n",
    "    output:\n",
    "        alphat - alpha at t\n",
    "    '''\n",
    "    alphat = at + bt\n",
    "    return alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(fpt,gfpt,fnt,gfnt,yt,pt,at,bt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to w\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        gfpt - positive function gradient at t\n",
    "        fnt - negative function at t\n",
    "        gfnt - negative function gradient at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        gradwt - gradient w.r.t. w at t\n",
    "    '''\n",
    "    gradwt = 0.0\n",
    "    if yt == 1:\n",
    "        gradwt = (2*alphat*(1-pt) + 2*(1-pt)*(fpt-at) - 2*(1-pt)*fpt)*gfpt\n",
    "    else:\n",
    "        gradwt = (2*alphat*pt + 2*pt*(fnt-bt) - 2*pt*fnt)*gfnt\n",
    "    return gradwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(wt,R):\n",
    "    '''\n",
    "    Projection\n",
    "    \n",
    "    input:\n",
    "        wt - w at t\n",
    "        R - radius\n",
    "        \n",
    "    output:\n",
    "        proj - projected wt\n",
    "    '''\n",
    "    norm = np.linalg.norm(wt)\n",
    "    if norm > R:\n",
    "        wt = wt/norm*R\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(fpt,yt,pt,at):\n",
    "    '''\n",
    "    Gradient with respect to a\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "    \n",
    "    output:\n",
    "        gradat - gradient w.r.t a at t\n",
    "    '''\n",
    "    gradat = 0.0 \n",
    "    if yt == 1:\n",
    "        gradat = -2*(1-pt)*(fpt-at)\n",
    "    else:\n",
    "        pass\n",
    "    return gradat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(fnt,yt,pt,bt):\n",
    "    '''\n",
    "    Gradient with respect to b\n",
    "    \n",
    "    input:\n",
    "        fnt - negative function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        bt - b at t\n",
    "    \n",
    "    output:\n",
    "        gradbt - gradient w.r.t b at t\n",
    "    '''\n",
    "    gradbt = 0.0 \n",
    "    if yt == 1:\n",
    "        pass\n",
    "    else:\n",
    "        gradbt = -2*pt*(fnt-bt)\n",
    "    return gradbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(pt,fpt,fnt,at,bt,alphat,yt):\n",
    "    '''\n",
    "    Compute objective function value\n",
    "    \n",
    "    input:\n",
    "        t - iteration\n",
    "        pt - \n",
    "        wt - \n",
    "    \n",
    "    output:\n",
    "        F - objective funciton value\n",
    "    '''\n",
    "    F = 0.0\n",
    "    if yt == 1:\n",
    "        F = -pt*(1-pt)*alphat**2 + 2*alphat*(1-pt)*fpt+(1-pt)*(fpt-at)**2 - (1-pt)*fpt**2\n",
    "    else:\n",
    "        F = -pt*(1-pt)*alphat**2 + 2*alphat*pt*fnt+pt*(fnt-bt)**2 - pt*fnt**2\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOLAM(t,batch,loss,pt,wt,at,bt,alphat):\n",
    "    '''\n",
    "    Stochastic Online AUC Maximization step\n",
    "    \n",
    "    input:\n",
    "        T - total number of iteration\n",
    "        F - objective function value\n",
    "        loss - loss function\n",
    "        pt - p at t\n",
    "        wt - w at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        W - record of each wt\n",
    "        A - record of each at\n",
    "        B - record of each bt\n",
    "        ALPHA - record of each alphat\n",
    "    '''\n",
    "    # Loop in the batch\n",
    "    eta = 1/np.sqrt(t+1)/2\n",
    "    for k in range(batch):\n",
    "        \n",
    "        # Update pt\n",
    "        pt = p_hat(t*batch+k,y[(t*batch+k)%M],pt)\n",
    "        # Update wt,at,bt\n",
    "        prod = np.inner(wt,x[(t*batch+k)%M])\n",
    "        fpt = np.zeros(N+1)\n",
    "        gfpt = np.zeros(N+1)\n",
    "        fnt = np.zeros(N+1)\n",
    "        gfnt = np.zeros(N+1)\n",
    "        gradwt = 0.0\n",
    "        gradat = 0.0\n",
    "        gradbt = 0.0\n",
    "        \n",
    "        \n",
    "        for i in range(N+1): # add up info of each i\n",
    "            fpt[i],gfpt[i] = pos(i,t,prod) # partial info\n",
    "            fnt[i],gfnt[i] = neg(loss,i,t,prod)\n",
    "            gradwt += w_grad(fpt[i],gfpt[i],fnt[i],gfnt[i],y[(t*batch+k)%M],pt,at[i],bt[i],alphat[i])\n",
    "            gradat = a_grad(fpt[i],y[(t*batch+k)%M],pt,at[i])\n",
    "            gradbt = b_grad(fnt[i],y[(t*batch+k)%M],pt,bt[i])\n",
    "            at[i] -= eta*gradat/(N+1)/batch\n",
    "            bt[i] -= eta*gradbt/(N+1)/batch\n",
    "            alphat[i] = at[i]+bt[i]\n",
    "            #F += obj(pt,fpt[i],fnt[i],at,bt,alphat,y[(t*batch+k)%T])\n",
    "        \n",
    "        wt -= eta*gradwt*y[(t*batch+k)%M]*x[(t*batch+k)%M]/(N+1)/batch # step size as 1/t gradient descent\n",
    "        \n",
    "    wt = proj(wt,1)    \n",
    "        \n",
    "    return pt,wt,at,bt,alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(filename):\n",
    "    '''\n",
    "    Data file loader\n",
    "    \n",
    "    input:\n",
    "        filename - filename\n",
    "    \n",
    "    output:\n",
    "        x - sample features\n",
    "        y - sample labels\n",
    "    '''\n",
    "    # raw data\n",
    "    L = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in csv.reader(file, delimiter = ' '):\n",
    "            line[0] = '0:'+line[0]\n",
    "            line.remove('')\n",
    "            L.append(dict(i.split(':') for i in line))\n",
    "    df = pd.DataFrame(L,dtype=float).fillna(0)\n",
    "    x = df.iloc[:,1:].values\n",
    "    y = df.iloc[:,0].values\n",
    "    # normalize\n",
    "    norm = np.linalg.norm(x,axis=1)\n",
    "    x = x/norm[:,None]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = loader('diabetes')\n",
    "M,d = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 AUC: 0.500000\n",
      "iteration: 2 AUC: 0.500000\n",
      "iteration: 3 AUC: 0.568955\n",
      "iteration: 4 AUC: 0.716127\n",
      "iteration: 5 AUC: 0.571448\n",
      "iteration: 6 AUC: 0.638388\n",
      "iteration: 7 AUC: 0.415112\n",
      "iteration: 8 AUC: 0.454776\n",
      "iteration: 9 AUC: 0.541597\n",
      "iteration: 10 AUC: 0.534075\n",
      "iteration: 11 AUC: 0.597284\n",
      "iteration: 12 AUC: 0.649910\n",
      "iteration: 13 AUC: 0.547366\n",
      "iteration: 14 AUC: 0.565843\n",
      "iteration: 15 AUC: 0.589672\n",
      "iteration: 16 AUC: 0.556104\n",
      "iteration: 17 AUC: 0.518545\n",
      "iteration: 18 AUC: 0.493418\n",
      "iteration: 19 AUC: 0.582799\n",
      "iteration: 20 AUC: 0.536134\n",
      "iteration: 21 AUC: 0.595284\n",
      "iteration: 22 AUC: 0.590590\n",
      "iteration: 23 AUC: 0.627119\n",
      "iteration: 24 AUC: 0.611560\n",
      "iteration: 25 AUC: 0.637806\n",
      "iteration: 26 AUC: 0.627649\n",
      "iteration: 27 AUC: 0.624179\n",
      "iteration: 28 AUC: 0.694358\n",
      "iteration: 29 AUC: 0.698657\n",
      "iteration: 30 AUC: 0.720358\n",
      "iteration: 31 AUC: 0.705724\n",
      "iteration: 32 AUC: 0.668343\n",
      "iteration: 33 AUC: 0.718993\n",
      "iteration: 34 AUC: 0.726142\n",
      "iteration: 35 AUC: 0.698560\n",
      "iteration: 36 AUC: 0.729134\n",
      "iteration: 37 AUC: 0.704269\n",
      "iteration: 38 AUC: 0.728067\n",
      "iteration: 39 AUC: 0.696619\n",
      "iteration: 40 AUC: 0.691776\n",
      "iteration: 41 AUC: 0.701157\n",
      "iteration: 42 AUC: 0.680507\n",
      "iteration: 43 AUC: 0.671619\n",
      "iteration: 44 AUC: 0.719134\n",
      "iteration: 45 AUC: 0.704925\n",
      "iteration: 46 AUC: 0.691381\n",
      "iteration: 47 AUC: 0.700433\n",
      "iteration: 48 AUC: 0.721701\n",
      "iteration: 49 AUC: 0.720045\n",
      "iteration: 50 AUC: 0.723366\n",
      "iteration: 51 AUC: 0.736627\n",
      "iteration: 52 AUC: 0.745746\n",
      "iteration: 53 AUC: 0.750060\n",
      "iteration: 54 AUC: 0.762948\n",
      "iteration: 55 AUC: 0.761657\n",
      "iteration: 56 AUC: 0.766373\n",
      "iteration: 57 AUC: 0.770328\n",
      "iteration: 58 AUC: 0.778866\n",
      "iteration: 59 AUC: 0.775522\n",
      "iteration: 60 AUC: 0.779910\n",
      "iteration: 61 AUC: 0.779657\n",
      "iteration: 62 AUC: 0.786276\n",
      "iteration: 63 AUC: 0.785418\n",
      "iteration: 64 AUC: 0.785515\n",
      "iteration: 65 AUC: 0.789328\n",
      "iteration: 66 AUC: 0.789948\n",
      "iteration: 67 AUC: 0.788537\n",
      "iteration: 68 AUC: 0.787157\n",
      "iteration: 69 AUC: 0.787597\n",
      "iteration: 70 AUC: 0.788246\n",
      "iteration: 71 AUC: 0.789231\n",
      "iteration: 72 AUC: 0.789090\n",
      "iteration: 73 AUC: 0.794694\n",
      "iteration: 74 AUC: 0.794679\n",
      "iteration: 75 AUC: 0.795134\n",
      "iteration: 76 AUC: 0.796082\n",
      "iteration: 77 AUC: 0.796149\n",
      "iteration: 78 AUC: 0.795776\n",
      "iteration: 79 AUC: 0.800104\n",
      "iteration: 80 AUC: 0.799164\n",
      "iteration: 81 AUC: 0.798440\n",
      "iteration: 82 AUC: 0.797843\n",
      "iteration: 83 AUC: 0.797067\n",
      "iteration: 84 AUC: 0.797030\n",
      "iteration: 85 AUC: 0.798463\n",
      "iteration: 86 AUC: 0.797806\n",
      "iteration: 87 AUC: 0.797866\n",
      "iteration: 88 AUC: 0.797172\n",
      "iteration: 89 AUC: 0.798366\n",
      "iteration: 90 AUC: 0.797485\n",
      "iteration: 91 AUC: 0.797030\n",
      "iteration: 92 AUC: 0.796590\n",
      "iteration: 93 AUC: 0.796821\n",
      "iteration: 94 AUC: 0.796164\n",
      "iteration: 95 AUC: 0.795022\n",
      "iteration: 96 AUC: 0.794552\n",
      "iteration: 97 AUC: 0.794157\n",
      "iteration: 98 AUC: 0.794007\n",
      "iteration: 99 AUC: 0.793627\n",
      "iteration: 100 AUC: 0.794836\n"
     ]
    }
   ],
   "source": [
    "L = 2 # range\n",
    "N = 10 # degree\n",
    "T = 100 # iteration\n",
    "\n",
    "hinge = lambda x:max(0,1+L-2*L*x)\n",
    "logistics = lambda x:np.log(1+np.exp(L-2*L*x))\n",
    "\n",
    "pt = 0.0\n",
    "wt = np.zeros(d)\n",
    "at = np.zeros(N+1)\n",
    "bt = np.zeros(N+1)\n",
    "alphat = np.zeros(N+1)\n",
    "\n",
    "roc_auc = np.zeros(T)\n",
    "batch = 1\n",
    "for t in range(T):\n",
    "    pt,wt,at,bt,alphat = SOLAM(t,batch,hinge,pt,wt,at,bt,alphat)\n",
    "    fpr, tpr, _ = roc_curve(y, np.dot(x,wt))\n",
    "    roc_auc[t] = auc(fpr, tpr)\n",
    "    print('iteration: %d AUC: %f' %(t+1,roc_auc[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0891991 , -0.07257935, -0.0165664 , -0.08278925, -0.1178296 ,\n",
       "       -0.06898348, -0.08609111, -0.13859913])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
