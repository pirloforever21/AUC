{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from itertools import product\n",
    "import multiprocessing\n",
    "from math import sqrt,log,exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(i,prod,L):\n",
    "    '''\n",
    "    Compute positive function and gradient information\n",
    "    \n",
    "    input:\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fpt - positive function value\n",
    "        gfpt - positive function gradient\n",
    "    '''\n",
    "    plus = L/2+prod\n",
    "    fpt = plus**i\n",
    "    gfpt = fpt*i/plus # no xt yet!\n",
    "    \n",
    "    # hfpt = i*(i-1)*(L/2+prod)**(i-2)\n",
    "    return fpt,gfpt              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    '''\n",
    "    Compute combination\n",
    "    \n",
    "    input:\n",
    "        n - total number\n",
    "        k - number of chosen\n",
    "    \n",
    "    output:\n",
    "        c - number of combination\n",
    "    '''\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_dict = {0:{0:1},1:{0:1,1:1},2:{0:1,1:2,2:1},3:{0:1,1:3,2:3,3:1},4:{0:1,1:4,2:6,3:4,4:1},\n",
    "             5:{0:1,1:5,2:10,3:10,4:5,5:1},6:{0:1,1:6,2:15,3:20,4:15,5:6,6:1},\n",
    "             7:{0:1,1:7,2:21,3:35,4:35,5:21,6:7,7:1},8:{0:1,1:8,2:28,3:56,4:70,5:56,6:28,7:8,8:1},\n",
    "             9:{0:1,1:9,2:36,3:84,4:126,5:126,6:84,7:36,8:9,9:1},\n",
    "             10:{0:1,1:10,2:45,3:120,4:210,5:252,6:210,7:120,8:45,9:10,10:1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_dict = {0:np.array([11,110,495,1320,2310,2772,2310,1320,495,110,11]),\n",
    "             1:np.array([110,990,3960,9240,13860,13860,9240,3960,990,110]),\n",
    "             2:np.array([495,3960,13860,27720,34650,27720,13860,3960,495]),\n",
    "             3:np.array([1320,9240,27720,46200,46200,27720,9240,1320]),\n",
    "             4:np.array([2310,13860,34650,46200,34650,13860,2310]),5:np.array([2772,13860,27720,27720,13860,2772]),\n",
    "             6:np.array([2310,9240,13860,9240,2310]),7:np.array([1320,3960,3960,1320]),8:np.array([495,990,495]),\n",
    "             9:np.array([110,110]),10:np.array([11])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(loss,i,prod,L):\n",
    "    '''\n",
    "    Compute negative function and gradient information\n",
    "    \n",
    "    input:\n",
    "        loss - loss function\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fnt - negative function value\n",
    "        gfnt - negative function gradient\n",
    "    '''\n",
    "    minus = L/2-prod\n",
    "    #FNT = np.zeros(N+1-i)\n",
    "    #GFNT = np.zeros(N+1-i)\n",
    "    fnt = 0.0\n",
    "    gfnt = 0.0\n",
    "    # hfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        # compute forward difference\n",
    "        delta = 0.0\n",
    "        for j in range(k+1):\n",
    "            delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
    "            \n",
    "        # compute coefficient\n",
    "        #beta = beta_dict[i][k-i]*delta/(2*L)**k*minus**(k-i)\n",
    "        beta = comb_dict[N][k]*comb_dict[k][i]*(N+1)*delta/(2*L)**k*minus**(k-i)\n",
    "        # compute function value\n",
    "        fnt += beta\n",
    "        # compute gradient\n",
    "        gfnt += beta*(k-i)/minus  # no xt yet!\n",
    "        '''\n",
    "        # compute temp function value\n",
    "        FNT[k-i] = delta/(2*L)**k*(minus**(k-i))\n",
    "        GFNT[k-i] = FNT[k-i]*(k-i)/minus\n",
    "    # compute function value\n",
    "    fnt = np.dot(beta_dict[i],FNT)\n",
    "    # compute gradient\n",
    "    gfnt = np.dot(beta_dict[i],GFNT) # no xt yet!\n",
    "    '''\n",
    "        # compute hessian\n",
    "        # hfnt += beta*(k-i)*(k-i-1)*(L/2-prod)**(k-i-2)\n",
    "    return fnt,gfnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(gfpt,gfnt,yt,at,bt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to w\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        gfpt - positive function gradient at t\n",
    "        fnt - negative function at t\n",
    "        gfnt - negative function gradient at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        gradwt - gradient w.r.t. w at t\n",
    "    '''\n",
    "    if yt == 1:\n",
    "        gradwt = 2*(alphat - at)*gfpt\n",
    "    else:\n",
    "        gradwt = 2*(alphat - bt)*gfnt\n",
    "    return gradwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_hess(hfpt,hfnt,yt,at,bt,alphat):\n",
    "    hesswt = 0.0\n",
    "    if yt == 1:\n",
    "        hesswt = 2*(alphat - at)*hfpt\n",
    "    else:\n",
    "        hesswt = 2*(alphat - bt)*hfnt\n",
    "    return hesswt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(wt,R):\n",
    "    '''\n",
    "    Projection\n",
    "    \n",
    "    input:\n",
    "        wt - w at t\n",
    "        R - radius\n",
    "        \n",
    "    output:\n",
    "        proj - projected wt\n",
    "    '''\n",
    "    norm = np.linalg.norm(wt)\n",
    "    if norm > R:\n",
    "        wt = wt/norm*R\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(fpt,yt,at):\n",
    "    '''\n",
    "    Gradient with respect to a\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "    \n",
    "    output:\n",
    "        gradat - gradient w.r.t a at t\n",
    "    '''\n",
    "    gradat = 0.0 \n",
    "    if yt == 1:\n",
    "        gradat = 2*(at - fpt)\n",
    "    else:\n",
    "        gradat = 2*at\n",
    "    return gradat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(fnt,yt,bt):\n",
    "    '''\n",
    "    Gradient with respect to b\n",
    "    \n",
    "    input:\n",
    "        fnt - negative function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        bt - b at t\n",
    "    \n",
    "    output:\n",
    "        gradbt - gradient w.r.t b at t\n",
    "    '''\n",
    "    gradbt = 0.0 \n",
    "    if yt == 1:\n",
    "        gradbt = 2*bt\n",
    "    else:\n",
    "        gradbt = 2*(bt - fnt)\n",
    "    return gradbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_grad(fpt,fnt,yt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to alpha\n",
    "    '''\n",
    "    gradalphat = 0.0\n",
    "    if yt == 1:\n",
    "        gradalphat = -2*(alphat - fpt)\n",
    "    else:\n",
    "        gradalphat = -2*(alphat - fnt)\n",
    "    return gradalphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(filename):\n",
    "    '''\n",
    "    Data file loader\n",
    "    \n",
    "    input:\n",
    "        filename - filename\n",
    "    \n",
    "    output:\n",
    "        x - sample features\n",
    "        y - sample labels\n",
    "    '''\n",
    "    # raw data\n",
    "    L = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in csv.reader(file, delimiter = ' '):\n",
    "            line[0] = '0:'+line[0]\n",
    "            line.remove('')\n",
    "            L.append(dict(i.split(':') for i in line))\n",
    "    df = pd.DataFrame(L,dtype=float).fillna(0)\n",
    "    X = df.iloc[:,1:].values\n",
    "    Y = df.iloc[:,0].values\n",
    "    # centralize\n",
    "    mean = np.mean(X,axis=1)\n",
    "    X = (X.transpose() - mean).transpose()\n",
    "    # normalize\n",
    "    norm = np.linalg.norm(X,axis=1)\n",
    "    X = X/norm[:,None]\n",
    "    # convert to binary class\n",
    "    r = np.ptp(Y)\n",
    "    index = np.argwhere(Y<r//2)\n",
    "    INDEX = np.argwhere(Y>=r//2)\n",
    "    Y[index] = -1\n",
    "    Y[INDEX] = 1\n",
    "    Y = Y.astype(int)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(FEATURES,LABELS,folder,folders):\n",
    "    \n",
    "    if folder > folders:\n",
    "        print('Exceed maximum folders!')\n",
    "        return\n",
    "    # load and split data\n",
    "    #FEATURES,LABELS = loader(dataset)\n",
    "    n,d = FEATURES.shape\n",
    "    # regular portion of each folder\n",
    "    portion = round(n/folders)\n",
    "    start = portion*folder\n",
    "    stop = portion*(folder+1)\n",
    "    if np.abs(stop - n) < portion: # remainder occurs\n",
    "        X_train = FEATURES[:start,:]\n",
    "        Y_train = LABELS[:start]\n",
    "        X_test = FEATURES[start:,:]\n",
    "        Y_test = LABELS[start:]\n",
    "    else:\n",
    "        mask = np.ones(n, bool)\n",
    "        mask[start:stop] = False\n",
    "        X_train = FEATURES[mask,:]\n",
    "        Y_train = LABELS[mask]\n",
    "        X_test = FEATURES[start:stop]\n",
    "        Y_test = LABELS[start:stop]\n",
    "    # get dimensions of the data\n",
    "    # n,_ = X_train.shape\n",
    "    # number of epoch\n",
    "    # epoch = T//n+1\n",
    "    # augment by epoch\n",
    "    # X_train_augmented = np.tile(X_train,(epoch,1)) # might have memory burden\n",
    "    # Y_train_augmented = np.tile(Y_train,epoch)\n",
    "    \n",
    "    #return X_train_augmented,X_test,Y_train_augmented,Y_test\n",
    "    return X_train,X_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox(eta,loss,x,y,L,gamma,lam,wj,aj,bj,alphaj,bwt,bat,bbt,balphat):\n",
    "    '''\n",
    "    perform proximal guided gradient descent when receive an sample\n",
    "    '''\n",
    "    prod = np.dot(wj,x)\n",
    "    fpt = np.zeros(N+1)\n",
    "    gfpt = np.zeros(N+1)\n",
    "    # hfpt = np.zeros(N+1)\n",
    "    fnt = np.zeros(N+1)\n",
    "    gfnt = np.zeros(N+1)\n",
    "    # hfnt = np.zeros(N+1)\n",
    "    gradwt = 0.0\n",
    "    gradat = 0.0\n",
    "    gradbt = 0.0\n",
    "    gradalphat = 0.0\n",
    "    # hesswt = 0.0\n",
    "    for i in range(N+1):\n",
    "        fpt[i],gfpt[i] = pos(i,prod,L)\n",
    "        #print(fpt[i])\n",
    "        fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
    "        gradwt += w_grad(gfpt[i],gfnt[i],y,aj[i],bj[i],alphaj[i])# accumulate i\n",
    "        # hesswt += w_hess(hfpt[i],hfnt[i],y,aj[i],bj[i],alphaj[i])\n",
    "        gradat = a_grad(fpt[i],y,aj[i])\n",
    "        gradbt = b_grad(fnt[i],y,bj[i])\n",
    "        gradalphat = alpha_grad(fpt[i],fnt[i],y,alphaj[i])\n",
    "        aj[i] = aj[i] - eta*(gradat/(N+1)+gamma*(aj[i]-bat[i]))\n",
    "        bj[i] = bj[i] - eta*(gradbt/(N+1)+gamma*(bj[i]-bbt[i]))\n",
    "        alphaj[i] = alphaj[i] + eta*gradalphat/(N+1)\n",
    "    # hessian = hesswt*np.outer(x,x)\n",
    "    # eigen,_ = np.linalg.eig(hessian)\n",
    "    \n",
    "    # print('minimum eigenvalue: %f' %(np.min(eigen)))\n",
    "    wj = wj - eta*(gradwt*x*y/(N+1) + lam*wj + gamma*(wj - bwt))\n",
    "    wj = proj(wj,L/2)\n",
    "    #aJ = proj(aJ,1)\n",
    "    #bJ = proj(bJ,1)\n",
    "    #alphaJ = proj(alphaJ,1)\n",
    "    \n",
    "    return wj,aj,bj,alphaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGSPD(t,loss,X,Y,L,gamma,lam,theta,c,bwt,bat,bbt,balphat):\n",
    "    '''\n",
    "    Proximally Guided Stochastic Primal Dual Algorithm\n",
    "    '''\n",
    "    \n",
    "    # initialize inner loop variables\n",
    "    Wt = bwt+0.0\n",
    "    At = bat+0.0\n",
    "    Bt = bbt+0.0\n",
    "    ALPHAt = balphat+0.0\n",
    "    \n",
    "    BWt = Wt+0.0\n",
    "    BAt = At+0.0\n",
    "    BBt = Bt+0.0\n",
    "    BALPHAt = ALPHAt+0.0\n",
    "    \n",
    "    ETAt = c/(t**theta)\n",
    "    # inner loop update at j\n",
    "    for j in range(t): \n",
    "        # update inner loop variables\n",
    "        Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
    "        BWt += Wt\n",
    "        BAt += At\n",
    "        BBt += Bt\n",
    "        BALPHAt += ALPHAt\n",
    "        \n",
    "    # update outer loop variables\n",
    "    bwt = BWt/t\n",
    "    bat = BAt/t\n",
    "    bbt = BBt/t\n",
    "    balphat = BALPHAt/t\n",
    "    \n",
    "    return bwt,bat,bbt,balphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOLAM(t,loss,batch,X,Y,L,lam,theta,c,wt,at,bt,alphat):\n",
    "    '''\n",
    "    Stochastic Online AUC Maximization step\n",
    "    \n",
    "    input:\n",
    "        T - total number of iteration\n",
    "        F - objective function value\n",
    "        loss - loss function\n",
    "        pt - p at t\n",
    "        wt - w at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        W - record of each wt\n",
    "        A - record of each at\n",
    "        B - record of each bt\n",
    "        ALPHA - record of each alphat\n",
    "    '''\n",
    "    # Loop in the batch\n",
    "    peta = c/(t**theta)\n",
    "    deta = sqrt(log(T*(T+1)/2/batch)/(T*(T+1)/2/batch))\n",
    "    for k in range(batch):\n",
    "        \n",
    "        # Update wt,at,bt\n",
    "        prod = np.dot(wt,X[k])\n",
    "        fpt = np.zeros(N+1)\n",
    "        gfpt = np.zeros(N+1)\n",
    "        fnt = np.zeros(N+1)\n",
    "        gfnt = np.zeros(N+1)\n",
    "        gradwt = 0.0\n",
    "        gradat = 0.0\n",
    "        gradbt = 0.0\n",
    "        gradalphat = 0.0\n",
    "        \n",
    "        for i in range(N+1): # add up info of each i\n",
    "            fpt[i],gfpt[i] = pos(i,prod,L) # partial info\n",
    "            fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
    "            gradwt += w_grad(gfpt[i],gfnt[i],Y[k],at[i],bt[i],alphat[i])\n",
    "            gradat = a_grad(fpt[i],Y[k],at[i])\n",
    "            gradbt = b_grad(fnt[i],Y[k],bt[i])\n",
    "            gradalphat = alpha_grad(fpt[i],fnt[i],Y[k],alphat[i])\n",
    "            at[i] -= deta*gradat/(N+1)/batch\n",
    "            bt[i] -= deta*gradbt/(N+1)/batch\n",
    "            alphat[i] += deta*gradalphat/(N+1)/batch\n",
    "        \n",
    "        wt = wt - peta*(gradwt*Y[k]*X[k]/(N+1)/batch + lam*wt) # step size as 1/t gradient descent\n",
    "        \n",
    "    wt = proj(wt,L/2)    \n",
    "        \n",
    "    return wt,at,bt,alphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(X_train_augmented,X_test,Y_train_augmented,Y_test,loss,alg,gamma=0.1,lam=1.0,theta=0.1,c = 1.0):\n",
    "    '''\n",
    "    Run it to get results\n",
    "    '''\n",
    "    # define loss function\n",
    "    if loss == 'hinge':\n",
    "        L = 2*sqrt(2/lam)\n",
    "        loss = lambda x: max(0,1+L-2*L*x)\n",
    "    elif loss == 'logistic':\n",
    "        L = 2*sqrt(2*log(2)/lam)\n",
    "        loss = lambda x:log(1+exp(L-2*L*x))\n",
    "    else:\n",
    "        print('Wrong loss function!')\n",
    "        return\n",
    "    \n",
    "    # get dimensions of the data\n",
    "    num,d = X_train_augmented.shape\n",
    "    \n",
    "    # initialize outer loop variables\n",
    "    WT = np.zeros(d) # d is the dimension of the features\n",
    "    AT = np.zeros(N+1)\n",
    "    BT = np.zeros(N+1)\n",
    "    ALPHAT = np.zeros(N+1)\n",
    "\n",
    "    # record auc\n",
    "    roc_auc = np.zeros(T)\n",
    "    # record time elapsed\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for t in range(1,T+1):    \n",
    "        if alg == 'PGSPD':\n",
    "        \n",
    "            if t<num:\n",
    "                begin = (t*(t-1)//2)%num\n",
    "                end = (t*(t+1)//2)%num\n",
    "                if begin < end:\n",
    "                    x_train = X_train_augmented[begin:end]\n",
    "                    y_train = Y_train_augmented[begin:end]\n",
    "                else: # need to think better\n",
    "                    x_train = np.append(X_train_augmented[begin:],X_train_augmented[:end],axis=0)\n",
    "                    y_train = np.append(Y_train_augmented[begin:],Y_train_augmented[:end],axis=0)\n",
    "                # print(sum(x_train))\n",
    "                x_train, y_train = shuffle(x_train,y_train)\n",
    "                # update outer loop variables\n",
    "                WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
    "            else:\n",
    "                x_train, y_train = shuffle(X_train_augmented,Y_train_augmented)\n",
    "                WT,AT,BT,ALPHAT = PGSPD(num,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
    "                \n",
    "        elif alg == 'SOLAM':\n",
    "\n",
    "            # sample a point\n",
    "            begin = (t-1)*batch%num\n",
    "            end = t*batch%num\n",
    "            if begin < end:\n",
    "                x_train = X_train_augmented[begin:end]\n",
    "                y_train = Y_train_augmented[begin:end]\n",
    "            else: # need to think better\n",
    "                x_train = np.append(X_train_augmented[begin:],X_train_augmented[:end],axis=0)\n",
    "                y_train = np.append(Y_train_augmented[begin:],Y_train_augmented[:end],axis=0)\n",
    "            WT,AT,BT,ALPHAT = SOLAM(t,loss,batch,x_train,y_train,L,lam,theta,c,WT,AT,BT,ALPHAT)\n",
    "        \n",
    "        else:\n",
    "            print('Wrong algorithm!')\n",
    "            return\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(Y_test, np.dot(X_test,WT))\n",
    "        roc_auc[t-1] = auc(fpr, tpr)\n",
    "        if t%100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('gamma: %.1f lam: %.1f theta: %.1f c: %.1f iteration: %d AUC: %.4f time eplapsed: %.2f' \n",
    "                  %(gamma,lam,theta,c,t,roc_auc[t-1],elapsed_time))\n",
    "            start_time = time.time()\n",
    "    \n",
    "            \n",
    "    return WT,AT,BT,ALPHAT,roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(dataset,loss,alg,folders,gamma=0.0,lam=0.0,theta=1.0,c=1.0):\n",
    "    '''\n",
    "    Cross validation\n",
    "    '''\n",
    "    # Load data set\n",
    "    FEATURES,LABELS = loader(dataset) \n",
    "    \n",
    "    # record auc\n",
    "    AUC_ROC = np.zeros(folders)\n",
    "    \n",
    "    # cross validation\n",
    "    for folder in range(folders):\n",
    "        print('folder = %d' %(folder))\n",
    "        X_train_augmented,X_test,Y_train_augmented,Y_test = split(FEATURES,LABELS,folder,folders)\n",
    "        \n",
    "        _,_,_,_,roc_auc = demo(X_train_augmented,X_test,Y_train_augmented,Y_test,loss,alg,gamma=gamma,lam=lam,theta=theta,c=c)\n",
    "        AUC_ROC[folder] = max(roc_auc)\n",
    "    print('auc score: %f +/- %f' %(np.mean(AUC_ROC),np.std(AUC_ROC)))\n",
    "    return AUC_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(para):\n",
    "    folder,gamma,lam,theta,c,paras = para\n",
    "    X_train_augmented,X_test,Y_train_augmented, Y_test,loss,alg = paras\n",
    "    _,_,_,_,roc_auc = demo(X_train_augmented,X_test,Y_train_augmented,Y_test,loss,alg\n",
    "                           ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
    "    return folder,gamma,lam,theta,c, np.max(roc_auc)\n",
    "    \n",
    "def gs(dataset,loss,alg,folders,GAMMA=[0.0],LAM=[0.0],THETA=[1.0],C=[1.0]):\n",
    "    '''\n",
    "    Grid search! Wuss up fellas?!\n",
    "    And we are using multiprocessing, fancy!\n",
    "    '''\n",
    "    # number of cpu want to use\n",
    "    num_cpus = 15\n",
    "    # Load data set\n",
    "    FEATURES,LABELS = loader(dataset) \n",
    "    # record auc\n",
    "    AUC_ROC = np.zeros((folders,len(GAMMA),len(LAM),len(THETA),len(C)))\n",
    "    # record parameters\n",
    "    input_paras = []\n",
    "    # grid search prepare\n",
    "    for folder in range(folders):\n",
    "        X_train_augmented,X_test,Y_train_augmented,Y_test = split(FEATURES,LABELS,folder,folders)\n",
    "        paras = X_train_augmented,X_test,Y_train_augmented, Y_test,loss,alg\n",
    "        for gamma,lam,theta,c in product(range(len(GAMMA)),range(len(LAM)),range(len(THETA)),range(len(C))):\n",
    "            input_paras.append((folder,gamma,lam,theta,c,paras))\n",
    "    print('how many paras: %d' % len(input_paras))\n",
    "    # grid search run on multiprocessors\n",
    "    pool = multiprocessing.Pool(processes=num_cpus)\n",
    "    results_pool = pool.map(single_run,input_paras)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    # save results\n",
    "    for folder,gamma,lam,theta,c, auc_roc in results_pool:\n",
    "        AUC_ROC[folder,gamma,lam,theta,c] = auc_roc\n",
    "    MEAN = np.zeros((len(GAMMA),len(LAM),len(THETA),len(C)))\n",
    "    STD = np.zeros((len(GAMMA),len(LAM),len(THETA),len(C)))\n",
    "    for gamma,lam,theta,c in product(range(len(GAMMA)),range(len(LAM)),range(len(THETA)),range(len(C))):    \n",
    "        MEAN[gamma,lam,theta,c] = np.mean(AUC_ROC[:,gamma,lam,theta,c])\n",
    "        STD[gamma,lam,theta,c] = np.std(AUC_ROC[:,gamma,lam,theta,c])\n",
    "    return AUC_ROC,MEAN,STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10\n",
    "T=2000\n",
    "batch=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = [0.1]\n",
    "LAM = [1.0,10.0]\n",
    "THETA = [0.25,0.5,0.75,1.0]\n",
    "C = [10.0,100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many paras: 80\n",
      "iteration: 100 AUC: 0.879456 time eplapsed: 36.989216\n",
      "iteration: 100 AUC: 0.838404 time eplapsed: 41.373804\n",
      "iteration: 100 AUC: 0.836429 time eplapsed: 41.902583\n",
      "iteration: 100 AUC: 0.801507 time eplapsed: 42.354053\n",
      "iteration: 100 AUC: 0.856245 time eplapsed: 43.054623\n",
      "iteration: 100 AUC: 0.842044 time eplapsed: 43.078292\n",
      "iteration: 100 AUC: 0.864675 time eplapsed: 42.528008\n",
      "iteration: 100 AUC: 0.852600 time eplapsed: 42.779291\n",
      "iteration: 100 AUC: 0.838594 time eplapsed: 43.562439\n",
      "iteration: 100 AUC: 0.799372 time eplapsed: 43.282868\n",
      "iteration: 100 AUC: 0.856717 time eplapsed: 43.991413\n",
      "iteration: 100 AUC: 0.847957 time eplapsed: 43.492809\n",
      "iteration: 100 AUC: 0.842033 time eplapsed: 46.105535\n",
      "iteration: 100 AUC: 0.851069 time eplapsed: 45.295066\n",
      "iteration: 100 AUC: 0.891161 time eplapsed: 45.544091\n",
      "iteration: 200 AUC: 0.768720 time eplapsed: 57.218038\n",
      "iteration: 200 AUC: 0.847124 time eplapsed: 56.402246\n",
      "iteration: 200 AUC: 0.651584 time eplapsed: 64.542506\n",
      "iteration: 200 AUC: 0.921839 time eplapsed: 59.859035\n",
      "iteration: 200 AUC: 0.778274 time eplapsed: 64.330667\n",
      "iteration: 200 AUC: 0.838439 time eplapsed: 65.208105\n",
      "iteration: 200 AUC: 0.873730 time eplapsed: 66.873954\n",
      "iteration: 200 AUC: 0.594381 time eplapsed: 64.661734\n",
      "iteration: 200 AUC: 0.917752 time eplapsed: 67.021653\n",
      "iteration: 200 AUC: 0.912543 time eplapsed: 65.765618\n",
      "iteration: 200 AUC: 0.736806 time eplapsed: 66.960693\n",
      "iteration: 200 AUC: 0.912981 time eplapsed: 66.571781\n",
      "iteration: 200 AUC: 0.684657 time eplapsed: 65.657358\n",
      "iteration: 200 AUC: 0.910791 time eplapsed: 65.880818\n",
      "iteration: 200 AUC: 0.843889 time eplapsed: 67.259689\n",
      "iteration: 300 AUC: 0.413807 time eplapsed: 73.083836\n",
      "iteration: 300 AUC: 0.735894 time eplapsed: 76.876328\n",
      "iteration: 300 AUC: 0.919151 time eplapsed: 76.957076\n",
      "iteration: 300 AUC: 0.855446 time eplapsed: 72.404776\n",
      "iteration: 300 AUC: 0.805118 time eplapsed: 74.143904\n",
      "iteration: 300 AUC: 0.610792 time eplapsed: 74.254998\n",
      "iteration: 300 AUC: 0.916477 time eplapsed: 83.403907\n",
      "iteration: 300 AUC: 0.800660 time eplapsed: 75.252612\n",
      "iteration: 300 AUC: 0.848857 time eplapsed: 80.740919\n",
      "iteration: 300 AUC: 0.933443 time eplapsed: 79.465857\n",
      "iteration: 300 AUC: 0.929196 time eplapsed: 77.896518\n",
      "iteration: 300 AUC: 0.376290 time eplapsed: 86.787091\n",
      "iteration: 300 AUC: 0.861867 time eplapsed: 84.838089\n",
      "iteration: 300 AUC: 0.864369 time eplapsed: 84.565417\n",
      "iteration: 300 AUC: 0.882573 time eplapsed: 84.778647\n",
      "iteration: 400 AUC: 0.911643 time eplapsed: 82.315476\n",
      "iteration: 400 AUC: 0.883602 time eplapsed: 89.412909\n",
      "iteration: 400 AUC: 0.812953 time eplapsed: 97.968774\n",
      "iteration: 400 AUC: 0.831479 time eplapsed: 96.887323\n",
      "iteration: 400 AUC: 0.804421 time eplapsed: 92.577749\n",
      "iteration: 400 AUC: 0.884316 time eplapsed: 95.760342\n",
      "iteration: 400 AUC: 0.868512 time eplapsed: 97.030171\n",
      "iteration: 400 AUC: 0.879625 time eplapsed: 91.281111\n",
      "iteration: 400 AUC: 0.909368 time eplapsed: 92.490947\n",
      "iteration: 400 AUC: 0.696127 time eplapsed: 98.491239\n",
      "iteration: 400 AUC: 0.868637 time eplapsed: 93.702659\n",
      "iteration: 400 AUC: 0.884405 time eplapsed: 94.667298\n",
      "iteration: 400 AUC: 0.683445 time eplapsed: 98.793224\n",
      "iteration: 400 AUC: 0.891065 time eplapsed: 96.844373\n",
      "iteration: 400 AUC: 0.733453 time eplapsed: 98.340988\n",
      "iteration: 500 AUC: 0.843138 time eplapsed: 102.759432\n",
      "iteration: 500 AUC: 0.880940 time eplapsed: 112.179713\n",
      "iteration: 500 AUC: 0.893554 time eplapsed: 109.191785\n",
      "iteration: 500 AUC: 0.841973 time eplapsed: 110.062917\n",
      "iteration: 500 AUC: 0.892052 time eplapsed: 104.894626\n",
      "iteration: 500 AUC: 0.884273 time eplapsed: 106.433399\n",
      "iteration: 500 AUC: 0.885330 time eplapsed: 112.433260\n",
      "iteration: 500 AUC: 0.894823 time eplapsed: 113.679766\n",
      "iteration: 500 AUC: 0.818513 time eplapsed: 114.473618\n",
      "iteration: 500 AUC: 0.878842 time eplapsed: 107.306162\n",
      "iteration: 500 AUC: 0.549086 time eplapsed: 115.032700\n",
      "iteration: 500 AUC: 0.903162 time eplapsed: 108.044986\n",
      "iteration: 500 AUC: 0.693988 time eplapsed: 110.145592\n",
      "iteration: 500 AUC: 0.885494 time eplapsed: 110.942209\n",
      "iteration: 500 AUC: 0.808462 time eplapsed: 112.273322\n",
      "iteration: 600 AUC: 0.675223 time eplapsed: 117.052725\n",
      "iteration: 600 AUC: 0.777338 time eplapsed: 120.239101\n",
      "iteration: 600 AUC: 0.273789 time eplapsed: 120.144909\n",
      "iteration: 600 AUC: 0.847265 time eplapsed: 128.745513\n",
      "iteration: 600 AUC: 0.871562 time eplapsed: 125.250624\n",
      "iteration: 600 AUC: 0.826773 time eplapsed: 123.223301\n",
      "iteration: 600 AUC: 0.876654 time eplapsed: 121.984455\n",
      "iteration: 600 AUC: 0.860360 time eplapsed: 124.578990\n",
      "iteration: 600 AUC: 0.873671 time eplapsed: 125.895323\n",
      "iteration: 600 AUC: 0.526124 time eplapsed: 133.375380\n",
      "iteration: 600 AUC: 0.364178 time eplapsed: 126.507783\n",
      "iteration: 600 AUC: 0.534818 time eplapsed: 122.218633\n",
      "iteration: 600 AUC: 0.857861 time eplapsed: 122.905768\n",
      "iteration: 600 AUC: 0.863100 time eplapsed: 133.320328\n",
      "iteration: 600 AUC: 0.564320 time eplapsed: 126.107239\n",
      "iteration: 700 AUC: 0.837929 time eplapsed: 133.849354\n",
      "iteration: 700 AUC: 0.810434 time eplapsed: 142.295535\n",
      "iteration: 700 AUC: 0.546066 time eplapsed: 137.545150\n",
      "iteration: 700 AUC: 0.906565 time eplapsed: 131.304345\n",
      "iteration: 700 AUC: 0.911990 time eplapsed: 133.854912\n",
      "iteration: 700 AUC: 0.897895 time eplapsed: 139.665050\n",
      "iteration: 700 AUC: 0.791233 time eplapsed: 138.383929\n",
      "iteration: 700 AUC: 0.902739 time eplapsed: 145.319155\n",
      "iteration: 700 AUC: 0.847146 time eplapsed: 138.640565\n",
      "iteration: 700 AUC: 0.901108 time eplapsed: 140.432265\n",
      "iteration: 700 AUC: 0.900529 time eplapsed: 136.524354\n",
      "iteration: 700 AUC: 0.656353 time eplapsed: 141.856105\n",
      "iteration: 700 AUC: 0.389108 time eplapsed: 143.823385\n",
      "iteration: 700 AUC: 0.754261 time eplapsed: 134.817142\n",
      "iteration: 700 AUC: 0.898743 time eplapsed: 143.461370\n",
      "iteration: 800 AUC: 0.917164 time eplapsed: 147.201069\n",
      "iteration: 800 AUC: 0.875506 time eplapsed: 141.878591\n",
      "iteration: 800 AUC: 0.867609 time eplapsed: 152.069708\n",
      "iteration: 800 AUC: 0.868548 time eplapsed: 144.960229\n",
      "iteration: 800 AUC: 0.917696 time eplapsed: 148.415408\n",
      "iteration: 800 AUC: 0.807060 time eplapsed: 157.084439\n",
      "iteration: 800 AUC: 0.825443 time eplapsed: 145.802343\n",
      "iteration: 800 AUC: 0.914233 time eplapsed: 150.363509\n",
      "iteration: 800 AUC: 0.837316 time eplapsed: 153.151726\n",
      "iteration: 800 AUC: 0.913678 time eplapsed: 158.883755\n",
      "iteration: 800 AUC: 0.877925 time eplapsed: 155.125974\n",
      "iteration: 800 AUC: 0.568532 time eplapsed: 152.649283\n",
      "iteration: 800 AUC: 0.336074 time eplapsed: 151.424877\n",
      "iteration: 800 AUC: 0.885853 time eplapsed: 149.495701\n",
      "iteration: 800 AUC: 0.784680 time eplapsed: 161.495341\n",
      "iteration: 900 AUC: 0.891946 time eplapsed: 165.222622\n",
      "iteration: 900 AUC: 0.863081 time eplapsed: 167.743747\n",
      "iteration: 900 AUC: 0.917987 time eplapsed: 168.670759\n",
      "iteration: 900 AUC: 0.913060 time eplapsed: 162.498081\n",
      "iteration: 900 AUC: 0.882243 time eplapsed: 166.022174\n",
      "iteration: 900 AUC: 0.864221 time eplapsed: 165.384467\n",
      "iteration: 900 AUC: 0.918611 time eplapsed: 158.023689\n",
      "iteration: 900 AUC: 0.900525 time eplapsed: 163.795711\n",
      "iteration: 900 AUC: 0.824337 time eplapsed: 176.545188\n",
      "iteration: 900 AUC: 0.911926 time eplapsed: 163.112097\n",
      "iteration: 900 AUC: 0.898135 time eplapsed: 168.312525\n",
      "iteration: 900 AUC: 0.710170 time eplapsed: 168.687742\n",
      "iteration: 900 AUC: 0.786526 time eplapsed: 170.376049\n",
      "iteration: 900 AUC: 0.906413 time eplapsed: 174.797233\n",
      "iteration: 900 AUC: 0.506644 time eplapsed: 173.069524\n",
      "iteration: 1000 AUC: 0.900723 time eplapsed: 179.030075\n",
      "iteration: 1000 AUC: 0.877598 time eplapsed: 174.737378\n",
      "iteration: 1000 AUC: 0.893455 time eplapsed: 176.865415\n",
      "iteration: 1000 AUC: 0.903685 time eplapsed: 172.546700\n",
      "iteration: 1000 AUC: 0.869885 time eplapsed: 184.424256\n",
      "iteration: 1000 AUC: 0.723726 time eplapsed: 186.188438\n",
      "iteration: 1000 AUC: 0.362825 time eplapsed: 181.170654\n",
      "iteration: 1000 AUC: 0.626748 time eplapsed: 178.095742\n",
      "iteration: 1000 AUC: 0.900989 time eplapsed: 174.556285\n",
      "iteration: 1000 AUC: 0.750010 time eplapsed: 180.062714\n",
      "iteration: 1000 AUC: 0.490094 time eplapsed: 177.743946\n",
      "iteration: 1000 AUC: 0.352943 time eplapsed: 183.493092\n",
      "iteration: 1000 AUC: 0.889837 time eplapsed: 189.664676\n",
      "iteration: 1000 AUC: 0.891667 time eplapsed: 178.772455\n",
      "iteration: 1000 AUC: 0.588126 time eplapsed: 182.059952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1100 AUC: 0.573605 time eplapsed: 189.067965\n",
      "iteration: 1100 AUC: 0.898364 time eplapsed: 187.243573\n",
      "iteration: 1100 AUC: 0.856454 time eplapsed: 184.833957\n",
      "iteration: 1100 AUC: 0.853955 time eplapsed: 196.893878\n",
      "iteration: 1100 AUC: 0.824177 time eplapsed: 193.618827\n",
      "iteration: 1100 AUC: 0.844606 time eplapsed: 181.090309\n",
      "iteration: 1100 AUC: 0.616435 time eplapsed: 192.609183\n",
      "iteration: 1100 AUC: 0.914196 time eplapsed: 202.392944\n",
      "iteration: 1100 AUC: 0.720914 time eplapsed: 195.058757\n",
      "iteration: 1100 AUC: 0.856990 time eplapsed: 204.090300\n",
      "iteration: 1100 AUC: 0.657650 time eplapsed: 200.741886\n",
      "iteration: 1100 AUC: 0.270385 time eplapsed: 197.639117\n",
      "iteration: 1100 AUC: 0.301777 time eplapsed: 189.739588\n",
      "iteration: 1100 AUC: 0.899370 time eplapsed: 198.627537\n",
      "iteration: 1100 AUC: 0.894290 time eplapsed: 202.275318\n",
      "iteration: 1200 AUC: 0.891086 time eplapsed: 194.014426\n",
      "iteration: 1200 AUC: 0.797819 time eplapsed: 202.112283\n",
      "iteration: 1200 AUC: 0.776759 time eplapsed: 199.401488\n",
      "iteration: 1200 AUC: 0.882928 time eplapsed: 212.802354\n",
      "iteration: 1200 AUC: 0.806239 time eplapsed: 201.536295\n",
      "iteration: 1200 AUC: 0.875150 time eplapsed: 209.944119\n",
      "iteration: 1200 AUC: 0.877874 time eplapsed: 216.124867\n",
      "iteration: 1200 AUC: 0.894888 time eplapsed: 209.899562\n",
      "iteration: 1200 AUC: 0.768116 time eplapsed: 211.206180\n",
      "iteration: 1200 AUC: 0.840155 time eplapsed: 214.925389\n",
      "iteration: 1200 AUC: 0.747027 time eplapsed: 208.070679\n",
      "iteration: 1200 AUC: 0.448662 time eplapsed: 213.758297\n",
      "iteration: 1200 AUC: 0.897415 time eplapsed: 208.183866\n",
      "iteration: 1200 AUC: 0.892302 time eplapsed: 206.891510\n",
      "iteration: 1200 AUC: 0.582286 time eplapsed: 214.067880\n",
      "iteration: 1300 AUC: 0.902209 time eplapsed: 209.618001\n",
      "iteration: 1300 AUC: 0.768915 time eplapsed: 224.193818\n",
      "iteration: 1300 AUC: 0.809061 time eplapsed: 218.226933\n",
      "iteration: 1300 AUC: 0.806709 time eplapsed: 226.665487\n",
      "iteration: 1300 AUC: 0.901982 time eplapsed: 226.596284\n",
      "iteration: 1300 AUC: 0.898568 time eplapsed: 213.217103\n",
      "iteration: 1300 AUC: 0.876543 time eplapsed: 226.813855\n",
      "iteration: 1300 AUC: 0.896658 time eplapsed: 228.975781\n",
      "iteration: 1300 AUC: 0.876830 time eplapsed: 213.228076\n",
      "iteration: 1300 AUC: 0.697402 time eplapsed: 209.177308\n",
      "iteration: 1300 AUC: 0.755459 time eplapsed: 232.723327\n",
      "iteration: 1300 AUC: 0.907498 time eplapsed: 213.291650\n",
      "iteration: 1300 AUC: 0.902488 time eplapsed: 219.622957\n",
      "iteration: 1300 AUC: 0.527922 time eplapsed: 223.024445\n",
      "iteration: 1300 AUC: 0.390254 time eplapsed: 238.955764\n",
      "iteration: 1400 AUC: 0.893100 time eplapsed: 231.993190\n",
      "iteration: 1400 AUC: 0.862458 time eplapsed: 222.454561\n",
      "iteration: 1400 AUC: 0.829315 time eplapsed: 233.466017\n",
      "iteration: 1400 AUC: 0.833807 time eplapsed: 242.246416\n",
      "iteration: 1400 AUC: 0.905390 time eplapsed: 240.415587\n",
      "iteration: 1400 AUC: 0.876580 time eplapsed: 234.542554\n",
      "iteration: 1400 AUC: 0.889926 time eplapsed: 225.369466\n",
      "iteration: 1400 AUC: 0.892163 time eplapsed: 257.825897\n",
      "iteration: 1400 AUC: 0.702022 time eplapsed: 236.769034\n",
      "iteration: 1400 AUC: 0.893061 time eplapsed: 240.681225\n",
      "iteration: 1400 AUC: 0.906221 time eplapsed: 236.641998\n",
      "iteration: 1400 AUC: 0.791281 time eplapsed: 239.947172\n",
      "iteration: 1400 AUC: 0.899153 time eplapsed: 236.078244\n",
      "iteration: 1400 AUC: 0.642571 time eplapsed: 238.131471\n",
      "iteration: 1400 AUC: 0.686327 time eplapsed: 248.151094\n",
      "iteration: 1500 AUC: 0.875536 time eplapsed: 236.008414\n",
      "iteration: 1500 AUC: 0.868778 time eplapsed: 255.024954\n",
      "iteration: 1500 AUC: 0.194755 time eplapsed: 236.921316\n",
      "iteration: 1500 AUC: 0.684844 time eplapsed: 248.287750\n",
      "iteration: 1500 AUC: 0.907862 time eplapsed: 247.815682\n",
      "iteration: 1500 AUC: 0.897513 time eplapsed: 249.072967\n",
      "iteration: 1500 AUC: 0.895813 time eplapsed: 260.503451\n",
      "iteration: 1500 AUC: 0.875124 time eplapsed: 244.301982\n",
      "iteration: 1500 AUC: 0.372818 time eplapsed: 253.148879\n",
      "iteration: 1500 AUC: 0.811333 time eplapsed: 243.091603\n",
      "iteration: 1500 AUC: 0.867814 time eplapsed: 249.847888\n",
      "iteration: 1500 AUC: 0.581826 time eplapsed: 258.204868\n",
      "iteration: 1500 AUC: 0.895025 time eplapsed: 247.621680\n",
      "iteration: 1500 AUC: 0.626214 time eplapsed: 253.704661\n",
      "iteration: 1500 AUC: 0.652891 time eplapsed: 264.624093\n",
      "iteration: 1600 AUC: 0.858650 time eplapsed: 258.104503\n",
      "iteration: 1600 AUC: 0.904873 time eplapsed: 249.773843\n",
      "iteration: 1600 AUC: 0.308384 time eplapsed: 265.555459\n",
      "iteration: 1600 AUC: 0.901158 time eplapsed: 256.471961\n",
      "iteration: 1600 AUC: 0.798257 time eplapsed: 263.367191\n",
      "iteration: 1600 AUC: 0.891941 time eplapsed: 258.306712\n",
      "iteration: 1600 AUC: 0.907458 time eplapsed: 273.902028\n",
      "iteration: 1600 AUC: 0.905879 time eplapsed: 247.041191\n",
      "iteration: 1600 AUC: 0.851249 time eplapsed: 259.445139\n",
      "iteration: 1600 AUC: 0.554717 time eplapsed: 261.282192\n",
      "iteration: 1600 AUC: 0.895836 time eplapsed: 279.127307\n",
      "iteration: 1600 AUC: 0.905719 time eplapsed: 270.845190\n",
      "iteration: 1600 AUC: 0.832446 time eplapsed: 277.177404\n",
      "iteration: 1600 AUC: 0.703114 time eplapsed: 265.498329\n",
      "iteration: 1600 AUC: 0.728202 time eplapsed: 259.809795\n",
      "iteration: 1700 AUC: 0.870418 time eplapsed: 253.901070\n",
      "iteration: 1700 AUC: 0.848893 time eplapsed: 278.687924\n",
      "iteration: 1700 AUC: 0.320559 time eplapsed: 267.793823\n",
      "iteration: 1700 AUC: 0.695256 time eplapsed: 268.519661\n",
      "iteration: 1700 AUC: 0.890369 time eplapsed: 274.763853\n",
      "iteration: 1700 AUC: 0.854328 time eplapsed: 267.830138\n",
      "iteration: 1700 AUC: 0.892716 time eplapsed: 272.913877\n",
      "iteration: 1700 AUC: 0.750264 time eplapsed: 268.411364\n",
      "iteration: 1700 AUC: 0.893975 time eplapsed: 281.070290\n",
      "iteration: 1700 AUC: 0.861619 time eplapsed: 287.055684\n",
      "iteration: 1700 AUC: 0.475124 time eplapsed: 292.932729\n",
      "iteration: 1700 AUC: 0.289616 time eplapsed: 285.336677\n",
      "iteration: 1700 AUC: 0.868615 time eplapsed: 290.502419\n",
      "iteration: 1700 AUC: 0.530477 time eplapsed: 280.696130\n",
      "iteration: 1700 AUC: 0.460754 time eplapsed: 272.845228\n",
      "iteration: 1800 AUC: 0.896011 time eplapsed: 301.939495\n",
      "iteration: 1800 AUC: 0.906620 time eplapsed: 294.855072\n",
      "iteration: 1800 AUC: 0.472464 time eplapsed: 291.046861\n",
      "iteration: 1800 AUC: 0.882313 time eplapsed: 275.346558\n",
      "iteration: 1800 AUC: 0.834924 time eplapsed: 296.094035\n",
      "iteration: 1800 AUC: 0.886819 time eplapsed: 284.327977\n",
      "iteration: 1800 AUC: 0.881662 time eplapsed: 289.694795\n",
      "iteration: 1800 AUC: 0.790193 time eplapsed: 292.823212\n",
      "iteration: 1800 AUC: 0.895857 time eplapsed: 282.624827\n",
      "iteration: 1800 AUC: 0.895013 time eplapsed: 285.236255\n",
      "iteration: 1800 AUC: 0.895930 time eplapsed: 299.739533\n",
      "iteration: 1800 AUC: 0.594839 time eplapsed: 295.576051\n",
      "iteration: 1800 AUC: 0.900740 time eplapsed: 293.597057\n",
      "iteration: 1800 AUC: 0.657406 time eplapsed: 290.693575\n",
      "iteration: 1800 AUC: 0.344661 time eplapsed: 297.626351\n",
      "iteration: 1900 AUC: 0.892456 time eplapsed: 272.456365\n",
      "iteration: 1900 AUC: 0.887877 time eplapsed: 314.357751\n",
      "iteration: 1900 AUC: 0.900661 time eplapsed: 298.664397\n",
      "iteration: 1900 AUC: 0.781142 time eplapsed: 304.802716\n",
      "iteration: 1900 AUC: 0.907438 time eplapsed: 293.615184\n",
      "iteration: 1900 AUC: 0.873419 time eplapsed: 304.802276\n",
      "iteration: 1900 AUC: 0.898969 time eplapsed: 302.561897\n",
      "iteration: 1900 AUC: 0.838275 time eplapsed: 295.720247\n",
      "iteration: 1900 AUC: 0.898577 time eplapsed: 297.157326\n",
      "iteration: 1900 AUC: 0.884709 time eplapsed: 300.113051\n",
      "iteration: 1900 AUC: 0.894675 time eplapsed: 310.300403\n",
      "iteration: 1900 AUC: 0.701993 time eplapsed: 314.926739\n",
      "iteration: 1900 AUC: 0.900330 time eplapsed: 315.786423\n",
      "iteration: 1900 AUC: 0.575226 time eplapsed: 310.815368\n",
      "iteration: 1900 AUC: 0.712216 time eplapsed: 304.368530\n",
      "iteration: 2000 AUC: 0.897623 time eplapsed: 317.230482\n",
      "iteration: 100 AUC: 0.101144 time eplapsed: 13.937667\n",
      "iteration: 200 AUC: 0.097372 time eplapsed: 32.363832\n",
      "iteration: 2000 AUC: 0.837923 time eplapsed: 314.481825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n  File \"<ipython-input-18-fd22d75dc72b>\", line 67, in demo\n    fpr, tpr, _ = roc_curve(Y_test, np.dot(X_test,WT))\n  File \"/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/sklearn/metrics/ranking.py\", line 613, in roc_curve\n    y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n  File \"/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/sklearn/metrics/ranking.py\", line 398, in _binary_clf_curve\n    assert_all_finite(y_score)\n  File \"/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/sklearn/utils/validation.py\", line 68, in assert_all_finite\n    _assert_all_finite(X.data if sp.issparse(X) else X, allow_nan)\n  File \"/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/sklearn/utils/validation.py\", line 56, in _assert_all_finite\n    raise ValueError(msg_err.format(type_err, X.dtype))\nValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-35e4dca2c0c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0musps_hinge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musps_hinge_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musps_hinge_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'usps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hinge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PGSPD'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLAM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTHETA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total elapsed time: %f'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrun_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-84100d25d341>\u001b[0m in \u001b[0;36mgs\u001b[0;34m(dataset, loss, alg, folders, GAMMA, LAM, THETA, C)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# grid search run on multiprocessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_cpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mresults_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_run\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_paras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100 AUC: 0.877416 time eplapsed: 15.031946\n",
      "iteration: 2000 AUC: 0.578624 time eplapsed: 321.224959\n",
      "iteration: 2000 AUC: 0.904158 time eplapsed: 331.695915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:18: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100 AUC: 0.101144 time eplapsed: 16.978620\n",
      "iteration: 100 AUC: 0.867755 time eplapsed: 16.158700\n",
      "iteration: 2000 AUC: 0.871026 time eplapsed: 330.630117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in double_scalars\n",
      "  app.launch_new_instance()\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:18: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 200 AUC: 0.915980 time eplapsed: 41.775288\n",
      "iteration: 300 AUC: 0.852729 time eplapsed: 66.287278\n",
      "iteration: 2000 AUC: 0.891756 time eplapsed: 347.537434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100 AUC: 0.801641 time eplapsed: 18.907396\n",
      "iteration: 2000 AUC: 0.809866 time eplapsed: 333.753017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100 AUC: 0.743900 time eplapsed: 21.125965\n",
      "iteration: 2000 AUC: 0.901125 time eplapsed: 355.767897\n",
      "iteration: 200 AUC: 0.097372 time eplapsed: 47.310659\n",
      "iteration: 100 AUC: 0.717481 time eplapsed: 16.719806\n",
      "iteration: 200 AUC: 0.602516 time eplapsed: 52.744552\n",
      "iteration: 2000 AUC: 0.900872 time eplapsed: 361.776330\n",
      "iteration: 100 AUC: 0.524965 time eplapsed: 21.743180\n",
      "iteration: 200 AUC: 0.897580 time eplapsed: 51.069170\n",
      "iteration: 100 AUC: 0.845117 time eplapsed: 21.923760\n",
      "iteration: 300 AUC: 0.925536 time eplapsed: 69.787021\n",
      "iteration: 200 AUC: 0.767132 time eplapsed: 54.279252\n",
      "iteration: 200 AUC: 0.480839 time eplapsed: 52.317259\n",
      "iteration: 300 AUC: 0.834668 time eplapsed: 68.420807\n",
      "iteration: 200 AUC: 0.488819 time eplapsed: 49.416291\n",
      "iteration: 400 AUC: 0.875734 time eplapsed: 100.266856\n",
      "iteration: 2000 AUC: 0.534690 time eplapsed: 371.315235\n",
      "iteration: 2000 AUC: 0.865313 time eplapsed: 390.044994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 200 AUC: 0.901963 time eplapsed: 46.252683\n",
      "iteration: 2000 AUC: 0.898455 time eplapsed: 395.320858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:18: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 300 AUC: 0.794445 time eplapsed: 80.674195\n",
      "iteration: 2000 AUC: 0.898315 time eplapsed: 394.821328\n",
      "iteration: 100 AUC: 0.906651 time eplapsed: 24.595155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/neyo/PycharmProjects/AUC/venv/lib/python3.5/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100 AUC: 0.773422 time eplapsed: 24.678809\n",
      "iteration: 2000 AUC: 0.654841 time eplapsed: 405.044140\n",
      "iteration: 2000 AUC: 0.328991 time eplapsed: 388.430632\n",
      "iteration: 300 AUC: 0.899193 time eplapsed: 84.060402\n",
      "iteration: 100 AUC: 0.821152 time eplapsed: 28.564067\n",
      "iteration: 300 AUC: 0.870160 time eplapsed: 72.525721\n",
      "iteration: 100 AUC: 0.865566 time eplapsed: 27.148165\n",
      "iteration: 400 AUC: 0.869614 time eplapsed: 88.019900\n",
      "iteration: 100 AUC: 0.898135 time eplapsed: 25.430711\n",
      "iteration: 100 AUC: 0.524965 time eplapsed: 26.823241\n",
      "iteration: 300 AUC: 0.784053 time eplapsed: 90.407627\n",
      "iteration: 200 AUC: 0.885831 time eplapsed: 51.966971\n",
      "iteration: 300 AUC: 0.902572 time eplapsed: 92.014251\n",
      "iteration: 200 AUC: 0.896955 time eplapsed: 60.692423\n",
      "iteration: 300 AUC: 0.856934 time eplapsed: 93.806286\n",
      "iteration: 200 AUC: 0.902876 time eplapsed: 56.176126\n",
      "iteration: 400 AUC: 0.854765 time eplapsed: 111.939065\n",
      "iteration: 200 AUC: 0.909298 time eplapsed: 54.355729\n",
      "iteration: 200 AUC: 0.881653 time eplapsed: 53.411406\n",
      "iteration: 200 AUC: 0.488819 time eplapsed: 52.043153\n",
      "iteration: 500 AUC: 0.898902 time eplapsed: 131.536669\n",
      "iteration: 400 AUC: 0.785238 time eplapsed: 116.009004\n",
      "iteration: 400 AUC: 0.861296 time eplapsed: 101.026469\n",
      "iteration: 400 AUC: 0.746772 time eplapsed: 99.870172\n",
      "iteration: 300 AUC: 0.900951 time eplapsed: 78.924899\n",
      "iteration: 300 AUC: 0.906719 time eplapsed: 73.583242\n",
      "iteration: 400 AUC: 0.733726 time eplapsed: 101.496077\n",
      "iteration: 300 AUC: 0.909964 time eplapsed: 77.208375\n",
      "iteration: 500 AUC: 0.903809 time eplapsed: 125.939775\n",
      "iteration: 300 AUC: 0.904062 time eplapsed: 73.809041\n",
      "iteration: 300 AUC: 0.804954 time eplapsed: 74.296814\n",
      "iteration: 400 AUC: 0.859690 time eplapsed: 97.959772\n",
      "iteration: 300 AUC: 0.834426 time eplapsed: 75.564113\n",
      "iteration: 400 AUC: 0.890251 time eplapsed: 89.300122\n",
      "iteration: 500 AUC: 0.914417 time eplapsed: 108.652245\n",
      "iteration: 500 AUC: 0.861822 time eplapsed: 111.851353\n",
      "iteration: 400 AUC: 0.812527 time eplapsed: 92.766445\n",
      "iteration: 400 AUC: 0.888342 time eplapsed: 91.083921\n",
      "iteration: 600 AUC: 0.838232 time eplapsed: 130.297055\n",
      "iteration: 500 AUC: 0.912229 time eplapsed: 117.553135\n",
      "iteration: 500 AUC: 0.800468 time eplapsed: 111.947288\n",
      "iteration: 400 AUC: 0.872446 time eplapsed: 90.876882\n",
      "iteration: 400 AUC: 0.873061 time eplapsed: 93.045703\n",
      "iteration: 400 AUC: 0.842558 time eplapsed: 90.270634\n",
      "iteration: 400 AUC: 0.823123 time eplapsed: 92.396197\n",
      "iteration: 500 AUC: 0.768780 time eplapsed: 114.989560\n",
      "iteration: 500 AUC: 0.898483 time eplapsed: 104.570986\n",
      "iteration: 500 AUC: 0.906168 time eplapsed: 112.964028\n",
      "iteration: 600 AUC: 0.868632 time eplapsed: 125.455358\n",
      "iteration: 600 AUC: 0.686329 time eplapsed: 126.968856\n",
      "iteration: 500 AUC: 0.881077 time eplapsed: 116.625014\n",
      "iteration: 500 AUC: 0.850347 time eplapsed: 108.364798\n",
      "iteration: 500 AUC: 0.891344 time eplapsed: 100.009672\n",
      "iteration: 600 AUC: 0.798509 time eplapsed: 126.556218\n",
      "iteration: 500 AUC: 0.918839 time eplapsed: 99.047380\n",
      "iteration: 500 AUC: 0.891443 time eplapsed: 109.767715\n",
      "iteration: 600 AUC: 0.859809 time eplapsed: 124.078026\n",
      "iteration: 500 AUC: 0.875855 time eplapsed: 109.992887\n",
      "iteration: 600 AUC: 0.768175 time eplapsed: 128.505038\n",
      "iteration: 600 AUC: 0.702258 time eplapsed: 121.374633\n",
      "iteration: 700 AUC: 0.905770 time eplapsed: 144.952570\n",
      "iteration: 600 AUC: 0.839746 time eplapsed: 112.690270\n",
      "iteration: 600 AUC: 0.893170 time eplapsed: 125.089896\n",
      "iteration: 700 AUC: 0.901487 time eplapsed: 142.446866\n",
      "iteration: 700 AUC: 0.872833 time eplapsed: 135.117331\n",
      "iteration: 600 AUC: 0.640752 time eplapsed: 124.084638\n",
      "iteration: 600 AUC: 0.860840 time eplapsed: 129.639874\n",
      "iteration: 600 AUC: 0.868383 time eplapsed: 125.395432\n",
      "iteration: 600 AUC: 0.861627 time eplapsed: 120.333939\n",
      "iteration: 600 AUC: 0.861377 time eplapsed: 124.527394\n",
      "iteration: 600 AUC: 0.223452 time eplapsed: 126.253340\n",
      "iteration: 700 AUC: 0.799715 time eplapsed: 142.012038\n",
      "iteration: 700 AUC: 0.815891 time eplapsed: 134.829208\n",
      "iteration: 700 AUC: 0.510472 time eplapsed: 129.978798\n",
      "iteration: 700 AUC: 0.742885 time eplapsed: 137.386329\n",
      "iteration: 800 AUC: 0.874856 time eplapsed: 151.530751\n",
      "iteration: 700 AUC: 0.899639 time eplapsed: 143.101068\n",
      "iteration: 700 AUC: 0.907464 time eplapsed: 146.327052\n",
      "iteration: 800 AUC: 0.913801 time eplapsed: 147.059628\n",
      "iteration: 700 AUC: 0.428655 time eplapsed: 136.603220\n",
      "iteration: 800 AUC: 0.848225 time eplapsed: 148.704882\n",
      "iteration: 700 AUC: 0.865968 time eplapsed: 132.842323\n",
      "iteration: 700 AUC: 0.858156 time eplapsed: 136.936793\n",
      "iteration: 700 AUC: 0.879419 time eplapsed: 134.163808\n",
      "iteration: 700 AUC: 0.895073 time eplapsed: 129.824307\n",
      "iteration: 700 AUC: 0.466001 time eplapsed: 135.152020\n",
      "iteration: 800 AUC: 0.806374 time eplapsed: 151.332568\n",
      "iteration: 800 AUC: 0.581206 time eplapsed: 153.220076\n",
      "iteration: 800 AUC: 0.876479 time eplapsed: 158.792491\n",
      "iteration: 800 AUC: 0.696915 time eplapsed: 153.596754\n",
      "iteration: 800 AUC: 0.871759 time eplapsed: 147.084128\n",
      "iteration: 900 AUC: 0.888649 time eplapsed: 152.535720\n",
      "iteration: 800 AUC: 0.915507 time eplapsed: 148.587438\n",
      "iteration: 900 AUC: 0.915404 time eplapsed: 165.637511\n",
      "iteration: 800 AUC: 0.762040 time eplapsed: 155.662639\n",
      "iteration: 800 AUC: 0.906595 time eplapsed: 151.831861\n",
      "iteration: 800 AUC: 0.898340 time eplapsed: 153.085955\n",
      "iteration: 800 AUC: 0.908585 time eplapsed: 151.405572\n",
      "iteration: 900 AUC: 0.906170 time eplapsed: 161.845345\n",
      "iteration: 800 AUC: 0.908543 time eplapsed: 161.277773\n",
      "iteration: 800 AUC: 0.871108 time eplapsed: 156.082485\n",
      "iteration: 900 AUC: 0.630174 time eplapsed: 169.165747\n",
      "iteration: 900 AUC: 0.350305 time eplapsed: 164.067186\n",
      "iteration: 900 AUC: 0.248565 time eplapsed: 171.712740\n",
      "iteration: 900 AUC: 0.273815 time eplapsed: 161.881625\n",
      "iteration: 900 AUC: 0.880618 time eplapsed: 161.742693\n",
      "iteration: 900 AUC: 0.912816 time eplapsed: 162.716274\n",
      "iteration: 1000 AUC: 0.873355 time eplapsed: 180.752154\n",
      "iteration: 900 AUC: 0.894248 time eplapsed: 159.076377\n",
      "iteration: 900 AUC: 0.824754 time eplapsed: 157.838445\n",
      "iteration: 1000 AUC: 0.902375 time eplapsed: 185.571859\n",
      "iteration: 900 AUC: 0.835011 time eplapsed: 168.069991\n",
      "iteration: 900 AUC: 0.836502 time eplapsed: 168.137382\n",
      "iteration: 900 AUC: 0.822813 time eplapsed: 174.862694\n",
      "iteration: 900 AUC: 0.859891 time eplapsed: 160.602891\n",
      "iteration: 1000 AUC: 0.531114 time eplapsed: 176.844942\n",
      "iteration: 1000 AUC: 0.720628 time eplapsed: 172.303814\n",
      "iteration: 1000 AUC: 0.854387 time eplapsed: 185.273232\n",
      "iteration: 1000 AUC: 0.868748 time eplapsed: 184.646359\n",
      "iteration: 1000 AUC: 0.796374 time eplapsed: 184.235341\n",
      "iteration: 1000 AUC: 0.899482 time eplapsed: 176.704015\n",
      "iteration: 1000 AUC: 0.879270 time eplapsed: 183.175791\n",
      "iteration: 1100 AUC: 0.904142 time eplapsed: 200.428837\n",
      "iteration: 1000 AUC: 0.900643 time eplapsed: 178.189168\n",
      "iteration: 1000 AUC: 0.391886 time eplapsed: 176.705775\n",
      "iteration: 1000 AUC: 0.859560 time eplapsed: 183.101411\n",
      "iteration: 1100 AUC: 0.868303 time eplapsed: 192.473256\n",
      "iteration: 1000 AUC: 0.865448 time eplapsed: 177.080135\n",
      "iteration: 1000 AUC: 0.287754 time eplapsed: 175.885578\n",
      "iteration: 1000 AUC: 0.846979 time eplapsed: 183.674541\n",
      "iteration: 1100 AUC: 0.877977 time eplapsed: 182.120156\n",
      "iteration: 1100 AUC: 0.656667 time eplapsed: 186.928814\n",
      "iteration: 1100 AUC: 0.689846 time eplapsed: 199.348976\n",
      "iteration: 1100 AUC: 0.341876 time eplapsed: 187.779899\n",
      "iteration: 1100 AUC: 0.863121 time eplapsed: 180.440876\n",
      "iteration: 1100 AUC: 0.723942 time eplapsed: 208.172859\n",
      "iteration: 1100 AUC: 0.898492 time eplapsed: 194.810644\n",
      "iteration: 1200 AUC: 0.898662 time eplapsed: 214.800573\n",
      "iteration: 1100 AUC: 0.256194 time eplapsed: 195.015417\n",
      "iteration: 1100 AUC: 0.802634 time eplapsed: 196.678645\n",
      "iteration: 1100 AUC: 0.824010 time eplapsed: 202.971217\n",
      "iteration: 1100 AUC: 0.835533 time eplapsed: 191.996069\n",
      "iteration: 1100 AUC: 0.505446 time eplapsed: 194.485689\n",
      "iteration: 1200 AUC: 0.877075 time eplapsed: 192.444233\n",
      "iteration: 1100 AUC: 0.852838 time eplapsed: 197.582440\n",
      "iteration: 1200 AUC: 0.877562 time eplapsed: 207.717186\n",
      "iteration: 1200 AUC: 0.623384 time eplapsed: 198.618160\n",
      "iteration: 1200 AUC: 0.886399 time eplapsed: 196.846621\n",
      "iteration: 1200 AUC: 0.836446 time eplapsed: 209.801365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1200 AUC: 0.778759 time eplapsed: 206.328303\n",
      "iteration: 1200 AUC: 0.892914 time eplapsed: 206.679784\n",
      "iteration: 1200 AUC: 0.881765 time eplapsed: 208.313877\n",
      "iteration: 1200 AUC: 0.713594 time eplapsed: 205.785928\n",
      "iteration: 1200 AUC: 0.882982 time eplapsed: 207.417614\n",
      "iteration: 1200 AUC: 0.849796 time eplapsed: 208.674839\n",
      "iteration: 1300 AUC: 0.903326 time eplapsed: 229.445761\n",
      "iteration: 1200 AUC: 0.863236 time eplapsed: 200.883970\n",
      "iteration: 1200 AUC: 0.891344 time eplapsed: 213.402394\n",
      "iteration: 1200 AUC: 0.886898 time eplapsed: 207.226332\n",
      "iteration: 1300 AUC: 0.883928 time eplapsed: 218.987850\n",
      "iteration: 1300 AUC: 0.905150 time eplapsed: 213.029779\n",
      "iteration: 1300 AUC: 0.725798 time eplapsed: 223.554361\n",
      "iteration: 1300 AUC: 0.481298 time eplapsed: 207.781207\n",
      "iteration: 1300 AUC: 0.909435 time eplapsed: 227.209880\n",
      "iteration: 1300 AUC: 0.799499 time eplapsed: 229.591065\n",
      "iteration: 1300 AUC: 0.902802 time eplapsed: 219.549040\n",
      "iteration: 1300 AUC: 0.874077 time eplapsed: 236.063601\n",
      "iteration: 1300 AUC: 0.799022 time eplapsed: 223.725718\n",
      "iteration: 1300 AUC: 0.891520 time eplapsed: 221.766026\n",
      "iteration: 1300 AUC: 0.870050 time eplapsed: 223.879529\n",
      "iteration: 1300 AUC: 0.852842 time eplapsed: 223.356856\n",
      "iteration: 1300 AUC: 0.894133 time eplapsed: 226.510093\n",
      "iteration: 1400 AUC: 0.905766 time eplapsed: 236.742515\n",
      "iteration: 1300 AUC: 0.900451 time eplapsed: 223.363992\n",
      "iteration: 1400 AUC: 0.900290 time eplapsed: 234.056981\n",
      "iteration: 1400 AUC: 0.894031 time eplapsed: 236.751394\n",
      "iteration: 1400 AUC: 0.712191 time eplapsed: 235.722471\n",
      "iteration: 1400 AUC: 0.811166 time eplapsed: 235.477314\n",
      "iteration: 1400 AUC: 0.906097 time eplapsed: 230.842633\n",
      "iteration: 1400 AUC: 0.863386 time eplapsed: 239.849015\n",
      "iteration: 1400 AUC: 0.894323 time eplapsed: 244.356435\n",
      "iteration: 1400 AUC: 0.894860 time eplapsed: 233.323132\n",
      "iteration: 1400 AUC: 0.819383 time eplapsed: 237.924270\n",
      "iteration: 1400 AUC: 0.903791 time eplapsed: 240.037407\n",
      "iteration: 1400 AUC: 0.816613 time eplapsed: 237.020088\n",
      "iteration: 1400 AUC: 0.916030 time eplapsed: 241.921281\n",
      "iteration: 1400 AUC: 0.890304 time eplapsed: 230.602203\n",
      "iteration: 1400 AUC: 0.903255 time eplapsed: 237.120404\n",
      "iteration: 1500 AUC: 0.868756 time eplapsed: 246.268298\n",
      "iteration: 1500 AUC: 0.334560 time eplapsed: 243.808748\n",
      "iteration: 1500 AUC: 0.904204 time eplapsed: 258.585228\n",
      "iteration: 1500 AUC: 0.698841 time eplapsed: 240.699310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-11:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Process ForkPoolWorker-7:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-20-84100d25d341>\", line 5, in single_run\n",
      "    ,gamma=GAMMA[gamma],lam=LAM[lam],theta=THETA[theta],c=C[c])\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 33, in neg\n",
      "    gfnt += beta*(k-i)/minus  # no xt yet!\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 8, in <lambda>\n",
      "    loss = lambda x: max(0,1+L-2*L*x)\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 33, in neg\n",
      "    gfnt += beta*(k-i)/minus  # no xt yet!\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 8, in <lambda>\n",
      "    loss = lambda x: max(0,1+L-2*L*x)\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 29, in neg\n",
      "    beta = comb_dict[N][k]*comb_dict[k][i]*(N+1)*delta/(2*L)**k*minus**(k-i)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 8, in <lambda>\n",
      "    loss = lambda x: max(0,1+L-2*L*x)\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 8, in <lambda>\n",
      "    loss = lambda x: max(0,1+L-2*L*x)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 25, in prox\n",
      "    gradalphat = alpha_grad(fpt[i],fnt[i],y,alphaj[i])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 20, in prox\n",
      "    fnt[i],gfnt[i] = neg(loss,i,prod,L)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-12-408fd4f896d3>\", line 7, in alpha_grad\n",
      "    gradalphat = -2*(alphat - fpt)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-6-418faf4458ea>\", line 25, in neg\n",
      "    delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-18-fd22d75dc72b>\", line 45, in demo\n",
      "    WT,AT,BT,ALPHAT = PGSPD(t,loss,x_train,y_train,L,gamma,lam,theta,c,WT,AT,BT,ALPHAT)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-16-8fe792301771>\", line 21, in PGSPD\n",
      "    Wt,At,Bt,ALPHAt = prox(ETAt,loss,X[j],Y[j],L,gamma,lam,Wt,At,Bt,ALPHAt,bwt,bat,bbt,balphat)\n",
      "  File \"<ipython-input-15-4097c1d9a577>\", line 33, in prox\n",
      "    wj = wj - eta*(gradwt*x*y/(N+1) + lam*wj + gamma*(wj - bwt))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "run_time = time.time()\n",
    "usps_hinge,usps_hinge_mean,usps_hinge_std = gs('usps','hinge','PGSPD',5,GAMMA,LAM,THETA,C)\n",
    "print('total elapsed time: %f' %(time.time() - run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'usps_hinge_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-16d8e05b8584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musps_hinge_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'usps_hinge_mean' is not defined"
     ]
    }
   ],
   "source": [
    "usps_hinge_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
