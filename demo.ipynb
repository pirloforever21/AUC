{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from math import fabs, sqrt, log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound(N, loss, L):\n",
    "    '''\n",
    "    Calculate annoying parameters to estimate rho\n",
    "    '''\n",
    "\n",
    "    R1 = 0.0\n",
    "    R2 = 0.0\n",
    "    Sp1 = 0.0\n",
    "    Sm1 = 0.0\n",
    "    Sp2 = 0.0\n",
    "    Sm2 = 0.0\n",
    "    for i in range(N + 1):\n",
    "        # compute plus\n",
    "        alpha0 = L ** i\n",
    "        alpha1 = i * L ** (i - 1)\n",
    "        alpha2 = i * (i - 1) * L ** (i - 2)\n",
    "        R1 += alpha0\n",
    "        Sp1 += alpha1\n",
    "        Sp2 += alpha2\n",
    "        # compute minus\n",
    "        beta0 = 0.0\n",
    "        beta1 = 0.0\n",
    "        beta2 = 0.0\n",
    "        for k in range(i, N + 1):\n",
    "            # compute forward difference\n",
    "            delta = 0.0\n",
    "            for j in range(k + 1):\n",
    "                delta += comb_dict[k][j] * (-1) ** (k - j) * loss(j / N)\n",
    "            # compute coefficient\n",
    "            beta0 += comb_dict[N][k] * comb_dict[k][i] * (N + 1) * fabs(delta) / (2 ** k) / (L ** i)\n",
    "            beta1 += comb_dict[N][k] * comb_dict[k][i] * (N + 1) * (k - i) * fabs(delta) / (2 ** k) / (L ** (i + 1))\n",
    "            beta2 += comb_dict[N][k] * comb_dict[k][i] * (N + 1) * (k - i) * (k - i - 1) * fabs(delta) / (2 ** k) / (\n",
    "                        L ** (i + 2))\n",
    "        R2 += beta0\n",
    "        Sm1 += beta1\n",
    "        Sm2 += beta2\n",
    "\n",
    "    gamma = max((2 * R1 + R2) * Sp2 + Sp1 ** 2, (2 * R2 + R1) * Sm2 + Sm1 ** 2) / (N + 1)\n",
    "\n",
    "    return R1, R2, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bern_loss_func(name, L):\n",
    "    \n",
    "    '''\n",
    "    Define loss function\n",
    "\n",
    "    input:\n",
    "        name - name of loss funtion\n",
    "        L - bound for prod\n",
    "\n",
    "    output:\n",
    "        loss - loss function\n",
    "    '''\n",
    "    \n",
    "    if name == 'hinge':\n",
    "        loss = lambda x: max(0, 1 + L - 2 * L * x)\n",
    "    elif name == 'logistic':\n",
    "        loss = lambda x: log(1 + exp(L - 2 * L * x))\n",
    "    else:\n",
    "        print('Wrong loss function!')\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(i,prod,L):\n",
    "    '''\n",
    "    Compute positive function and gradient information\n",
    "    \n",
    "    input:\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fpt - positive function value\n",
    "        gfpt - positive function gradient\n",
    "        wasted - time wasted on computing\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    plus = L/2+prod\n",
    "    fpt = plus**i\n",
    "    gfpt = fpt*i/plus # no xt yet!\n",
    "    \n",
    "    wasted = time.time() - start\n",
    "    \n",
    "    return fpt,gfpt,wasted              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n, k):\n",
    "    '''\n",
    "    Compute combination\n",
    "    \n",
    "    input:\n",
    "        n - total number\n",
    "        k - number of chosen\n",
    "    \n",
    "    output:\n",
    "        c - number of combination\n",
    "    '''\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_dict = {0:{0:1},1:{0:1,1:1},2:{0:1,1:2,2:1},3:{0:1,1:3,2:3,3:1},4:{0:1,1:4,2:6,3:4,4:1},\n",
    "             5:{0:1,1:5,2:10,3:10,4:5,5:1},6:{0:1,1:6,2:15,3:20,4:15,5:6,6:1},\n",
    "             7:{0:1,1:7,2:21,3:35,4:35,5:21,6:7,7:1},8:{0:1,1:8,2:28,3:56,4:70,5:56,6:28,7:8,8:1},\n",
    "             9:{0:1,1:9,2:36,3:84,4:126,5:126,6:84,7:36,8:9,9:1},\n",
    "             10:{0:1,1:10,2:45,3:120,4:210,5:252,6:210,7:120,8:45,9:10,10:1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(N,loss,i,prod,L):\n",
    "    '''\n",
    "    Compute negative function and gradient information\n",
    "    \n",
    "    input:\n",
    "        loss - loss function\n",
    "        i - index of function\n",
    "        t - iteration\n",
    "        prod - wt*xt\n",
    "        \n",
    "    output:\n",
    "        fnt - negative function value\n",
    "        gfnt - negative function gradient\n",
    "        wasted - time wasted on computing\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    minus = L/2-prod\n",
    "    fnt = 0.0\n",
    "    gfnt = 0.0\n",
    "    for k in range(i,N+1):\n",
    "        # compute forward difference\n",
    "        delta = 0.0\n",
    "        for j in range(k+1):\n",
    "            delta += comb_dict[k][j]*(-1)**(k-j)*loss(j/N)\n",
    "            \n",
    "        # compute coefficient\n",
    "        beta = (comb_dict[N][k]*comb_dict[k][i]*(N+1)*delta/((2*L)**k))*(minus**(k-i))\n",
    "        # compute function value\n",
    "        fnt += beta\n",
    "        # compute gradient\n",
    "        gfnt += beta*(k-i)/minus  # no xt yet!\n",
    "    \n",
    "    wasted = time.time() - start\n",
    "    return fnt,gfnt,wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_grad(gfpt,gfnt,yt,at,bt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to w\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        gfpt - positive function gradient at t\n",
    "        fnt - negative function at t\n",
    "        gfnt - negative function gradient at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "        bt - b at t\n",
    "        alphat - alpha at t\n",
    "    output:\n",
    "        gradwt - gradient w.r.t. w at t\n",
    "    '''\n",
    "    if yt == 1:\n",
    "        gradwt = 2*(alphat - at)*gfpt\n",
    "    else:\n",
    "        gradwt = 2*(alphat - bt)*gfnt\n",
    "    return gradwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(x,R):\n",
    "    '''\n",
    "    Projection\n",
    "    \n",
    "    input:\n",
    "        x - \n",
    "        R - radius\n",
    "        \n",
    "    output:\n",
    "        proj - projected \n",
    "    '''\n",
    "    norm = np.linalg.norm(x)\n",
    "    if norm > R:\n",
    "        x = x/norm*R\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_grad(fpt,yt,at):\n",
    "    '''\n",
    "    Gradient with respect to a\n",
    "    \n",
    "    input:\n",
    "        fpt - positive function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        at - a at t\n",
    "    \n",
    "    output:\n",
    "        gradat - gradient w.r.t a at t\n",
    "    '''\n",
    "    gradat = 0.0 \n",
    "    if yt == 1:\n",
    "        gradat = 2*(at - fpt)\n",
    "    else:\n",
    "        gradat = 2*at\n",
    "    return gradat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_grad(fnt,yt,bt):\n",
    "    '''\n",
    "    Gradient with respect to b\n",
    "    \n",
    "    input:\n",
    "        fnt - negative function at t\n",
    "        yt - sample label at t\n",
    "        pt - p at t\n",
    "        bt - b at t\n",
    "    \n",
    "    output:\n",
    "        gradbt - gradient w.r.t b at t\n",
    "    '''\n",
    "    gradbt = 0.0 \n",
    "    if yt == 1:\n",
    "        gradbt = 2*bt\n",
    "    else:\n",
    "        gradbt = 2*(bt - fnt)\n",
    "    return gradbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_grad(fpt,fnt,yt,alphat):\n",
    "    '''\n",
    "    Gradient with respect to alpha\n",
    "    '''\n",
    "    gradalphat = 0.0\n",
    "    if yt == 1:\n",
    "        gradalphat = -2*(alphat - fpt)\n",
    "    else:\n",
    "        gradalphat = -2*(alphat - fnt)\n",
    "    return gradalphat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox(N, eta, loss, index, X, Y, L, R1, R2, gamma, wj, aj, bj, alphaj, bwt):\n",
    "    '''\n",
    "    perform proximal guided gradient descent when receive an sample\n",
    "\n",
    "    input:\n",
    "        N - \n",
    "        eta - step size\n",
    "        loss - loss function\n",
    "        index -\n",
    "        X - sample features\n",
    "        Y - sample labels\n",
    "        L -\n",
    "        R1 -\n",
    "        R2 -\n",
    "        gamma -\n",
    "        wj -\n",
    "        aj -\n",
    "        bj -\n",
    "        alphaj -\n",
    "        bwt -\n",
    "\n",
    "    output:\n",
    "        wj - w at jth step\n",
    "        aj -\n",
    "        bj -\n",
    "        alphaj -\n",
    "        wasted - time wasted on computing\n",
    "    '''\n",
    "\n",
    "    prod = np.dot(wj, X[index])\n",
    "    wasted = 0.0\n",
    "    fpt = np.zeros(N + 1)\n",
    "    gfpt = np.zeros(N + 1)\n",
    "    fnt = np.zeros(N + 1)\n",
    "    gfnt = np.zeros(N + 1)\n",
    "    gradwt = 0.0\n",
    "    \n",
    "    for i in range(N + 1):\n",
    "        fpt[i], gfpt[i], _ = pos(i, prod, L)\n",
    "        wasted += _\n",
    "        fnt[i], gfnt[i], _ = neg(N, loss, i, prod, L)\n",
    "        wasted += _\n",
    "        gradwt += w_grad(gfpt[i], gfnt[i], Y[index], aj[i], bj[i], alphaj[i])  # accumulate i\n",
    "        gradat = a_grad(fpt[i], Y[index], aj[i])\n",
    "        gradbt = b_grad(fnt[i], Y[index], bj[i])\n",
    "        gradalphat = alpha_grad(fpt[i], fnt[i], Y[index], alphaj[i])\n",
    "\n",
    "        aj[i] = aj[i] - eta * gradat / (2 * (N + 1))\n",
    "        bj[i] = bj[i] - eta * gradbt / (2 * (N + 1))\n",
    "        alphaj[i] = alphaj[i] + eta * gradalphat / (2 * (N + 1))\n",
    "\n",
    "    wj = wj - eta * (gradwt * X[index] * Y[index] / (2 * (N + 1)) + gamma * (wj - bwt))\n",
    "\n",
    "    wj = proj(wj, L / 2)\n",
    "    aj = proj(aj, R1)\n",
    "    bj = proj(bj, R2)\n",
    "    alphaj = proj(alphaj, R1 + R2)\n",
    "\n",
    "    return wj, aj, bj, alphaj, wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGSPD(N, t, loss, passing_list, X, Y, L, R1, R2, gamma, c, bwt, bat, bbt, balphat):\n",
    "\n",
    "    '''\n",
    "    Proximally Guided Stochastic Primal Dual Inner loop\n",
    "\n",
    "    input:\n",
    "        N - \n",
    "        t - iteration at t\n",
    "        loss - loss function\n",
    "        passing_list\n",
    "        X -\n",
    "        Y -\n",
    "        L -\n",
    "        R1 -\n",
    "        R2 -\n",
    "        gamma -\n",
    "        c -\n",
    "        bwt - last outer loop w\n",
    "        bat - last outer loop a\n",
    "        bbt - last outer loop b\n",
    "        balphat - last outer loop alpha\n",
    "\n",
    "    output:\n",
    "        bwt - next outer loop w\n",
    "        bat - next outer loop a\n",
    "        bbt - next outer loop b\n",
    "        balphat - next outer loop alpha\n",
    "        wasted - time wasted on computing\n",
    "    '''\n",
    "    \n",
    "    wasted = 0.0\n",
    "    # initialize inner loop variables\n",
    "    Wt = bwt + 0.0\n",
    "    At = bat + 0.0\n",
    "    Bt = bbt + 0.0\n",
    "    ALPHAt = balphat + 0.0\n",
    "\n",
    "    BWt = 0.0\n",
    "    BAt = 0.0\n",
    "    BBt = 0.0\n",
    "    BALPHAt = 0.0\n",
    "\n",
    "    ETAt = c / sqrt(t) / gamma\n",
    "    \n",
    "    \n",
    "\n",
    "    # inner loop update at j\n",
    "    for j in range(t):\n",
    "        # update inner loop variables\n",
    "        Wt, At, Bt, ALPHAt, _ = prox(N, ETAt, loss, passing_list[j], X, Y, L, R1, R2, gamma, Wt, At, Bt, ALPHAt, bwt)\n",
    "        wasted += _\n",
    "        \n",
    "        BWt += Wt\n",
    "        BAt += At\n",
    "        BBt += Bt\n",
    "        BALPHAt += ALPHAt\n",
    "\n",
    "    # update outer loop variables\n",
    "    bwt = BWt / t\n",
    "    bat = BAt / t\n",
    "    bbt = BBt / t\n",
    "    balphat = BALPHAt / t\n",
    "\n",
    "    return bwt, bat, bbt, balphat, wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAUC(T,name,N,L,c,Xtr,Ytr,Xte,Yte,stamp = 10):\n",
    "    '''\n",
    "    Stochastic AUC Optimization with General Loss\n",
    "\n",
    "    input:\n",
    "        T -\n",
    "        name -\n",
    "        N - Bernstein degree\n",
    "        L - Bound for prod\n",
    "        c - step size parameter\n",
    "        Xtr - Training features\n",
    "        Ytr - Training labels\n",
    "        Xte - Testing features\n",
    "        Yte - Testing labels\n",
    "        stamp - record stamp\n",
    "\n",
    "    output:\n",
    "        elapsed_time -\n",
    "        roc_auc - auc scores\n",
    "    '''\n",
    "    \n",
    "    sum_time = 0.0\n",
    "    \n",
    "    wasted = 0.0 \n",
    "    \n",
    "    print('SAUC......')\n",
    "    \n",
    "    # get the dimension of what we are working with\n",
    "    n, d = Xtr.shape\n",
    "\n",
    "    WT = np.zeros(d)\n",
    "    AT = np.zeros(N + 1)\n",
    "    BT = np.zeros(N + 1)\n",
    "    ALPHAT = np.zeros(N + 1)\n",
    "\n",
    "    # define loss function\n",
    "    loss = bern_loss_func(name, L)\n",
    "\n",
    "    # compute gamma(get it done, bitch!)\n",
    "    R1, R2, gamma = bound(N,loss,L)\n",
    "\n",
    "    # record auc\n",
    "    roc_auc = []\n",
    "\n",
    "    # record time elapsed\n",
    "    elapsed_time = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Begin algorithm\n",
    "    for t in range(1, T + 1):\n",
    "\n",
    "        # Prepare the indices if you know what I mean\n",
    "        prep_time = time.time()\n",
    "        epoch = t // n\n",
    "        begin = (t * (t - 1) // 2) % n\n",
    "        end = (t * (t + 1) // 2) % n\n",
    "        if epoch < 1:\n",
    "            if begin < end:\n",
    "                tr_list = [i for i in range(begin, end)]\n",
    "            else:  # need to think better\n",
    "                tr_list = [i for i in range(begin, n)] + [i for i in range(end)]\n",
    "        else:\n",
    "            if begin < end:\n",
    "                tr_list = [i for i in range(begin, n)] + [i for i in range(n)] * (epoch - 1) + [i for i in range(end)]\n",
    "            else:\n",
    "                tr_list = [i for i in range(begin, n)] + [i for i in range(n)] * epoch + [i for i in range(end)]\n",
    "        sum_time += time.time() - prep_time\n",
    "        # Inner loop\n",
    "        WT, AT, BT, ALPHAT, _ = PGSPD(N, t, loss, tr_list, Xtr, Ytr, L, R1, R2, gamma, c, WT, AT, BT, ALPHAT)\n",
    "        wasted += _\n",
    "        if t % stamp == 0:\n",
    "            elapsed_time.append(time.time() - start_time - sum_time - wasted)\n",
    "            roc_auc.append(roc_auc_score(Yte, np.dot(Xte, WT)))\n",
    "            print('gamma: %.2f c: %.2f iteration: %d AUC: %.6f time eplapsed: %.2f/%.2f' % (gamma, c, t, roc_auc[-1], elapsed_time[-1],wasted))\n",
    "\n",
    "            sum_time = 0.0\n",
    "            # start_time = time.time()\n",
    "\n",
    "    return elapsed_time, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(name):\n",
    "    '''\n",
    "    Define loss function\n",
    "\n",
    "    input:\n",
    "        name - name of loss function\n",
    "\n",
    "    output:\n",
    "        loss - loss function\n",
    "    '''\n",
    "\n",
    "    if name == 'hinge':\n",
    "        loss = lambda x: max(0, 1 - x)\n",
    "    elif name == 'logistic':\n",
    "        loss = lambda x: log(1 + exp(-x))\n",
    "    else:\n",
    "        print('Wrong loss function!')\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservior(Bt,xt,N,M):\n",
    "    '''\n",
    "    Reservior Sampling\n",
    "\n",
    "    input:\n",
    "        Bt - current buffer\n",
    "        xt - a training instance\n",
    "        N - the buffer size\n",
    "        M - the number of instances received till trial t\n",
    "\n",
    "    output:\n",
    "        Bt - updated buffer\n",
    "    '''\n",
    "\n",
    "    L = len(Bt)\n",
    "    if L < N:\n",
    "        Bt.append(xt)\n",
    "    else:\n",
    "        z = np.random.binomial(1, p=N/M)\n",
    "        if z == 1:\n",
    "            ind = np.random.randint(L)\n",
    "            Bt[ind] = xt\n",
    "\n",
    "    return Bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq(loss,wt,xt,yt,B,ct):\n",
    "    '''\n",
    "    Sequential update\n",
    "\n",
    "    input:\n",
    "        grad - gradient of loss function\n",
    "        wt - the current classifier\n",
    "        xt -\n",
    "        yt -\n",
    "        B - the buffer to be compared to\n",
    "        ct - a parameter that weights the comparison\n",
    "\n",
    "    output:\n",
    "        wt - th updated classifier\n",
    "    '''\n",
    "    L = len(B)\n",
    "    for i in range(L):\n",
    "        prod = np.inner(wt,xt - B[i])\n",
    "        norm = np.inner(xt - B[i],xt - B[i])\n",
    "        tau = min(ct/2,loss(prod*yt)/norm)\n",
    "        wt += tau*yt*(xt - B[i])\n",
    "\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gra(wt,xt,yt,B,ct):\n",
    "    '''\n",
    "    gradient updating\n",
    "\n",
    "    input:\n",
    "        wt - the current classifier\n",
    "        xt -\n",
    "        yt -\n",
    "        B - the bufferto be compared to\n",
    "        ct - a parameter that weights the comparison\n",
    "\n",
    "    output:\n",
    "        wt - th updated classifier\n",
    "    '''\n",
    "    L = len(B)\n",
    "    w = wt + 0.0\n",
    "    for i in range(L):\n",
    "        prod = np.inner(w,xt - B[i])\n",
    "        if yt*prod <= 1:\n",
    "            wt += ct*yt*(xt - B[i])/2\n",
    "\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OAM(T,name,option,c,Np,Nn,Xtr,Ytr,Xte,Yte,stamp = 10):\n",
    "    '''\n",
    "    Online AUC Maximization\n",
    "\n",
    "    input:\n",
    "        T -\n",
    "        name -\n",
    "        option - update option\n",
    "        c - penalty parameter\n",
    "        Np - maximum buffer size of positive samples\n",
    "        Nn - maximum buffer size of negative samples\n",
    "        Xtr -\n",
    "        Ytr -\n",
    "        Xte -\n",
    "        Yte -\n",
    "        stamp - record stamp\n",
    "\n",
    "    output:\n",
    "        elapsed_time -\n",
    "        roc_auc - auc scores\n",
    "    '''\n",
    "\n",
    "    print('OAM......')\n",
    "\n",
    "    # get the dimension of what we are working with\n",
    "    n, d = Xtr.shape\n",
    "\n",
    "    # define loss function\n",
    "    loss = loss_func(name)\n",
    "\n",
    "    # initialize\n",
    "    wt = np.zeros(d)\n",
    "    Bpt = []\n",
    "    Bnt = []\n",
    "    Npt = 0\n",
    "    Nnt = 0\n",
    "    # record auc\n",
    "    roc_auc = []\n",
    "\n",
    "    # record time elapsed\n",
    "    elapsed_time = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for t in range(1,T+1):\n",
    "        if Ytr[t%n] == 1:\n",
    "            Npt += 1\n",
    "            ct = c*max(1,Nnt/Nn)\n",
    "            Bpt = reservior(Bpt,Xtr[t%n],Np,Npt)\n",
    "            if option == 'sequential':\n",
    "                wt = seq(loss,wt,Xtr[t%n],Ytr[t%n],Bnt,ct)\n",
    "            elif option == 'gradient':\n",
    "                wt = gra(wt, Xtr[t % n], Ytr[t % n], Bnt, ct)\n",
    "            else:\n",
    "                print('Wrong update option!')\n",
    "                return\n",
    "        else:\n",
    "            Nnt += 1\n",
    "            ct = c*max(1,Npt/Np)\n",
    "            Bnt = reservior(Bnt,Xtr[t%n],Nn,Nnt)\n",
    "            if option == 'sequential':\n",
    "                wt = seq(loss,wt,Xtr[t%n],Ytr[t%n],Bpt,ct)\n",
    "            elif option == 'gradient':\n",
    "                wt = gra(wt, Xtr[t % n], Ytr[t % n], Bpt, ct)\n",
    "            else:\n",
    "                print('Wrong update option!')\n",
    "                return\n",
    "\n",
    "        if t % stamp == 0:\n",
    "            elapsed_time.append(time.time() - start_time)\n",
    "            roc_auc.append(roc_auc_score(Yte, np.dot(Xte, wt)))\n",
    "            print('c: %.2f iteration: %d AUC: %.6f time eplapsed: %.2f' % (c, t, roc_auc[-1], elapsed_time[-1]))\n",
    "\n",
    "            # start_time = time.time()\n",
    "\n",
    "    return elapsed_time, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from hdf5 file\n",
    "dataset = 'mnist'\n",
    "hf = h5py.File('/Users/yangzhenhuan/PycharmProjects/AUC/datasets/%s.h5' % (dataset), 'r')\n",
    "FEATURES = hf['FEATURES'][:]\n",
    "LABELS = hf['LABELS'][:]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyper parameters\n",
    "N = 3\n",
    "T = 500\n",
    "folders = 2\n",
    "stamp = 10\n",
    "iteration = list(range(T//stamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "L = [1]\n",
    "C = [10]\n",
    "Np = [100]\n",
    "Nn = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define losses and algorithms\n",
    "NAME = ['logistic']\n",
    "ALG = ['SAUC','OAM']\n",
    "OPTION = ['sequential','gradient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and testing\n",
    "n = len(LABELS)\n",
    "training = [i for i in range(n // 2)]\n",
    "testing = [i for i in range(n // 2, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAUC......\n",
      "gamma: 48.61 c: 10.00 iteration: 10 AUC: 0.511837 time eplapsed: 0.01/0.00\n",
      "gamma: 48.61 c: 10.00 iteration: 20 AUC: 0.550063 time eplapsed: 0.05/0.01\n",
      "gamma: 48.61 c: 10.00 iteration: 30 AUC: 0.595815 time eplapsed: 0.09/0.03\n",
      "gamma: 48.61 c: 10.00 iteration: 40 AUC: 0.638711 time eplapsed: 0.15/0.06\n",
      "gamma: 48.61 c: 10.00 iteration: 50 AUC: 0.671776 time eplapsed: 0.23/0.10\n",
      "gamma: 48.61 c: 10.00 iteration: 60 AUC: 0.703791 time eplapsed: 0.36/0.16\n",
      "gamma: 48.61 c: 10.00 iteration: 70 AUC: 0.733332 time eplapsed: 0.46/0.20\n",
      "gamma: 48.61 c: 10.00 iteration: 80 AUC: 0.753681 time eplapsed: 0.68/0.30\n",
      "gamma: 48.61 c: 10.00 iteration: 90 AUC: 0.777661 time eplapsed: 0.86/0.38\n",
      "gamma: 48.61 c: 10.00 iteration: 100 AUC: 0.793801 time eplapsed: 1.05/0.46\n",
      "gamma: 48.61 c: 10.00 iteration: 110 AUC: 0.808781 time eplapsed: 1.18/0.53\n",
      "gamma: 48.61 c: 10.00 iteration: 120 AUC: 0.824192 time eplapsed: 1.34/0.62\n",
      "gamma: 48.61 c: 10.00 iteration: 130 AUC: 0.836282 time eplapsed: 1.52/0.71\n",
      "gamma: 48.61 c: 10.00 iteration: 140 AUC: 0.846780 time eplapsed: 1.70/0.81\n",
      "gamma: 48.61 c: 10.00 iteration: 150 AUC: 0.854508 time eplapsed: 1.87/0.91\n",
      "gamma: 48.61 c: 10.00 iteration: 160 AUC: 0.863728 time eplapsed: 2.07/1.02\n",
      "gamma: 48.61 c: 10.00 iteration: 170 AUC: 0.871215 time eplapsed: 2.27/1.13\n",
      "gamma: 48.61 c: 10.00 iteration: 180 AUC: 0.877026 time eplapsed: 2.47/1.25\n",
      "gamma: 48.61 c: 10.00 iteration: 190 AUC: 0.882134 time eplapsed: 2.75/1.41\n",
      "gamma: 48.61 c: 10.00 iteration: 200 AUC: 0.886889 time eplapsed: 2.98/1.54\n",
      "gamma: 48.61 c: 10.00 iteration: 210 AUC: 0.891221 time eplapsed: 3.22/1.69\n",
      "gamma: 48.61 c: 10.00 iteration: 220 AUC: 0.894321 time eplapsed: 3.47/1.84\n",
      "gamma: 48.61 c: 10.00 iteration: 230 AUC: 0.897940 time eplapsed: 3.72/1.99\n",
      "gamma: 48.61 c: 10.00 iteration: 240 AUC: 0.901368 time eplapsed: 3.99/2.16\n",
      "gamma: 48.61 c: 10.00 iteration: 250 AUC: 0.904023 time eplapsed: 4.26/2.32\n",
      "gamma: 48.61 c: 10.00 iteration: 260 AUC: 0.906462 time eplapsed: 4.59/2.52\n",
      "gamma: 48.61 c: 10.00 iteration: 270 AUC: 0.908236 time eplapsed: 4.89/2.71\n",
      "gamma: 48.61 c: 10.00 iteration: 280 AUC: 0.910333 time eplapsed: 5.27/2.94\n",
      "gamma: 48.61 c: 10.00 iteration: 290 AUC: 0.911893 time eplapsed: 5.62/3.15\n",
      "gamma: 48.61 c: 10.00 iteration: 300 AUC: 0.913352 time eplapsed: 6.00/3.37\n",
      "gamma: 48.61 c: 10.00 iteration: 310 AUC: 0.914547 time eplapsed: 6.33/3.58\n",
      "gamma: 48.61 c: 10.00 iteration: 320 AUC: 0.915727 time eplapsed: 6.68/3.79\n",
      "gamma: 48.61 c: 10.00 iteration: 330 AUC: 0.916758 time eplapsed: 7.03/4.01\n",
      "gamma: 48.61 c: 10.00 iteration: 340 AUC: 0.917902 time eplapsed: 7.40/4.24\n",
      "gamma: 48.61 c: 10.00 iteration: 350 AUC: 0.918861 time eplapsed: 7.78/4.48\n",
      "gamma: 48.61 c: 10.00 iteration: 360 AUC: 0.919692 time eplapsed: 8.16/4.72\n",
      "gamma: 48.61 c: 10.00 iteration: 370 AUC: 0.920361 time eplapsed: 8.56/4.96\n",
      "gamma: 48.61 c: 10.00 iteration: 380 AUC: 0.921062 time eplapsed: 8.97/5.22\n",
      "gamma: 48.61 c: 10.00 iteration: 390 AUC: 0.921596 time eplapsed: 9.44/5.51\n",
      "gamma: 48.61 c: 10.00 iteration: 400 AUC: 0.922188 time eplapsed: 9.93/5.81\n",
      "gamma: 48.61 c: 10.00 iteration: 410 AUC: 0.922603 time eplapsed: 10.38/6.09\n",
      "gamma: 48.61 c: 10.00 iteration: 420 AUC: 0.923239 time eplapsed: 10.83/6.38\n",
      "gamma: 48.61 c: 10.00 iteration: 430 AUC: 0.923659 time eplapsed: 11.35/6.69\n",
      "gamma: 48.61 c: 10.00 iteration: 440 AUC: 0.923981 time eplapsed: 11.82/6.99\n",
      "gamma: 48.61 c: 10.00 iteration: 450 AUC: 0.924395 time eplapsed: 12.30/7.29\n",
      "gamma: 48.61 c: 10.00 iteration: 460 AUC: 0.924671 time eplapsed: 12.79/7.60\n",
      "gamma: 48.61 c: 10.00 iteration: 470 AUC: 0.925020 time eplapsed: 13.29/7.92\n",
      "gamma: 48.61 c: 10.00 iteration: 480 AUC: 0.925252 time eplapsed: 13.80/8.24\n",
      "gamma: 48.61 c: 10.00 iteration: 490 AUC: 0.925555 time eplapsed: 14.35/8.59\n",
      "gamma: 48.61 c: 10.00 iteration: 500 AUC: 0.925799 time eplapsed: 14.93/8.94\n",
      "OAM......\n",
      "c: 10.00 iteration: 10 AUC: 0.500000 time eplapsed: 0.00\n",
      "c: 10.00 iteration: 20 AUC: 0.758041 time eplapsed: 0.02\n",
      "c: 10.00 iteration: 30 AUC: 0.748818 time eplapsed: 0.04\n",
      "c: 10.00 iteration: 40 AUC: 0.725695 time eplapsed: 0.06\n",
      "c: 10.00 iteration: 50 AUC: 0.859686 time eplapsed: 0.08\n",
      "c: 10.00 iteration: 60 AUC: 0.857818 time eplapsed: 0.10\n",
      "c: 10.00 iteration: 70 AUC: 0.855178 time eplapsed: 0.12\n",
      "c: 10.00 iteration: 80 AUC: 0.853155 time eplapsed: 0.14\n",
      "c: 10.00 iteration: 90 AUC: 0.855024 time eplapsed: 0.16\n",
      "c: 10.00 iteration: 100 AUC: 0.857386 time eplapsed: 0.19\n",
      "c: 10.00 iteration: 110 AUC: 0.857133 time eplapsed: 0.21\n",
      "c: 10.00 iteration: 120 AUC: 0.855835 time eplapsed: 0.23\n",
      "c: 10.00 iteration: 130 AUC: 0.855633 time eplapsed: 0.27\n",
      "c: 10.00 iteration: 140 AUC: 0.852925 time eplapsed: 0.30\n",
      "c: 10.00 iteration: 150 AUC: 0.894701 time eplapsed: 0.32\n",
      "c: 10.00 iteration: 160 AUC: 0.895175 time eplapsed: 0.34\n",
      "c: 10.00 iteration: 170 AUC: 0.896406 time eplapsed: 0.37\n",
      "c: 10.00 iteration: 180 AUC: 0.896011 time eplapsed: 0.39\n",
      "c: 10.00 iteration: 190 AUC: 0.903354 time eplapsed: 0.41\n",
      "c: 10.00 iteration: 200 AUC: 0.904521 time eplapsed: 0.44\n",
      "c: 10.00 iteration: 210 AUC: 0.903148 time eplapsed: 0.48\n",
      "c: 10.00 iteration: 220 AUC: 0.902752 time eplapsed: 0.51\n",
      "c: 10.00 iteration: 230 AUC: 0.902116 time eplapsed: 0.53\n",
      "c: 10.00 iteration: 240 AUC: 0.902929 time eplapsed: 0.55\n",
      "c: 10.00 iteration: 250 AUC: 0.904043 time eplapsed: 0.58\n",
      "c: 10.00 iteration: 260 AUC: 0.906113 time eplapsed: 0.60\n",
      "c: 10.00 iteration: 270 AUC: 0.904161 time eplapsed: 0.62\n",
      "c: 10.00 iteration: 280 AUC: 0.904968 time eplapsed: 0.65\n",
      "c: 10.00 iteration: 290 AUC: 0.904376 time eplapsed: 0.68\n",
      "c: 10.00 iteration: 300 AUC: 0.901221 time eplapsed: 0.72\n",
      "c: 10.00 iteration: 310 AUC: 0.901357 time eplapsed: 0.74\n",
      "c: 10.00 iteration: 320 AUC: 0.905189 time eplapsed: 0.77\n",
      "c: 10.00 iteration: 330 AUC: 0.905140 time eplapsed: 0.79\n",
      "c: 10.00 iteration: 340 AUC: 0.904716 time eplapsed: 0.81\n",
      "c: 10.00 iteration: 350 AUC: 0.905303 time eplapsed: 0.84\n",
      "c: 10.00 iteration: 360 AUC: 0.920782 time eplapsed: 0.87\n",
      "c: 10.00 iteration: 370 AUC: 0.921729 time eplapsed: 0.90\n",
      "c: 10.00 iteration: 380 AUC: 0.919798 time eplapsed: 0.93\n",
      "c: 10.00 iteration: 390 AUC: 0.927660 time eplapsed: 0.96\n",
      "c: 10.00 iteration: 400 AUC: 0.927511 time eplapsed: 0.98\n",
      "c: 10.00 iteration: 410 AUC: 0.929609 time eplapsed: 1.01\n",
      "c: 10.00 iteration: 420 AUC: 0.930610 time eplapsed: 1.03\n",
      "c: 10.00 iteration: 430 AUC: 0.930399 time eplapsed: 1.06\n",
      "c: 10.00 iteration: 440 AUC: 0.929891 time eplapsed: 1.09\n",
      "c: 10.00 iteration: 450 AUC: 0.933527 time eplapsed: 1.13\n",
      "c: 10.00 iteration: 460 AUC: 0.933009 time eplapsed: 1.16\n",
      "c: 10.00 iteration: 470 AUC: 0.931875 time eplapsed: 1.19\n",
      "c: 10.00 iteration: 480 AUC: 0.928481 time eplapsed: 1.21\n",
      "c: 10.00 iteration: 490 AUC: 0.927343 time eplapsed: 1.24\n",
      "c: 10.00 iteration: 500 AUC: 0.934381 time eplapsed: 1.26\n"
     ]
    }
   ],
   "source": [
    "# Prepare results\n",
    "sauc_time,sauc_auc = SAUC(T,NAME[0],N,L[0],C[0],FEATURES[training],LABELS[training],FEATURES[testing],LABELS[testing],stamp = stamp)\n",
    "oam_time,oam_auc = OAM(T,NAME[0],OPTION[0],C[0],Np[0],Nn[0],FEATURES[training],LABELS[training],FEATURES[testing],LABELS[testing],stamp = stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xl8FfW5+PHPc5bkZGcJa8ImBJUdQdC6K1pX8NpWoLVWbLW3t1q1Xltt+7Ot3byva2vt79paSi1aLaBW/alXxX2pogIqICAS2RIgBAIkZD3b8/tjJockJAHCOTlJzvN+Ma9ZzpyZZ0Iyz8z3O/P9iqpijDHGAHiSHYAxxpiuw5KCMcaYGEsKxhhjYiwpGGOMibGkYIwxJsaSgjHGmBhLCsYYY2IsKRhjjImxpGCMMSbGl+wAjlZ+fr4OHz482WEYY0y3snLlyj2q2u9w63W7pDB8+HBWrFiR7DCMMaZbEZGtR7KeFR8ZY4yJsaQQb5FQsiMwxpgO63bFR12KKtTshn1b4f0HoGAK7PgQ6vbBVf9MdnTGGHPULCl01Nqn4M3/hvK1B5d98oQzPukbyYnJGJMUqooqqDsdVVDcZe50VA9+RotlzvecZe4/oi22qQq9s9LITk/saduSwtEKB2Hbu/D4Nc78eXeCeGDiV2HZ/4X351tSMHHVeCIJRaJEoko4ooSjznQoqkTc+agqkShEoupOKxFVotGm0xBRJRKNHrJu7DvudFSdE1M06kw3Lldt3IbzWdj9Ttj9PBxxt+/Go7HtNx7LwW1r4z60+Umz6Wexk2qUZvtv+p22tk/jNmiyrZbbp+39NsbW9OTe7ESNs7yz/PLycVx1yrCE7sOSwtF6Yh58+pwzfe1SGHrKwc8u+KUzmG4nElUawhEaQlGCkag7jlDfZL4hHKEhHHWG0MHp+lCEYNhZL+SOg03GoUiUUEQJufPhqDZbFopEm53owy1P/JGu3xGWR8Dn8eD1CD6P4HHHIoLXAx4RZ2gyLdI4DYLzHY8QWy6xzw5+BwGvR/B75OA2G7fjOTgtgvt9cb/vzEvjvlrZT9OYmsXmbuPg94ltR5p93nzbbe0PDn6neVzORhqPt+W+EDhpaO+E/19aUjgaqrD5beg1FKZeC4XTkh1RjxeJKrXBMHXBCDXBCLXBMLXBCHXBCHWhCPWhg9N1oQj1TadDzgn74DhCfbj58gZ3WTxOvH6vkOb14Pd5nLHXQ7rPGaf5PPi8gt/rIcPvJSfgc5Z7neU+jwefR9xpwevx4PdK7CTr8zonXGdZ83UbT8Yej+BtchJuvkxiy7we8Ho8eN2TtNddp9m6jcvck5bXc+i0RxpjdU6spmewpNCeaBQ8TR7Q2rcFGirh/J/D1HlJC6u7CIaj7K8NUlkXoqo+zIH6EAfqw1S54wP1Iarrw+6yMNUNjcvDVDeEqWkI0xCOHvV+A37nxJvh9xKIDR4y0rzkZfgJ+L2k+zyku8sDfi8Bn5d0v3MST/d5SfM1Tjsn9DSf5+D3fI3fPzid5vXg8diJ0XR/lhQaqcJ7f4SyNU6R0IFd8K/fQa9hcNzZ0FAFax531h08KZmRJtWB+hC7quopr2qg/EAD5Qfq2VXVwJ7qBvbVhthfG2RvTZD9tSGqG8LtbsvrEXICPrLTfeQE/OQEfAzMDVDU30d2wEdWuo9Mv4/MNC8ZaV6y0r1kuPOZac7JPiPNG0sAGWnOCdquWo3pOEsKjbZ/CEt/5EyvWnRweVoWfPBnZ7rvKBg1AwZO6Pz4OkkkquzYX8fWilq2VNRQsq+Wkr21lOytY9veWirrDn0PI8PvpV9OOr2z0uiTlcbIftn0zkyjd6afXllp5GX4yQ04J/7YOMNHht9rJ3BjuhhLCgBv/Be88Wtn+geboXYvREPQ/0Rn2dZ3YX8JjP8yeLzJizNOaoNhSvbWOSf7fY0n/Bo276mhZG8dwcjBIps0r4fC3hkM6ZPJxCF5DOmdycC8AP1zAvTPTad/TjrZ6T47uRvTQ1hSAHj3/4LHB1fMh8w+ztDUsC9AYp8CS4hgOEpxeTWfllWxoewAn5Yd4NOyKnZVNTRbL8PvZWifTEb1z2bGmAEM75vlDPmZDMgJWFm5MSnEkkI0CsEDcNYPYdyXkh1Nh0WjypaKGlaV7mdVSSUflexn/Y6q2FV/mtfDqP7ZnDYyn5H9sxnSJ5Mh7h1A36w0u9I3xgCWFCBY7YzTc5Mbx1GoD0XYUHaAdTurWLejivU7naEmGAEgM83L+II85p02nLEFeZw4MIfh+Vn4vdbUlTGmfZYUGqqccXpOcuNoh6ry+e5qXv90N69vKGf5lr2x5+pz0n2cOCiXr0wdwphBuUwc0otR/bPxWpGPMaYDLCk0HHDGXTAplO6r5bEVpTz1USkle+sAOH5ADteeNoLJQ3szdnAuhb0zrOjHGBM3lhQak0KgaxQfRaLKy+vK+McHJby9cTcAZxT149/PGsnZx/enoFdGkiM0xvRkqZcUti6DQB4MGOPM1zcWHyU3KdSHIjz54Xb+/NbnbK2oZVBegBvPLeLKqYUU9s5MamzGmNSReklh4cWgUecFtKodUL/fWZ6k4qOq+hCL3t/Ggn9tZveBBiYW5nHHVSdx/piBVi9gjOl0qZcUPH6INEDZahg0CWr3OMszEt/6YFMle2v52ztbWLJ8GzXBCKePyue+2ZM4dWRfqyMwxiRN6iWF7P7Qezic82MYdirUVMCWtyBnYKfsvqyynt+8sJ5nV+3AI8JlEwfzzdNHMK4gr1P2b4wx7Um9pBANQ58RTkIAyOoLY/8t4bsNR6I8vGwrv3v5M4KRKNedeRzzvjCCgXmBhO/bGGOOVGomBenc9os+2V7JD/+5mrU7qjhzdD9+MWssw/pmdWoMxhhzJFIzKXg657AbwhH+8OpGHnhzE32y0rj/qydx8fiBVmdgjOmyUjApRDslKXxcsp/bHl/FxvJqvjylkP9zyRjyMv0J368xxhyLFEwK4YQ3f/3Mqh3csuRj+uek87d5J3PO8f0Tuj9jjImXFE0KiTvsZ1bt4ObFHzF1eB8WfGMquQG7OzDGdB8JbTZTRC4UkQ0iUiwit7fy+VAReV1EPhKR1SJycSLjARKaFJomhIXzTraEYIzpdhKWFETEC9wPXASMAeaKyJgWq/0EeExVJwNzgD8mKh7A6YdZIwkpPnrxk53NEkJmWurdhBljur9E3ilMA4pVdZOqBoHFwKwW6yjQ2OhQHrAjgfE4zVtA3O8U3ttUwfcWfcykIb0sIRhjurVEnr0KgJIm86XA9Bbr/Ax4SURuBLKAGQmMxyk6grjeKXxaVsV1D69gaN9MHrzGEoIxpntLdldcc4GFqloIXAz8XUQOiUlErheRFSKyYvfu3R3fWywpxOfE/dmuA3zjwQ/ITPPy0LXT6JWZFpftGmNMsiQyKWwHhjSZL3SXNfVN4DEAVV0GBID8lhtS1fmqOlVVp/br16/jEcUxKbxTvIcv/eldogoPXzvd+jkwxvQIiUwKy4EiERkhImk4FcnPtFhnG3AegIiciJMUjuFW4DCiTh/Gx9rMxeMrSvjGgx8wKC/A0989jeMHdr1e24wxpiMSVgCuqmERuQFYCniBB1V1rYjcBaxQ1WeAW4G/iMgtOJXO16iqJiqmeNQp3PvyZ9z36kbOKMrn/q+dZI+dGmN6lITWiqrq88DzLZbd2WR6HXBaImNopvFOoYPFR8+u2sF9r27kK1MK+fUV4/F7k10lY4wx8ZVaj8ocQ51C6b5afvTUGk4a2ovfXDEenyUEY0wPlFpntg4mhUhU+f6SVajCfXMmW0IwxvRYqXWnUPyKMz7KpPDH14v5YMte7p09kSF9MhMQmDHGdA2pc8nbUA3P/6cz7Tnyw16xZS+/f3UjMycO5vJJBQkKzhhjuoY2z44i8kUR+XIry78sIucnNqwEiIYOTh/hncKyzyu45m/LKeiVwS//bZx1jmOM6fHau2S+E3izleVvAHclJJpEavqk6xEkhZfWlvGNvznvIjz27VPt0VNjTEpo7+yYrqqHvEimqntEpPt1MNzYGB4cNik8sbKUH/5zNeMK8lh4zcn0zrLmK4wxqaG9s2OuiPhUNdx0oYj4ge7XpkPTpBAJtbnaX/+1mV88t47TR+Xz569PISs9terijTGprb3ioydx3jaO3RWISDbwgPtZ99K0+Kh+f6urbNpdzS+eW8cXxw7gr9dMtYRgjEk57SWFnwC7gK0islJEPgQ247RN9JPOCC6umt4p1Fe2usr/rt4JwM9njiPdl9h+nI0xpitq81LYLTa6XUR+DoxyFxeral2nRBZvTZPC5KtaXeV/1+xk6rDeDMwLdFJQxhjTtbSZFETkihaLFOglIh+r6oHEhpUAjUlh5v9AIO+QjzftrubTsgPceWnLHkONMSZ1tFdoflkry/oAE0Tkm6r6WoJiSozGpHBoHz4AvPBJGQAXjR/YWREZY0yX017x0bzWlovIMJyOcVp2rdm1HSYp/O/qnUwZ1ptBed3vwSpjjImXo27mQlW3At3vTa52kkJxeTXrdlZx0Ti7SzDGpLajTgoicgLQkIBYEqvxkdRWksJjK0rweYRZ1raRMSbFtVfR/CxO5XJTfYBBQOuP73RlsTuF5u0XBcNRnvywlPNO7E+/nPQkBGaMMV1HexXN97SYV2AvTmK4CliWqKASoo3io9c+3cWe6iBzTh6ahKCMMaZraa+iOdYYnohMBr4KfAXnBbZ/Jj60OGsjKSxeXsLA3ABnju6XhKCMMaZraa/4aDQw1x32AEsAUdVzOim2+GolKezYX8dbn+3mu+eMwuuxZrGNMaa94qNPgbeBS1W1GEBEbumUqBKhlTqFJ1aWElX4ypQhSQrKGGO6lvaeProC2Am8LiJ/EZHzgG58OX3o00dL15Zx8vDeDO1rXWwaYwy0kxRU9WlVnQOcALwO3Az0F5E/icgFnRVg3LQoPtpXE2TdzirOKLK6BGOMaXTYtqFVtQb4B/APEemNU9n8Q+ClBMcWXy2SwrJNFajCaaP6JjEoY0yihEIhSktLqa+vT3YonSoQCFBYWIjf37F3jI+qwwBV3QfMd4fupcXLa+9+voesNC8TCnslMShjTKKUlpaSk5PD8OHDU6Z/dVWloqKC0tJSRowY0aFtHPUbzd1Wi4rmd4srmDaiD35v6vwIjEkl9fX19O3bN2USAoCI0Ldv32O6O0qdM2KT4qOdlXVs2lPDaaPykxuTMSahUikhNDrWY07JpPBucQUAXxhpScEYY5pKyaSwYdcB0nweThiYk9yYjDE92q9+9SvGjh3LhAkTmDRpEu+//z4Ae/bswe/388ADDzRbPzs7u9n8woULueGGG2LzDz/8MOPGjWP8+PFMnjyZe+5p2RrRsUvJpFC6r5bC3hl47C1mY0yCLFu2jOeee44PP/yQ1atX88orrzBkiPOi7OOPP84pp5zCokWLjnh7L7zwAr///e956aWXWLNmDe+99x55eYf2InmsUjIplOytY0hve2HNGJM4O3fuJD8/n/R0p/Xl/Px8Bg8eDMCiRYv47W9/y/bt2yktLT2i7f3mN7/hnnvuiW0jPT2d6667Lu5xH9Ujqd1a06Swr5oJhfHPsMaYrunnz65l3Y6quG5zzOBcfnrZ2DY/v+CCC7jrrrsYPXo0M2bMYPbs2Zx11lmUlJSwc+dOpk2bxpVXXsmSJUu49dZbD7u/Tz75hClTpsTzEFqV0DsFEblQRDaISLGI3N7GOleKyDoRWSsi/0hYMG5SqAlF2V8bYkgfu1MwxiROdnY2K1euZP78+fTr14/Zs2ezcOFClixZwpVXXgnAnDlzDluE1NlPUCXsTkFEvMD9wPlAKbBcRJ5R1XVN1ikC7gBOU9V9ItI/UfE0vrxWfiAIQGFv64vZmFTR3hV9Inm9Xs4++2zOPvtsxo8fz0MPPcSOHTsoKyvj0UcfBWDHjh1s3LiRoqIiMjIyCAaDpKWlAbB3717y852nJMeOHcvKlSs599xzExpzIu8UpgHFqrpJVYPAYmBWi3WuA+5335RGVcsTFo2bFHa5ScHqFIwxibRhwwY2btwYm//444+JRCJUV1ezfft2tmzZwpYtW7jjjjtidwtnnXUWjzzyCAB1dXU89thjnHOO01vBHXfcwW233UZZWRkAwWCQBQsWxD3uRNYpFAAlTeZLgekt1hkNICLvAF7gZ6r6YkKicYuPnKQgdqdgjEmo6upqbrzxRvbv34/P52PUqFFMnz6dCRMmNFvvS1/6ErNnz+bOO+/kvvvu49vf/jZ/+MMfUFWuvvpqzjzzTAAuvvhidu3axYwZM1BVRIRrr7027nEnu6LZBxQBZwOFwFsiMl5V9zddSUSuB64HGDq0g91mukmh7ECQzLRM+mSldThoY4w5nClTpvDuu+8edr0JEyawfv16AAoKCnjuuefaXHfevHnMmzcvbjG2JpHFR9uBpr3XFLrLmioFnlHVkKpuBj7DSRLNqOp8VZ2qqlP79etgU9duUthTHWJgXiAlX383xpjDSWRSWA4UicgIEUkD5gDPtFjnaZy7BEQkH6c4aVNConGTQkVNmAE5gYTswhhjuruEJQVVDQM3AEuB9cBjqrpWRO4SkZnuakuBChFZh9ORz22qWpGYgNw7hdog/XPTE7ILY4zp7hJap6CqzwPPt1h2Z5NpBb7vDokVKz4Kc3yOJQVjjGlNyjVz0RBR+lvxkTHGtCqFkoLznkIUjxUfGWNMG1IoKTh3ClHE7hSMMZ2itLSUWbNmUVRUxMiRI7npppsIBoOxz2+++WYKCgqIRqOxZQsXLkREeOWVV2LLnn76aUSEJ554IuExp2ZSsDsFY0yCqSpXXHEFl19+ORs3buSzzz6jurqaH//4xwBEo1GeeuophgwZwptvvtnsu+PHj2fx4sWx+UWLFjFx4sROiTvlkoIi9LeKZmNMgr322msEAoHYy2Zer5d7772XBx98kNraWt544w3Gjh3Ld77znUMaxTvjjDP44IMPCIVCVFdXU1xczKRJkzol7mS/0dx53KSQ7vORnZ46h22MAV64HcrWxHebA8fDRXe3+fHatWsPaeo6NzeXoUOHUlxczKJFi5g7dy6zZs3iRz/6EaFQCL/fDzgto86YMYOlS5dSWVnJzJkz2bx5c3zjb0Pq3CngVDT3zk63t5mNMUn3/PPPc/nll5Obm8v06dNZunRps8/nzJnD4sWLWbx4MXPnzu20uFLnktm9U+iTZZXMxqScdq7oE2XMmDGHVAxXVVWxbds2Nm/ezP79+xk/fjwAtbW1ZGRkcOmll8bWnTZtGmvWrCEzM5PRo0d3Wtypc6fQmBTsySNjTCc477zzqK2t5eGHHwYgEolw6623cs0117BkyRIWLFgQaz578+bNvPzyy9TW1jbbxt13382vf/3rTo079ZKC3SkYYzqBiPDUU0/x+OOPU1RUxOjRowkEAtx55528+OKLXHLJJbF1s7KyOP3003n22WebbeOiiy6K9afQWVKm+CgUjuAH+uZYPwrGmM4xZMiQQ0704PSo1tKTTz4Zm77mmmsO+XzhwoXxDK1NKXOnUF3vvDDSN9vuFIwxpi2pkxTq3KSQa0nBGGPakjJJoS4YAuxOwZhUom6bZ6nkWI85ZZLC6P5ZABw/MC/JkRhjOkMgEKCioiKlEoOqUlFRQSDQ8YvflKlobnz6yOv1JjkQY0xnKCwspLS0lN27dyc7lE4VCAQoLCzs8PdTJykMOx3O/wV405IdiTGmE/j9fkaMGJHsMLqd1EkKhVOcwRhjTJtSpk7BGGPM4VlSMMYYEyPdrWZeRHYDWzv49XxgTxzDSQSLMT66eoxdPT6wGOOlq8Q4TFX7HW6lbpcUjoWIrFDVqcmOoz0WY3x09Ri7enxgMcZLd4ixKSs+MsYYE2NJwRhjTEyqJYX5yQ7gCFiM8dHVY+zq8YHFGC/dIcaYlKpTMMYY075Uu1MwxhjTDksKxhhjYlImKYjIhSKyQUSKReT2ZMfTkogMEZHXRWSdiKwVkZuSHVNrRMQrIh+JyHPJjqU1ItJLRJ4QkU9FZL2InJrsmFoSkVvc/+NPRGSRiCS9PXcReVBEykXkkybL+ojIyyKy0R337oIx/rf7f71aRJ4SkV5dKb4mn90qIioi+cmI7WikRFIQES9wP3ARMAaYKyJjkhvVIcLArao6BjgF+G4XjBHgJmB9soNox33Ai6p6AjCRLhariBQA3wOmquo4wAvMSW5UACwELmyx7HbgVVUtAl5155NpIYfG+DIwTlUnAJ8Bd3R2UE0s5ND4EJEhwAXAts4OqCNSIikA04BiVd2kqkFgMTAryTE1o6o7VfVDd/oAzsmsILlRNScihcAlwIJkx9IaEckDzgT+CqCqQVXdn9yoWuUDMkTEB2QCO5IcD6r6FtCy4+BZwEPu9EPA5Z0aVAutxaiqL6lq2J19D+h4m9HHqI2fIcC9wA+AbvFUT6okhQKgpMl8KV3shNuUiAwHJgPvJzeSQ/we55c7muxA2jAC2A38zS3iWiAiWckOqilV3Q7cg3PVuBOoVNWXkhtVmwao6k53ugwYkMxgjsC1wAvJDqIpEZkFbFfVVcmO5UilSlLoNkQkG/gncLOqViU7nkYicilQrqorkx1LO3zAScCfVHUyUEPyizyaccvlZ+EksMFAlohcldyoDk+dZ9e77JWuiPwYpwj20WTH0khEMoEfAXcmO5ajkSpJYTswpMl8obusSxERP05CeFRVn0x2PC2cBswUkS04xW/nisgjyQ3pEKVAqao23mE9gZMkupIZwGZV3a2qIeBJ4AtJjqktu0RkEIA7Lk9yPK0SkWuAS4Gvadd68WokTvJf5f7dFAIfisjApEZ1GKmSFJYDRSIyQkTScCr2nklyTM2IiOCUha9X1d8lO56WVPUOVS1U1eE4P7/XVLVLXeGqahlQIiLHu4vOA9YlMaTWbANOEZFM9//8PLpYZXgTzwDfcKe/Afy/JMbSKhG5EKdIc6aq1iY7nqZUdY2q9lfV4e7fTSlwkvt72mWlRFJwK6JuAJbi/AE+pqprkxvVIU4Dvo5zBf6xO1yc7KC6oRuBR0VkNTAJ+HWS42nGvYt5AvgQWIPzN5j0ZhBEZBGwDDheREpF5JvA3cD5IrIR5w7n7i4Y4/8AOcDL7t/MA10svm7HmrkwxhgTkxJ3CsYYY46MJQVjjDExlhSMMcbE+JIdwNHKz8/X4cOHJzsMY4zpVlauXLnnSPpo7nZJYfjw4axYsSLZYRhjTLciIluPZD0rPjLGGBPT7e4UjDEmKSJhqNkNKHj84PGC1w8e38F5kcTsu74SSpZD/xMgL7Ft/llSMMZ0T5Ew1O+Hun0Hh2ANhBsgXAehegi7g0YBAfE0GcQ5kXt8hw7RMFRth/0lUFnijKu2g0bajke84M84OPjccUYvyC2EvALILXDHhZDdH9KywJt2aDKp3A4l78G292DrMtj1CaDwxd/Aqf+RyJ+qJQVjTBcRjcKuNbDpTdj8JuxaC6ruCVMOnshVoaHKGY6QigdUkaNo00/FQzhrIMHsAoL5U2gYdhkNWQOJ4oVICI2GkWgYjYYhEkIiDUi4Hk+4Dk+4Dok04A3Xkra3gowd6wjU7251/1E8RLzphD0Bwt4AEo2QHXSamQp6MtiZM57tQ75FSc5ETuh/DhOP+Ag6xpKCMeaIqCqhiNIQjtAQjhJsHCJRGkJRwvVV5H32JN7a8tgVuoYbkHADEqkngo86bw713mzqPNnUeHOokSzSG/YyrGoFww98SFakEoBdaUPZHJhICC9olKgqqlFUFVWlWrKoysimkiaDZlEdTac66qM24qMm6udA1EddxEso2ngl7iQGjzt4ieAlio9Is0ERyulFuM4He+Lz8/MRZgD7GCQVDJa95EslAYIEpIGMcJAAQTIkiBDlk+gXWR49nvU6jEitF085eET4+bAIE0fGJ5624zTGdFuqSjASpS4YoS4UiY3rQxHqglFnWShCfePn7mf1oSgNYXccilAfdr5bH4o2WcdZvyHsnPQbwhGirVxopxPkKu/L/IfvGfrKAaIqNOA/OKifIH78hBkkteRSg1eab6iMvrwqk1nhHc/H3gns8+Xjj3rweQWfx4PfK/i87tjjLPeK4PUcHNI9QpbHQ6FX8HuddfxeDz6P811fk3WbftcTm3dOvJ4W2226fuO6Hnddr0fwCIg0WSf2/ebbEzk473Fvfppu1+NuQwRmeg6uJ4mqp2iDJQVjkiAciVJVH2Z/bZD9dSEq60JU1YWoaYhQ3RCiuiFCTUOY6vow1cEwtQ1haoLOstpghOoGZ1ldqPUT9eGkeT2k+z2k+7wE/B4Cfi8Z7pCb4WdAbjoZfm/s83S/l3Sfxx28pPk8ZEiY0Tue5ISN8wnU72bvwNNYPekWQgNPwu/z4ndP4n6vhwyvhzSvB4/XQ4NX8Udq8TVUIg1V4M9kYJ/jmCnCzPj/qM1RsqRguqdoBEK1TmViNORU6KVlO0+DdBZV6utr2b93Nwf2llNTuYf6qj0Eq/cSrq2kKuJnfzSTPZEMdoczKA9mUBZMp6o+RLC+njRCpEmYNEL4CSNAGC8hvITxkpaWTlpaGpKehaTnkZnuY1BegMw0H1npXjLTfGSmeQ+e0NO8ZKY5J/LMNGc+w+8l4POQFd5LZu12Ahp0rrIbL1UbicetZPU2r3xFnJ9vpA4iIXc6BHs3wb/udSphh34Bzl1In+Gn0+eIf3hpkNkr3v8jJg4sKZiuK1gDJR/A1ndg67vOiShUC6E6iARb/443DfyZToJIyzx4ohNvk5Ne49jvrO/1HZzWKEQaCAfraWioJ9hQRyRYTyTU4FQgRurxRRpIU2cIiDIQOOpeUwJHuF7IHRoyIWcg+AdD2iDIHuQ8veJWoIJCKApBdZ6QqdoB+7fBvq3OOFx3tBEeXsEUuOw+GHlu4h7FNJ3OkoKJj5o9zqOA0bA7RNxxyDmJh2oh6J7QQzXOFT64J+wmV6nigd0bnESw4yNnG+KBQRNh5HnOI3z+DOfE3/jon8d3cLvBFkNjLBqJxaTRMKGGeoLBSsKhBiKhENFIEMIhQgp1ES/16iOInyA+guojSCYRX18UNi1nAAAc1klEQVTwBZCMTLzpzpAWyMKb1Ye07D5k5OWT2asfeb3yyc7tgydSB3X7nccm6yvdaaciFV8aeNPBl+4kI186B6/Kg87jlo1X5cFqOFDmnOgP7HQS5YGdbSdGgEAe9BoG+UVQdL4z3WsIpOc4n7dsMj/282n8Wbn/jxp1E6bfHbsJND0bBk6wZNADWVIwx+6VnzlFCfHi8UPBSfCF78Gw02DINAjkHtFXVZWKmiBb9tSwtaKWsqp6drlDWVUD5VX1lB9oINKiIL53pp+C3hkMysugoFcGg/ICDOqVweC8ACN7ZdA/Jx2ftwMNACTqRaPGxzIbH9kUD85jm+60PyMx+zU9niUFc2w2vuIkhLH/Bsed0+QFoCbl0o1X9mmZ7hV+5sGTVrMrU3ecM8hZtx21wTCbdtfw+e5qPi+vZnNFLVv21LBlTw0HGsLN1s0N+BiYF2BAboBR/fIZkJvO4F4ZFPTOoLBXBoN7ZZCV3s3+FEScuwFj4qyb/SWYLqW6HJ7+d+g/Bi7/U0KuTqvqQ2zcdYANZdVsLD/A57tr+Ly8mu37D5aRewQKe2cyPD+LyUN7MbxvFiPysxjWN5NBeRlkpHnjHpcxPZUlBdMxqvD0f0B9FVz9zDEnhFAkSnF5NWt3VLGhrIoNu6rZuOsAOyvrY+tk+L2M7J/FycN7M6ffEEb1z2ZU/2yG9s0k3WcnfmPiwZKC6Zj3/wzFL8PF98CAMUf11XAkyrqdVawurWTtjirW7qjk07IDBMNRANJ8Hor6Z3PKcX0ZPSCH0QOyGT0gh4JeGXg8VrFpTCJZUjBHr+wTePn/wOgL4eRvHXb1cCTKJzuqeG9TBe9tqmDFln1Uu+X+uQEf4wry+MapwxhXkMfYwbmMyM92n6M3xnQ2Swrm6ARr4Z/fhIzeMOv+Nh9J3L6/jlfW7eL1DeUs37yXmqDTuuSo/tlcPnkw00f0ZdKQXhT2zuj01/iNMW2zpBBPHz0Ky+53Hp8M5LUYekFWP8jKd146yuoHmX07/gZuNOI0FVxbcXBoqG7eLLB4Dr645c9wn/HPbDJ2X+6KPc7YpFlhTxuPX770E9j9KXz9KedYXKrK2h1VvLxuFy+v28W6nU4LlsflZ3HFSYVMP64P00f0pV9OeseO1xjTKSwpxNOHD0NNOWT2cV40Kl/vvKzUUOW2596KjN4Hh0Avd7qXMx0JOt9vNjS2H78fjqIZ4KOWlu0krsYElt3fSSAr/gpfuBFGnkv5gXqWfV7Bss8reOuz3eyorEcEpgztzR0XncCMMQMY2S87cTEaY+LOkkK8hIPOG7jTroMv/qr5Z9Gokxhq9jg9N9XsdpJH43zjSb5uH+zb7IzrK503XZvebWT2gT4j3LuOfOdOo+mQnu08FaTqJCGNOONo2HnjN1jjvlnsvu0bqju4TmzAWVZf6TxyWlMOFZ/DtmVo7V725Z/M/9R8ibd/9yYby6sByAn4OPW4vtx8/mjOO6E/fbPtbsCY7sqSQryUrYZIg/P2bUsej3P1n9EL8kcd2fZinYskV30owqvry3lm1Xbe+rSMulLILN/FycP78OUphXxhZD5jBudaxbAxPYQlhXgp+cAZF7aSFDoiiQkhFInyTvEenvl4B0vXllETjNA/J525pxzHReMHMrGwF2m+DjT5YIzp8iwpxEvJ+5A3BHIHJTuSDttWUcui5dt4fEUpe6obyA34uGziYGZOHMz04/ra3YAxKcCSQryULoehpyQ7iqMWikR5df0uHn1/G29v3INH4LwTB/CVKYWcdXw/e1PYmBRjSSEeKkuhanv8io46wd6aIAvf2czi5SWUH2hgUF6AW2aMZvbJQxiYd6SN/RtjehpLCvHQWJ/QWiVzF1NZF2LB25t48F+bqQ1FOOf4/nx12lDOPr5fx5qGNsb0KJYU4qF0OfgyYOD4ZEfSpuqGMH/712b+8vYmqurDXDJ+EDfPKKJoQE6yQzPGdCEJTQoiciFwH+AFFqjq3S0+Hwo8BPRy17ldVZ9PZEwJUfI+DJ7cuf0DH6HaYJi/L9vKA29+zr7aEDNOHMAt5xcxdrC1xW+MOVTCkoKIeIH7gfOBUmC5iDyjquuarPYT4DFV/ZOIjAGeB4YnKqaECNXDztVw6neTHUkztcEwj7y3lT+/uYmKmiBnje7H988fzcQh1lm6MaZtibxTmAYUq+omABFZDMwCmiYFBRr7WcwDdiQwnsTY+bHTl24XqU9ovDOY/5aTDM4oyuem84qYOrxPskMzxnQDiUwKBUBJk/lSYHqLdX4GvCQiNwJZwIzWNiQi1wPXAwwdOjTugR6TkvedcZKfPGotGdw8o4gpwywZGGOOXLIrmucCC1X1tyJyKvB3ERmn2rz1OFWdD8wHmDp1agJbgeuAkg+g9wjI7peU3YciURYvL+G+Vzayp7qBM0f346bzipgyrHdS4jHGdG+JTArbgSFN5gvdZU19E7gQQFWXiUgAyAfKExhX/Kg6Tx4dd3an7zoaVZ7/ZCf3LN3Alopapg3vw5+/fpLdGRhjjkkik8JyoEhERuAkgznAV1ussw04D1goIicCAWB3AmOKr/1boXoXFJ7cqbt9p3gP//Xip6wureT4ATk8eM1Uzjm+v3VWY4w5ZglLCqoaFpEbgKU4j5s+qKprReQuYIWqPgPcCvxFRG7BqXS+RlW7VvFQe0qWO+MhLatKEqOqPsRtj69i6dpdDM4LcM9XJvJvkwusTSJjTNwktE7Bfefg+RbL7mwyvQ44LZExJFTpB+DPgv5H13F9RxSXH+D6h1eybW8tP7jweK49bQQBv7VLZIyJr2RXNHdvJe9D4RTwJvbHuHRtGd9f8jEZaV4e/dZ0ph/XN6H7M8akLmvspqOCNVD2SUIfRY1Gld+9tIFv/30lo/pn8+yNp1tCMMYklN0pVJfDk9fBgHFw/MVO/cCRXPnv+MjptjJBL61V1oW4ZcnHvPZpOV+ZUsgvLh9nxUXGmIRr8+wnIl8EclT1iRbLvwxUqurLiQ6uU2x7Dza9AZvfgmX/Axl9YPQXnQQx8lyn3+PWxF5ai/+TRyu37uX7j61i+746fjFrLFedMsyeLDLGdIr2LonvBC5vZfkbwLNAz0gKlaXO+HsfwY6PYcPzsOEFWLUIvOlw+i1w1g/A0+IqvWQ59C2CzPi9F1AfinDvK5/xl7c2MSgvg0XXn8LJ1jyFMaYTtZcU0lX1kHcGVHWPiGQlMKbOVVkK/kzoNQx6D4exl0MkDNuWwcq/wZt3O08ZXbEAstzyfFVn2egL4xbGmtJKvv/Yx2wsr2butCH8+JIxZKdb6Z4xpnO1d9bJFRGfqoabLhQRP5CR2LA6UWUJ5BZA0+IZrw9GnAHDT4cRZ8Hzt8Gfz4CvPARDToa9m6C2Ii71CcFwlP95vZj7Xy+mX3Y6C+edzNnH9z/m7RpjTEe0lxSexHmx7AZVrQEQkWyc/hGe7IzgOkXVdsgrbP0zEZjyDRg0ER67Gv52EXzxV5DudkxzjE8e7ays41sPrWDtjiqumFzATy8bS15m1+uTwRiTOtpLCj8BfglsFZGtgOC0ZfRX4P90Qmydo7IUii5of53Bk+Dbb8JT34EXfgCBXpCeC/1O6PBut++vY+7899hbE+SBq6Zw4biBHd6WMcbES5tJwS02ul1Efg6MchcXq2pdp0TWGcINTttFeUMOv25Gb5jzD3jnXnjtlzDyPPB07DWPkr21zP3Le1TWhfj7N6cxeai1aGqM6RraeyT1ihaLFOglIh+r6oHEhtVJqtxGW9sqPmrJ44EzboXRF0FGx3ow21bhJITqhjCPfms6EwqtJzRjTNfRXvHRZa0s6wNMEJFvquprCYqp81Q2JoWCo/vegI61dbR5Tw1f/ct71IUiPPqt6YwrsH6SjTFdS3vFR/NaWy4iw4DHOLQXte6n8R2FIyk+Okaf767mq395j1BE+ce3TmHM4NzDf8kYYzrZUT8Ir6pb3cdSu7/GpJA7OKG7KS6vZu5f3kNVWXTdKRw/MCeh+zPGmI466qQgIicADQmIpfNVlkBWP/An7rWLkr21XLXg/VhCKBpgCcEY03W1V9H8LE7lclN9gEHAVYkMqtNUljovriVI+YF6vv7X96kNhlny7VMtIRhjurz27hTuaTGvwF6cxHAVsCxRQXWaqu3Qd9Th1+uAytoQV//1A3ZVNfDIt6Zz4iCrQzDGdH3tVTS/2TgtIpNx+lf+CrAZ+GfiQ0swVedO4bhz4r7pmoYw1yz8gE27a/jrNVOZMszeQzDGdA/tFR+NBua6wx5gCSCqGv+zaDLU74dg9ZG/o3Ckmw1FuP7vK1hVsp8/fm0KZxT1i+v2jTEmkdorPvoUeBu4VFWLAUTklk6JqjPEHkeNX1IIR6J8b9FHvFNcwT1fmWhNVxhjup322mm4AtgJvC4ifxGR83DaP+oZKo/ybebDUFVuf3INL63bxU8vG8OXp8T3DsQYYzpDm0lBVZ9W1TnACcDrwM1AfxH5k4gcpgW5bqCyxBnHKSnc/eKnPLGylJvOK2LeaSPisk1jjOlsh23RTVVrVPUfqnoZUAh8BPww4ZElWmUpePyQdex9Fyx4exN/fnMTV50ylJtnFMUhOGOMSY6jenlNVfcB892he6ssddo86mBLp42e+qiUX/7vei4eP5CfzxxnfSkb00WEQiFKS0upr69PdiidKhAIUFhYiN/fsYYnUre/x8pSyD22oqPXN5Rz2+OrOfW4vtw7exJejyUEY7qK0tJScnJyGD58eMpcrKkqFRUVlJaWMmJEx4qxj+0yuTtrr8e1I/DRtn38xyMfMnpADvOvnkK6zxvH4Iwxx6q+vp6+ffumTEIAEBH69u17THdHqZkUImGo2tHhpFBcXs21C5fTLyedhdeeTE6gZ7QPaExPk0oJodGxHnNqJoXqMtBIh5JCdUOYaxcux+sR/v7NafTPCSQgQGNMT/CrX/2KsWPHMmHCBCZNmsT7778PwJ49e/D7/TzwwAPN1s/Ozm42v3DhQm644YbY/MMPP8y4ceMYP348kydP5p57WrZGdOxSMykcQz8Kdz27lpJ9tfzxa1MY1jcrzoEZY3qKZcuW8dxzz/Hhhx+yevVqXnnlFYYMcc45jz/+OKeccgqLFi064u298MIL/P73v+ell15izZo1vPfee+Tlxb+jrhRPCkfXQurStWU8tqKU75w1kmkj+iQgMGNMT7Fz507y8/NJT08HID8/n8GDnb5bFi1axG9/+1u2b99OaWnpEW3vN7/5Dffcc09sG+np6Vx33XVxjzs1nz6Kda5z5Emh/EA9dzy5hnEFudw8Y3SCAjPGJMLPn13Luh1Vcd3mmMG5/PSysW1+fsEFF3DXXXcxevRoZsyYwezZsznrrLMoKSlh586dTJs2jSuvvJIlS5Zw6623HnZ/n3zyCVOmTInnIbQqde8UAnkQOLLmrFWVHzyxmpqGML+fPYk0X2r+2IwxRy47O5uVK1cyf/58+vXrx+zZs1m4cCFLlizhyiuvBGDOnDmHLULq7MryhN4piMiFwH2AF1igqne3ss6VwM9w+mtYpapfTWRMgPvi2pHXJzzy3lbe2LCbn102hlH9raMcY7qb9q7oE8nr9XL22Wdz9tlnM378eB566CF27NhBWVkZjz76KAA7duxg48aNFBUVkZGRQTAYJC0tDYC9e/eSn58PwNixY1m5ciXnnntuQmNO2CWviHiB+4GLgDHAXBEZ02KdIuAO4DRVHYvTvlLiHUWPa5/vruZXz6/nzNH9uPrU4YmNyxjTY2zYsIGNGzfG5j/++GMikQjV1dVs376dLVu2sGXLFu64447Y3cJZZ53FI488AkBdXR2PPfYY55zj9FZwxx13cNttt1FWVgZAMBhkwYIFcY87kXcK04BiVd0EICKLgVnAuibrXAfc7zafgaqWJzCeg6pKYci0w64WikS5ZcnHBPxe/vvLE/DYG8vGmCNUXV3NjTfeyP79+/H5fIwaNYrp06czYcKEZut96UtfYvbs2dx5553cd999fPvb3+YPf/gDqsrVV1/NmWeeCcDFF1/Mrl27mDFjBqqKiHDttdfGPe5EJoUCoKTJfCkwvcU6owFE5B2cIqafqeqLCYwJGqqhbt8RvaPwh1c3srq0kj997SQG5Nr7CMaYIzdlyhTefffdw643YcIE1q9fD0BBQQHPPfdcm+vOmzePefPmxS3G1iT76SMfUAScjdMC61siMl5V9zddSUSuB64HGDp06LHtsaqxH4X26xRWlezn/teL+dJJhVw0ftCx7dMYY7qJRD5Gsx1oeuYtdJc1VQo8o6ohVd0MfIaTJJpR1fmqOlVVp/brd4zdWx5BPwoN4Qi3PbGK/jkBfjpzTJvrGWNMT5PIpLAcKBKRESKSBswBnmmxztM4dwmISD5OcdKmBMZ0RC+u3f/653y2q5pfXzGOXGvXyBiTQhKWFFQ1DNwALAXWA4+p6loRuUtEZrqrLQUqRGQdTu9ut6lqRaJiApxuOMUDOa0XCa3bUcUfXy/miskFnHvCgISGYowxXU1C6xRU9Xng+RbL7mwyrcD33aFzVJY6CcF76B1AKBLltidW0SszjTsvs2IjY0zqSXZFc+erLGmzPmH+W5tYu6OKB646iV6ZaZ0cmDHGJF/qtdfQxotrxeUHuO+VjVwyfhAXjrOnjYwxx660tJRZs2ZRVFTEyJEjuemmmwgGg7HPb775ZgoKCohGo7FlCxcuRER45ZVXYsuefvppRIQnnngi4TGnVlKIRlvtcS0SVW57YjVZ6V5+NjM5r8MbY3oWVeWKK67g8ssvZ+PGjXz22WdUV1fz4x//GIBoNMpTTz3FkCFDePPNN5t9d/z48SxevDg2v2jRIiZOnNgpcadWUqjdA5HgIe8o/O2dzXy0bT8/mzmWfjnpSQrOGNOTvPbaawQCgdjLZl6vl3vvvZcHH3yQ2tpa3njjDcaOHct3vvOdQxrFO+OMM/jggw8IhUJUV1dTXFzMpEmTOiXu1KpTaOUdheLyau55aQMzTuzPzImDkxSYMSahXrgdytbEd5sDx8NFh7TxGbN27dpDmrrOzc1l6NChFBcXs2jRIubOncusWbP40Y9+RCgUwu93HoAREWbMmMHSpUuprKxk5syZbN68Ob7xtyG17hRi7yg4SaG6Icy/P7KSrDQfv7x8fEr252qMSY7nn3+eyy+/nNzcXKZPn87SpUubfT5nzhwWL17M4sWLmTt3bqfFlWJ3CgeTgtNHwio27a7mkW9OZ2CetW1kTI/VzhV9oowZM+aQiuGqqiq2bdvG5s2b2b9/P+PHjwegtraWjIwMLr300ti606ZNY82aNWRmZjJ6dOd17JVidwrbwZ8JGb35y9ubeH5NGT+88AS+MCo/2ZEZY3qY8847j9raWh5++GEAIpEIt956K9dccw1LlixhwYIFseazN2/ezMsvv0xtbW2zbdx99938+te/7tS4UywpOO8ovPt5BXe/8CkXjx/I9Wcel+yojDE9kIjw1FNP8fjjj1NUVMTo0aMJBALceeedvPjii1xyySWxdbOysjj99NN59tlnm23joosuivWn0GlxOy8Vdx9Tp07VFStWdOzL88+h3pfLF7bfQJ+sNJ7+7mlkp6dWCZoxqWL9+vWceOKJyQ4jKVo7dhFZqapTD/fdlLpT0MpS3i5PIxiO8uevT7GEYIwxLaTOWTHcgNSUsyaUyz1zJzKyX3ayIzLGmC4nZe4U/vdfTpHT8cefyIXjBiY5GmOM6ZpSJikUZTiduV142mGL1IwxPUR3qzONh2M95pRJCqPTKwHw9mq/G05jTM8QCASoqKhIqcSgqlRUVBAIdPy9q9SpU6h0ewJtpYVUY0zPU1hYSGlpKbt37052KJ0qEAhQWNh2d8OHkzpJ4cz/hKnzwG9vLhuTCvx+PyNGjEh2GN1OyhQfIQJZ9uayMca0J3WSgjHGmMOypGCMMSam2zVzISK7ga0d/Ho+sCeO4XQXqXrckLrHbsedWo7kuIepar/DbajbJYVjISIrjqTtj54mVY8bUvfY7bhTSzyP24qPjDHGxFhSMMYYE5NqSWF+sgNIklQ9bkjdY7fjTi1xO+6UqlMwxhjTvlS7UzDGGNOOlEkKInKhiGwQkWIRuT3Z8SSKiDwoIuUi8kmTZX1E5GUR2eiOeyczxkQQkSEi8rqIrBORtSJyk7u8Rx+7iARE5AMRWeUe98/d5SNE5H33932JiKQlO9ZEEBGviHwkIs+58z3+uEVki4isEZGPRWSFuyxuv+cpkRRExAvcD1wEjAHmisiY5EaVMAuBC1ssux14VVWLgFfd+Z4mDNyqqmOAU4Dvuv/HPf3YG4BzVXUiMAm4UEROAf4LuFdVRwH7gG8mMcZEuglY32Q+VY77HFWd1OQx1Lj9nqdEUgCmAcWquklVg8BiYFaSY0oIVX0L2Nti8SzgIXf6IeDyTg2qE6jqTlX90J0+gHOiKKCHH7s6qt1ZvzsocC7whLu8xx03gIgUApcAC9x5IQWOuw1x+z1PlaRQAJQ0mS91l6WKAaq6050uAwYkM5hEE5HhwGTgfVLg2N0ilI+BcuBl4HNgv6qG3VV66u/774EfAFF3vi+pcdwKvCQiK0XkendZ3H7PU6fpbAM4V5Yi0mMfORORbOCfwM2qWuVcPDp66rGragSYJCK9gKeAE5IcUsKJyKVAuaquFJGzkx1PJztdVbeLSH/gZRH5tOmHx/p7nip3CtuBpl2uFbrLUsUuERkE4I7LkxxPQoiIHychPKqqT7qLU+LYAVR1P/A6cCrQS0QaL/p64u/7acBMEdmCUxx8LnAfPf+4UdXt7rgc5yJgGnH8PU+VpLAcKHKfTEgD5gDPJDmmzvQM8A13+hvA/0tiLAnhlif/FVivqr9r8lGPPnYR6efeISAiGcD5OPUprwNfdlfrccetqneoaqGqDsf5e35NVb9GDz9uEckSkZzGaeAC4BPi+HueMi+vicjFOGWQXuBBVf1VkkNKCBFZBJyN02riLuCnwNPAY8BQnBZmr1TVlpXR3ZqInA68DazhYBnzj3DqFXrssYvIBJyKRS/ORd5jqnqXiByHcwXdB/gIuEpVG5IXaeK4xUf/qaqX9vTjdo/vKXfWB/xDVX8lIn2J0+95yiQFY4wxh5cqxUfGGGOOgCUFY4wxMZYUjDHGxFhSMMYYE2NJwRhjTIwlBZOyRKTaHQ8Xka/Geds/ajH/bjy3b0yiWFIwBoYDR5UUmrw125ZmSUFVv3CUMRmTFJYUjIG7gTPc9ulvcRuY+28RWS4iq0Xk2+C8JCUib4vIM8A6d9nTbsNkaxsbJxORu4EMd3uPussa70rE3fYnbpv4s5ts+w0ReUJEPhWRR6Vpw03GdBJrEM8Yp+35/1TVSwHck3ulqp4sIunAOyLykrvuScA4Vd3szl+rqnvdJiaWi8g/VfV2EblBVSe1sq8rcPo9mIjz1vlyEXnL/WwyMBbYAbyD077Pv+J/uMa0ze4UjDnUBcDVbnPU7+M0yVzkfvZBk4QA8D0RWQW8h9PoYhHtOx1YpKoRVd0FvAmc3GTbpaoaBT7GKdYyplPZnYIxhxLgRlVd2myh08ZOTYv5GcCpqlorIm8AgWPYb9M2eiLY36dJArtTMAYOADlN5pcC33Gb4kZERrstUraUB+xzE8IJON2ANgo1fr+Ft4HZbr1FP+BM4IO4HIUxcWBXIsbAaiDiFgMtxGmXfzjwoVvZu5vWuzd8Efh3EVkPbMApQmo0H1gtIh+6TTo3egqnv4NVOD1o/UBVy9ykYkzSWSupxhhjYqz4yBhjTIwlBWOMMTGWFIwxxsRYUjDGGBNjScEYY0yMJQVjjDExlhSMMcbEWFIwxhgT8/8BdZWN7U8QSIkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot results\n",
    "fig = plt.figure()  # create a figure object\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "ax1.plot(sauc_time, sauc_auc, label='SAUC')\n",
    "ax1.plot(oam_time, oam_auc, label='OAM')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('AUC')\n",
    "plt.legend()\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "ax2.plot(iteration, sauc_auc, label='SAUC')\n",
    "ax2.plot(iteration, oam_auc, label='OAM')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
